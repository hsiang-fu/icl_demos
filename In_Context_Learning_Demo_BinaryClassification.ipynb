{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction to In-Context Learning Demo: Binary Classification**\n",
        "\n",
        "## **Overview**\n",
        "This notebook presents a step-by-step walkthrough of an interactive demo that explores how Large Language Models (LLMs) can perform **image-based binary classification purely through in-context learning**. Inspired by the idea that LLMs can function as general pattern recognizers, this experiment uses small image snippets of hazelnuts—labeled as good or bad—to examine how effectively Gemini can internalize visual defect patterns and reproduce consistent judgments on new samples. The goal is to illustrate how an LLM—without explicit training, fine-tuning, or feature engineering—can infer quality cues such as surface texture, cracks, mold, or deformation from a few examples and generalize to unseen hazelnuts. The resulting approach **allows for online and incremental learning of perception** without any gradient-based updates.  \n",
        "\n",
        "## **Background**\n",
        "This demo focuses on real hazelnut images categorized into two classes:\n",
        "- Good hazelnuts: Typically clean, symmetric, and free of surface defects.\n",
        "- Bad hazelnuts: Containing surface markings, cracks, discoloration, mold, dents, or structural abnormalities.\n",
        "\n",
        "Each example image serves as an example pairing:\n",
        "- A compact visual representation\n",
        "- A binary label: \"Good\" or \"Bad\"\n",
        "\n",
        "What makes this setup particularly compelling:\n",
        "- The data was checked before utilization to make sure the LLM cannot classify the quality of hazelnuts before ICL.\n",
        "- The model receives visual examples only through the prompt context\n",
        "- No training or gradient updates occur—classification arises from pattern matching\n",
        "- Prompts can include descriptions, annotations, or multi-step chains of examples\n",
        "- The test images require generalization, not memorization\n",
        "- Predictions are generated sample-by-sample, mimicking standard evaluation flows\n",
        "\n",
        "This setting provides a clear benchmark for understanding how well LLMs can perform visual classification tasks when guided only through carefully constructed prompts.\n",
        "\n",
        "## **Let's Take a Look at an Example**\n",
        "\n",
        "The illustration below shows good and bad hazelnut examples. Given a set of labeled samples in context, the LLM must determine the correct label for each new, unseen hazelnut image during evaluation.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/intro-to-icl/intro-to-icl.github.io/refs/heads/master/static/images/binary_classification.png\" height=\"500\">\n",
        "\n",
        "## **LLM as the Classifier**\n",
        "\n",
        "In this demo, we use Gemini 2.0/2.5 Flash in non-reasoning mode, prompting the model to rely on direct pattern recognition rather than symbolic explanation or analytic reasoning.\n",
        "\n",
        "The workflow proceeds as follows:\n",
        "- Provide 10 total labeled hazelnut images (5 good, 5 bad) as in-context examples\n",
        "- Send a new unlabeled image through the prompt to classify\n",
        "- Ask the model to output either \"Good\" or \"Bad\"\n",
        "\n",
        "The LLM functions as a lightweight, prompt-driven classifier—absorbing visual differences, structural patterns, and defect signatures from the in-context examples.\n",
        "\n",
        "## **Evaluation**\n",
        "Finally, we compare the model’s predicted labels against ground-truth quality labels and accuracy. This provides an insight into how effectively a LLM can approximate visual quality-control decisions through in-context learning alone—without any dedicated training pipeline.\n",
        "\n",
        "## **Code Overview**\n",
        "The implementation is structured modularly, with each component handling a distinct stage of the ICL classification pipeline. This separation makes the system easy to modify, extend, and reuse:\n",
        "- Data loading & preprocessing: Read images, convert to model-compatible format\n",
        "- Visualization: Display sets of good/bad examples\n",
        "- Prompt construction: Insert labeled samples into few-shot prompts\n",
        "- LLM inference: Retrieve predictions one image at a time\n",
        "- Evaluation: Compute metrics and compare with a baseline (CNN)\n",
        "\n",
        "## **Before Running the Demo, Follow the Instructions Below**\n",
        "To run the full experiment:\n",
        "1. Ensure all dependencies are imported and installed.\n",
        "2.\tVisit Google AI Studio (https://aistudio.google.com/) to obtain your Gemini API key.\n",
        "3.\tOnce API key is generated, copy and paste it into the demo when prompted.\n",
        "\n",
        "#### **If having trouble creating an API Key, follow the link below for instructions:**\n",
        "* #### **[Instructions on How to Obtain a Gemini API Key](https://docs.google.com/document/d/17pgikIpvZCBgcRh4NNrcee-zp3Id0pU0vq2SG52KIGE/edit?usp=sharing)**\n",
        "\n"
      ],
      "metadata": {
        "id": "JWBl0iMOvbE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Import Necessary Libraries**\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "import re\n",
        "import itertools\n",
        "import math\n",
        "import ast\n",
        "import json\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "from IPython.display import Image\n",
        "import os\n",
        "import getpass\n",
        "import time"
      ],
      "metadata": {
        "cellView": "form",
        "id": "qDV03CJIve4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lh6d1fWygOfe"
      },
      "outputs": [],
      "source": [
        "#@title **Setting Up Gemini Client**\n",
        "apikey = getpass.getpass(\"Enter your Gemini API Key: \")\n",
        "client = genai.Client(api_key=apikey)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Choose a Model**\n",
        "model_dropdown = widgets.Dropdown(\n",
        "    options=[\n",
        "        (\"Gemini 2.5 Flash\", \"gemini-2.5-flash\"),\n",
        "        (\"Gemini 2.0 Flash\", \"gemini-2.0-flash\"),\n",
        "    ],\n",
        "    description=\"Model:\",\n",
        "    value=\"gemini-2.5-flash\",\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "confirm_button = widgets.Button(\n",
        "    description=\"Confirm Selection\"\n",
        ")\n",
        "\n",
        "output = widgets.Output()\n",
        "\n",
        "model_name = None\n",
        "\n",
        "def on_confirm_click(b):\n",
        "    global model_name, batch_size\n",
        "\n",
        "    model_name = model_dropdown.value\n",
        "\n",
        "    with output:\n",
        "        output.clear_output()\n",
        "        print(f\"\\nSelected model: {model_name}\")\n",
        "\n",
        "confirm_button.on_click(on_confirm_click)\n",
        "\n",
        "display(model_dropdown, confirm_button, output)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "3uUXIAcs2KxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Download Data from GitHub**\n",
        "if not os.path.exists(\"intro_to_icl_data\"):\n",
        "    !git clone https://github.com/hsiang-fu/intro_to_icl_data.git\n",
        "\n"
      ],
      "metadata": {
        "id": "6UMMqb9zX20A",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cell `ICL Binary Classification` below  is responsible for running ICL binary hazelnut classification. It loads labeled training examples, constructs the few-shot prompt, sends the prompt to the LLM for each test image, parses the prediction, evaluates correctness, and finally reports overall accuracy.\n",
        "\n",
        "Each test image is displayed, and the model’s prediction and the ground truth label are printed. Running each cell will perform inference across all test images.\n",
        "\n",
        "After the cell runs, it performs several steps. First, it builds the ICL training examples by loading ten labeled hazelnut images (five Good and five Bad). Each example is converted into a Gemini Part so it can be embedded directly into the prompt. These form the annotated few-shot demonstrations the model uses to learn the classification pattern. Next, it constructs the full ICL prompt for each test item by including the instruction, all labeled example images, the unlabeled test image, and a rule specifying that the model should respond only with “Good” or “Bad.” Then it loads the unseen test images and their ground-truth labels. For each test image, the cell displays the image, sends the entire ICL prompt to Gemini, reads the model’s label prediction, compares it to the ground truth, and stores the results. After all images are processed, the cell computes summary metrics such as accuracy, total number of correct predictions, and incorrect predictions.\n",
        "\n",
        "Each evaluation cycle outputs the test hazelnut image, the model’s predicted label, the true label, and whether the prediction was correct. At the end, the code prints a performance summary showing the model’s accuracy across all ten test images."
      ],
      "metadata": {
        "id": "NkxcjLfcvzzw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Let's take a look at the prompt**\n",
        "\n",
        "```\n",
        "You are an expert in visual inspection.\n",
        "Based on the examples provided, classify whether the test hazelnut is 'Good' or 'Bad'.\n",
        "\n",
        "{Example Image: Label Pairs}\n",
        "\n",
        "What is the condition of the hazelnut? Only return 'Good' or 'Bad'.\n",
        "```\n",
        "\n",
        "This prompt shows the model several example image–label pairs of hazelnuts, where each example presents an image of a hazelnut alongside its correct classification label (“Good” or “Bad”). These examples act as demonstrations of the visual criteria that define each category. After providing these labeled examples, the prompt then presents a new unseen and unlabeled test hazelnut and asks the model to classify it based on the pattern it inferred from the examples.\n",
        "\n",
        "This is considered ICL because the model is not being updated or fine-tuned. All “learning” occurs within the prompt itself: the example image–label pairs serve as the model’s temporary training data. By examining these demonstrations, the model infers the visual characteristics associated with “Good” versus “Bad” hazelnuts and applies that inferred pattern to the test hazelnut during the same forward pass. In other words, the model learns from context, not from parameter updates, which is exactly what defines ICL."
      ],
      "metadata": {
        "id": "1dzsNIanp27e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5_8IkaFgOff"
      },
      "outputs": [],
      "source": [
        "#@title **ICL Binary Classification**\n",
        "\n",
        "print(\"\\nRunning Binary Image Classification\\n\")\n",
        "\n",
        "test_ground_truth = {\n",
        "    \"test1\": \"Bad\",\n",
        "    \"test2\": \"Good\",\n",
        "    \"test3\": \"Good\",\n",
        "    \"test4\": \"Bad\",\n",
        "    \"test5\": \"Bad\",\n",
        "    \"test6\": \"Bad\",\n",
        "    \"test7\": \"Good\",\n",
        "    \"test8\": \"Bad\",\n",
        "    \"test9\": \"Good\",\n",
        "    \"test10\": \"Good\",\n",
        "}\n",
        "\n",
        "test_images = list(test_ground_truth.keys())\n",
        "\n",
        "model_predictions = []\n",
        "icl_results = []\n",
        "\n",
        "train_filenames = [f\"good{i}\" for i in range(1, 6)] + [f\"bad{i}\" for i in range(1, 6)]\n",
        "train_labels = [\"Good\"] * 5 + [\"Bad\"] * 5\n",
        "\n",
        "def load_part(path):\n",
        "    \"\"\"Load an image file as a Gemini Part.\"\"\"\n",
        "    with open(path, \"rb\") as f:\n",
        "        return types.Part.from_bytes(\n",
        "            data=f.read(),\n",
        "            mime_type=\"image/png\"\n",
        "        )\n",
        "\n",
        "train_parts = [\n",
        "    load_part(f\"intro_to_icl_data/hazelnuts/{name}.png\")\n",
        "    for name in train_filenames\n",
        "]\n",
        "\n",
        "def classify_image(selected_label):\n",
        "\n",
        "    test_path = f\"intro_to_icl_data/hazelnuts/{selected_label}.png\"\n",
        "    test_part = load_part(test_path)\n",
        "    with open(test_path, \"rb\") as f:\n",
        "      display(Image(data=f.read()))\n",
        "\n",
        "    contents = [\n",
        "        \"You are an expert in visual inspection. \"\n",
        "        \"Based on the examples provided, classify whether the test hazelnut is 'Good' or 'Bad'.\\n\"\n",
        "    ]\n",
        "\n",
        "    # Add example images as few-shot ICL\n",
        "    for i, (label, part) in enumerate(zip(train_labels, train_parts)):\n",
        "        contents.append(f\"Example {i+1}: {label}\")\n",
        "        contents.append(part)\n",
        "\n",
        "    # Add the test image\n",
        "    contents.extend([\n",
        "        \"What is the condition of the hazelnut? Only return 'Good' or 'Bad'.\",\n",
        "        test_part,\n",
        "    ])\n",
        "\n",
        "    print(f\"\\nClassifying {selected_label}...\")\n",
        "    try:\n",
        "      response = client.models.generate_content(\n",
        "          model=model_name,\n",
        "          contents=contents,\n",
        "          config=types.GenerateContentConfig(\n",
        "              thinking_config=types.ThinkingConfig(thinking_budget=0)\n",
        "          )\n",
        "      )\n",
        "    except Exception as e:\n",
        "      print(\"Waiting 60 seconds for quota limit reset.\")\n",
        "      time.sleep(60)\n",
        "      response = client.models.generate_content(\n",
        "          model=model_name,\n",
        "          contents=contents,\n",
        "          config=types.GenerateContentConfig(\n",
        "              thinking_config=types.ThinkingConfig(thinking_budget=0)\n",
        "          )\n",
        "      )\n",
        "\n",
        "    pred = response.text.strip()\n",
        "    print(f\"LLM Prediction: {pred}\")\n",
        "    return pred\n",
        "\n",
        "def evaluate_prediction(test_name, pred):\n",
        "    true_label = test_ground_truth[test_name]\n",
        "    correct = (pred == true_label)\n",
        "\n",
        "    model_predictions.append({\n",
        "        \"image\": test_name,\n",
        "        \"prediction\": pred,\n",
        "        \"label\": true_label,\n",
        "        \"correct\": correct\n",
        "    })\n",
        "\n",
        "    icl_results.append({\n",
        "        \"image\": test_name,\n",
        "        \"pred\": pred,\n",
        "        \"true\": true_label,\n",
        "        \"correct\": correct\n",
        "    })\n",
        "\n",
        "    print(f\"Ground Truth: {true_label}\\n\")\n",
        "\n",
        "for test_name in test_images:\n",
        "    pred = classify_image(test_name)\n",
        "    evaluate_prediction(test_name, pred)\n",
        "\n",
        "def compute_final_metrics():\n",
        "    if not model_predictions:\n",
        "        print(\"No predictions collected.\")\n",
        "        return\n",
        "\n",
        "    acc = np.mean([p[\"correct\"] for p in model_predictions])\n",
        "\n",
        "    print(\"\\n=== Model Performance Summary ===\")\n",
        "    print(f\"Accuracy: {acc:.3f}\")\n",
        "    print(f\"Total Samples: {len(model_predictions)}\")\n",
        "    print(f\"Correct: {sum(p['correct'] for p in model_predictions)}\")\n",
        "    print(f\"Incorrect: {len(model_predictions) - sum(p['correct'] for p in model_predictions)}\")\n",
        "\n",
        "compute_final_metrics()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell `Baseline CNN Binary Classification` implements the baseline model used to compare against the two in-context learning binary classification methods. This baseline relies on a standard convolutional neural network (CNN) that learns directly from pixel data to distinguish Good vs. Bad hazelnut images.\n",
        "\n",
        "After defining a small image-loading function, the code loads ten training images (five Good, five Bad) and assigns binary labels. It then loads a separate test set with known ground-truth labels. All images are resized to 128×128 and normalized. With the data prepared, the script builds a compact CNN consisting of three convolution-pooling blocks followed by dense layers, ending in a sigmoid output for binary classification.\n",
        "\n",
        "The model is trained for 100 epochs, evaluated on the same test images as the ones used in ICL, and its overall accuracy is reported. The final section generates predictions for each test image and prints whether each classification matches the true label. This produces a clear, image-based baseline to compare against the LLM’s in-context learning binary classification approaches."
      ],
      "metadata": {
        "id": "6eg0QRmsim4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title **Baseline CNN Binary Classification**\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "print(\"\\nRunning Baseline CNN Model\\n\")\n",
        "\n",
        "def load_image(path, img_size=(128, 128)):\n",
        "    img = load_img(path, target_size=img_size)\n",
        "    arr = img_to_array(img) / 255.0\n",
        "    return arr\n",
        "\n",
        "train_filenames = [f\"good{i}\" for i in range(1, 6)] + [f\"bad{i}\" for i in range(1, 6)]\n",
        "train_labels = [\"Good\"] * 5 + [\"Bad\"] * 5\n",
        "\n",
        "X_train = []\n",
        "y_train = []\n",
        "\n",
        "for file, label in zip(train_filenames, train_labels):\n",
        "    path = f\"intro_to_icl_data/hazelnuts/{file}.png\"\n",
        "    X_train.append(load_image(path))\n",
        "    y_train.append(1 if label == \"Good\" else 0)\n",
        "\n",
        "X_train = np.array(X_train, dtype=\"float32\")\n",
        "y_train = np.array(y_train, dtype=\"int32\")\n",
        "\n",
        "print(f\"Loaded training images: {X_train.shape}\")\n",
        "\n",
        "test_ground_truth = {\n",
        "    \"test1\": \"Bad\",\n",
        "    \"test2\": \"Good\",\n",
        "    \"test3\": \"Good\",\n",
        "    \"test4\": \"Bad\",\n",
        "    \"test5\": \"Bad\",\n",
        "    \"test6\": \"Bad\",\n",
        "    \"test7\": \"Good\",\n",
        "    \"test8\": \"Bad\",\n",
        "    \"test9\": \"Good\",\n",
        "    \"test10\": \"Good\",\n",
        "}\n",
        "\n",
        "test_images = list(test_ground_truth.keys())\n",
        "\n",
        "X_test = []\n",
        "y_test = []\n",
        "\n",
        "for name in test_images:\n",
        "    path = f\"intro_to_icl_data/hazelnuts/{name}.png\"\n",
        "    X_test.append(load_image(path))\n",
        "    y_test.append(1 if test_ground_truth[name] == \"Good\" else 0)\n",
        "\n",
        "X_test = np.array(X_test, dtype=\"float32\")\n",
        "y_test = np.array(y_test, dtype=\"int32\")\n",
        "\n",
        "print(f\"Loaded test images: {X_test.shape}\")\n",
        "\n",
        "baseline = models.Sequential([\n",
        "    layers.Conv2D(16, (3, 3), activation=\"relu\", input_shape=(128, 128, 3)),\n",
        "    layers.MaxPooling2D(2, 2),\n",
        "\n",
        "    layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
        "    layers.MaxPooling2D(2, 2),\n",
        "\n",
        "    layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
        "    layers.MaxPooling2D(2, 2),\n",
        "\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation=\"relu\"),\n",
        "    layers.Dense(1, activation=\"sigmoid\")  # binary output\n",
        "])\n",
        "\n",
        "baseline.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"binary_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "print(\"\\nTraining Baseline Model...\\n\")\n",
        "\n",
        "history = baseline.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=100,\n",
        "    batch_size=4,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nEvaluating Baseline Model...\\n\")\n",
        "test_loss, test_acc = baseline.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Baseline Test Accuracy: {test_acc:.3f}\")\n",
        "baseline_results = []\n",
        "preds = (baseline.predict(X_test) > 0.5).astype(int).flatten()\n",
        "\n",
        "print(\"\\n=== Baseline Model Predictions ===\")\n",
        "for i, name in enumerate(test_images):\n",
        "    pred_label = \"Good\" if preds[i] == 1 else \"Bad\"\n",
        "    true_label = test_ground_truth[name]\n",
        "    correct = pred_label == true_label\n",
        "    baseline_results.append({\n",
        "        \"image\": name,\n",
        "        \"pred\": pred_label,\n",
        "        \"true\": true_label,\n",
        "        \"correct\": pred_label == true_label\n",
        "    })\n",
        "    print(f\"{name}: Pred={pred_label} | True={true_label} | {'Correct' if correct else 'Wrong'}\")"
      ],
      "metadata": {
        "id": "SuuV-bgu6MYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Comparing ICL(annotation aware and agnostic) vs Baseline Model**\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Convert to DataFrames\n",
        "df_icl = pd.DataFrame(icl_results)\n",
        "df_baseline = pd.DataFrame(baseline_results)\n",
        "\n",
        "# Merge by image name\n",
        "comparison = df_icl.merge(df_baseline, on=\"image\", suffixes=(\"_icl\", \"_baseline\"))\n",
        "\n",
        "display(comparison)\n",
        "\n",
        "# Compute accuracy\n",
        "acc_icl = comparison[\"correct_icl\"].mean()\n",
        "acc_base = comparison[\"correct_baseline\"].mean()\n",
        "\n",
        "print(f\"\\nICL Accuracy: {acc_icl:.3f}\")\n",
        "print(f\"Baseline Accuracy: {acc_base:.3f}\")\n",
        "\n",
        "# Bar Chart Comparison\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.bar([\"ICL Aware\", \"Baseline CNN\"], [acc_icl, acc_base])\n",
        "plt.ylim(0,1)\n",
        "plt.title(\"Accuracy Comparison\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.show()\n",
        "\n",
        "# Confusion Matrix Option\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "y_true = comparison[\"true_baseline\"].map({\"Good\":1,\"Bad\":0})\n",
        "\n",
        "def print_confusion_stats(cm, title=\"Confusion Matrix\"):\n",
        "    TN, FP, FN, TP = cm.ravel()\n",
        "\n",
        "    print(f\"\\n=== {title} ===\")\n",
        "    print(f\"True Positive : {TP}\")\n",
        "    print(f\"False Positive: {FP}\")\n",
        "    print(f\"False Negative: {FN}\")\n",
        "    print(f\"True Negative : {TN}\")\n",
        "\n",
        "cm_icl = confusion_matrix(y_true, comparison[\"pred_icl\"].map({\"Good\":1,\"Bad\":0}))\n",
        "cm_base = confusion_matrix(y_true, comparison[\"pred_baseline\"].map({\"Good\":1,\"Bad\":0}))\n",
        "\n",
        "print_confusion_stats(cm_icl, \"ICL\")\n",
        "print_confusion_stats(cm_base, \"Baseline CNN\")"
      ],
      "metadata": {
        "id": "OvcxVDPk-vMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Summary**\n",
        "\n",
        "This demo illustrates how LLMs can perform ICL for binary image classification, using hazelnuts as the target task. By providing the model with a small set of annotated image–label pairs, we show that it can infer the visual distinctions between “Good” and “Bad” hazelnuts—such as cracks, surface irregularities, or discoloration—without ever being trained as a model for this specific purpose.\n",
        "\n",
        "To ground the evaluation, we compare the LLM’s classification behavior against a traditional Convolutional Neural Network (CNN) baseline trained directly on the pixel data. The CNN provides a fully supervised reference point, learning visual features from scratch and offering a stable benchmark for accuracy. When evaluated across a set of unseen test images, ICL performs better than the baseline CNN and achieves a higher accuracy, by classifying all the test images correct. Even if we increase the number of epochs the CNN was trained on, the amount of training data was still not enough for the baseline to classify the hazelnuts correctly\n",
        "\n",
        "The techniques demonstrated here can be extended beyond hazelnut inspection to a broad range of lightweight image-based classification tasks. With a small set of curated examples, LLMs can be prompted to generalize visual patterns related to binary categories: quality control, medical triage, material defects, etc. These results highlight how ICL can provide quick, adaptable classification capabilities without the need to train or fine-tune a neural network, significantly reducing computational costs and setup time.\n",
        "\n",
        "## **Conclusion**\n",
        "\n",
        "This demonstration shows that LLMs can perform meaningful visual classification through pattern recognition alone—interpreting defects in hazelnut images using only the examples supplied in the prompt. The ICL approach allows for stronger generalization from very small datasets (10 images). Compared to the CNN baseline, ICL performs better and more consistent, proving that the LLM’s flexibility and ability to mimic a human-like pattern learning skill. Together, the comparison between ICL and the CNN baseline offers a clear perspective on why LLMs are capable of excelling in tasks that normally would take traditional approaches much more time to retrain and larger datasets that would need to be used."
      ],
      "metadata": {
        "id": "fSvKO3MLk-ZN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9n-DrhP6pzmp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}