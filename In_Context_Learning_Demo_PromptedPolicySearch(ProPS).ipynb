{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhX16i8VfLtT",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "# **Prompted Policy Search (ProPS): Reinforcement Learning through Linguistic and Numerical Reasoning in LLMs**\n",
        "\n",
        "This notebook serves as a detailed tutorial on *[Prompted Policy Search (ProPS)](https://props-llm.github.io/)* (Zhou et al., 2025) a novel RL method that unifies numerical and linguistic reasoning within a single framework. ProPS places a large language model (LLM) at the center of the policy optimization loop; directly proposing policy updates based on both reward feedback and natural language input.\n",
        "\n",
        "In this tutorial, we utilize a Large Language Model (LLM), specifically the Gemini 2.0/2.5 Flash, to perform Policy Search for a linear control policy within an OpenAI Gym reinforcement learning environment. Our focus will be on the MountainCar environment, where we aim to employ the Gemini family of models to discover the optimal parameters for a simple linear policy that enables the car to successfully reach its goal.\n",
        "\n",
        "## **Environment: State and Action Variables**\n",
        "<img src=\"https://raw.githubusercontent.com/intro-to-icl/intro-to-icl.github.io/refs/heads/master/static/images/mountain_car.gif\" height=\"200\"><img src=\"https://raw.githubusercontent.com/intro-to-icl/intro-to-icl.github.io/refs/heads/master/static/images/mountaincar_parameters.png\" height=\"200\">\n",
        "\n",
        "The MountainCar environment presents a classic control challenge framed as a deterministic Markov Decision Process (MDP). In this setup, the environment's future state depends solely on the current state and the action taken, not on the history of preceding states. The term \"deterministic\" signifies that a specific action performed in a particular state will consistently lead to the identical next state and reward. The core task involves an underpowered car, initially placed randomly in the valley between two hills. The objective is to drive the car to the goal located at the top of the right hill. Due to the car's limited engine power, it cannot ascend the steep slope directly. Instead, the agent must learn a strategy of driving back and forth, building momentum to eventually conquer the hill.\n",
        "\n",
        "Control over the car is exerted by applying an external force at each discrete timestep. This force is determined by an action value, $a$, chosen from the continuous range [-1, 1]. This action value is then scaled by a constant factor (0.0015) to yield the actual physical force applied. A positive action propels the car rightward, while a negative action pushes it leftward. At every timestep, the agent observes the environment's current state, which is captured by a two-dimensional vector:\n",
        "\n",
        "\\begin{bmatrix} x \\\\ v \\end{bmatrix}\n",
        "\n",
        "Here, $x$ represents the car's horizontal position (ranging from -1.2 to 0.6, with the goal at 0.5), and $v$ denotes its current velocity (ranging from -0.07 to 0.07). The environment provides a reward signal at each step; typically, this is -0.1 per step to encourage speed, plus a significant bonus (e.g., +100) upon reaching the goal.\n",
        "\n",
        "## **Policy Representation**\n",
        "\n",
        "In reinforcement learning, the agent's behavior is dictated by a \"policy,\" which essentially maps observed states to appropriate actions. For this specific problem, we adopt a straightforward **linear policy**. This implies that the action is calculated as a linear combination of the current state variables (position and velocity). This policy is parameterized by a weight matrix (in this case, a 2x1 vector) denoted as:\n",
        "\n",
        "$$θ = \\begin{bmatrix} \\theta_1 \\\\ \\theta_2 \\end{bmatrix}$$\n",
        "\n",
        "Given a state $s^T = [x, v]$, the action is computed through a dot product:\n",
        "\n",
        "$$π_θ = s^T θ = x \\cdot θ_1 + v \\cdot θ_2$$\n",
        "\n",
        "The parameters $θ_1$ and $θ_2$ quantify the influence of the car's position and velocity, respectively, on the chosen action. The central aim of our optimization process is to determine the specific values for $θ_1$ and $θ_2$ that result in the highest possible total reward accumulated over an entire episode.\n",
        "\n",
        "## **Optimization Strategy: LLM-Driven Policy Search**\n",
        "\n",
        "The fundamental objective is to identify the optimal policy parameters $π_θ$ that maximize the cumulative reward $R$ gathered over a complete episode, which consists of a sequence of steps from the start until termination (either reaching the goal or hitting the maximum step limit, e.g., 1000). This task is formally known as **Policy Search**. Mathematically, we seek to solve:\n",
        "$$ \\max_{θ} \\mathbb{E}\\left[ \\sum_{t=0}^{T} r_t \\right]$$\n",
        "\n",
        "where $r_t$ is the reward at timestep $t$ and $T$ is the episode length.\n",
        "\n",
        "To facilitate this optimization, we utilize a \"Replay Buffer.\" After each episode concludes, having been run with the current parameters $θ$, the total reward $R$ is calculated. This $(θ, R)$ pair is then stored in the buffer. We approach the relationship between the policy parameters $π_θ$ and the resulting reward $R$ as a **black-box function**. This means the optimizer, which is the LLM in our case, operates without explicit knowledge of the MountainCar environment's internal physics or reward structure. It only observes the input parameters $θ$ and the corresponding output reward $R$.\n",
        "\n",
        "## **LLM as the Optimizer**\n",
        "\n",
        "We harness the capabilities of the Gemini models to conduct this black-box optimization. The LLM is instructed via a prompt to function as an optimization assistant. The process begins with a \"warmup\" phase, where several episodes are run using randomly selected parameters $π_θ$. The resulting $(θ, R)$ pairs populate the Replay Buffer, providing initial data. Subsequently, the LLM is presented with a detailed prompt containing the optimization goal, the historical data from the Replay Buffer, output format instructions, and guidance on balancing exploration (trying novel parameters) versus exploitation (refining promising parameters), adapting this balance as the optimization progresses. Based on this prompt and the historical context (enabling in-context learning), the LLM proposes a new set of parameters $θ$ anticipated to yield improved rewards.\n",
        "\n",
        "<br />\n",
        "<p style=\"text-align:center;\">\n",
        "<img src=\"https://github.com/k-pratyush/props-llm-examples/blob/main/static/props_approach_overview.jpeg?raw=1\" alt=\"image\" width=350>\n",
        "</p>\n",
        "Figure (a): Optimization Approach\n",
        "\n",
        "<br />\n",
        "<br />\n",
        "\n",
        "The agent's policy is then updated with these suggested parameters, and one or more evaluation episodes are executed in the environment. The cumulative reward obtained from these evaluations is recorded, and the new $(π_θ, \\text{cumulative } R)$ pair is added to the Replay Buffer. This cycle of prompting the LLM, receiving parameter suggestions, evaluating the updated policy, and updating the buffer is repeated for a predetermined number of episodes (e.g., 400), allowing the LLM to iteratively refine the policy parameters towards optimality. The prompt design treats the task purely as optimizing an unknown function $f(θ_1, θ_2) = R$, guiding the LLM with hints on step size and search ranges but without revealing the underlying simulation details.\n",
        "\n",
        "## **Code Overview**\n",
        "\n",
        "The implementation follows a modular design. The **World** component (`MountaincarContinuousActionWorld`) wraps the standard Gymnasium environment, managing state transitions, reward calculations, and episode termination. The **Agent** component (`MountaincarContinuousActionLLMNumOptimAgent`) integrates the learning elements. It includes the **`LinearPolicy`** module, which stores the policy weights ($θ_1, θ_2$) and computes actions based on states. It also contains the **`EpisodeRewardBufferNoBias`** module, responsible for maintaining the Replay Buffer of (weights, reward) pairs. Finally, the **`LLMBrain`** module orchestrates all interactions with the LLM, including prompt generation using Jinja2 templates, API communication (handling both OpenAI and Gemini models), and parsing the LLM's responses to extract the suggested new parameters.\n",
        "\n",
        "## **Hyperparameters**\n",
        "\n",
        "Several hyperparameters govern the experiment's execution. `NUM_EPISODES` (e.g., 400) sets the total number of optimization iterations. `RENDER_MODE` controls environment visualization. `MAX_TRAJ_COUNT` (e.g., 1000) defines the Replay Buffer size, influencing the historical context available to the LLM. `MAX_TRAJ_LENGTH` (e.g., 1000) sets the maximum steps per episode. `LLM_MODEL_NAME` specifies the LLM used. `NUM_EVALUATION_EPISODES` (e.g., 20) determines how many runs are averaged to evaluate a new policy. `WARMUP_EPISODES` (e.g., 20) sets the number of initial random runs. `SEARCH_STD` (e.g., 1.0) provides a hint to the LLM regarding the step size for parameter exploration.\n",
        "\n",
        "## **Training Loop**\n",
        "<p style=\"text-align:center;\">\n",
        "<img src=\"https://raw.githubusercontent.com/intro-to-icl/intro-to-icl.github.io/refs/heads/master/static/images/mountaincar_loop.gif\" alt=\"image\" height=\"350\">\n",
        "</p>\n",
        "Figure (b): Code Overview\n",
        "\n",
        "<br />\n",
        "<br />\n",
        "\n",
        "The `run_training_loop` function orchestrates the process. It initializes the World and Agent components. It performs the initial warmup runs if necessary, populating the replay buffer. Then, it enters the main loop, iterating `NUM_EPISODES` times. In each iteration, it interacts with the LLM (`agent.train_policy`) to get updated policy parameters based on the replay buffer history. It then evaluates the performance of this new policy over `NUM_EVALUATION_EPISODES` (`agent.evaluate_policy`), calculates the cumulative reward, and adds the new (parameters, cumulative reward) pair back into the replay buffer. Logging occurs at each step.\n",
        "\n",
        "## **Output Structure**\n",
        "\n",
        "The training process generates structured logs. A main log directory contains subdirectories for each episode (`episode_*`) and potentially a `warmup/` directory. Each episode directory stores logs of evaluation trajectories, the parameters suggested by the LLM for that episode (`parameters.txt`), and the full LLM interaction including its reasoning (`parameters_reasoning.txt`). The final notebook cells typically include code for visualizing the learned policy in action and plotting the reward curve over episodes, illustrating the learning progress."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Df4h5KlfLtW"
      },
      "source": [
        "## **Before Running the Demo, Follow the Instructions Below**\n",
        "To run the full experiment:\n",
        "1. Ensure all dependencies are imported and installed.\n",
        "2.\tVisit Google AI Studio (https://aistudio.google.com/) to obtain your Gemini API key.\n",
        "3.\tOnce API key is generated, copy and paste it into the demo when prompted.\n",
        "\n",
        "#### **If having trouble creating an API Key, follow the link below for instructions:**\n",
        "* #### **[Instructions on How to Obtain a Gemini API Key](https://docs.google.com/document/d/17pgikIpvZCBgcRh4NNrcee-zp3Id0pU0vq2SG52KIGE/edit?usp=sharing)**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uH1ovM2YfLtY",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title **Install and Import Necessary Libraries**\n",
        "import re\n",
        "import os\n",
        "import time\n",
        "from decimal import Decimal\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "import traceback\n",
        "import numpy as np\n",
        "from IPython.display import display, update_display\n",
        "\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from jinja2 import Template\n",
        "from google import genai\n",
        "import getpass\n",
        "import ipywidgets as widgets\n",
        "import imageio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Setting Up Gemini Client**\n",
        "apikey = getpass.getpass(\"Enter your Gemini API Key: \")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Rjl9Zcyo7Mk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Choose a Model**\n",
        "model_dropdown = widgets.Dropdown(\n",
        "    options=[\n",
        "        (\"Gemini 2.5 Flash\", \"gemini-2.5-flash\"),\n",
        "        (\"Gemini 2.0 Flash\", \"gemini-2.0-flash\")\n",
        "    ],\n",
        "    description=\"Model:\",\n",
        "    value=\"gemini-2.5-flash\",\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "confirm_button = widgets.Button(\n",
        "    description=\"Confirm Selection\"\n",
        ")\n",
        "\n",
        "output = widgets.Output()\n",
        "\n",
        "model_name = None\n",
        "\n",
        "def on_confirm_click(b):\n",
        "    global model_name, batch_size\n",
        "\n",
        "    model_name = model_dropdown.value\n",
        "\n",
        "    with output:\n",
        "        output.clear_output()\n",
        "        print(f\"\\nSelected model: {model_name}\")\n",
        "\n",
        "confirm_button.on_click(on_confirm_click)\n",
        "\n",
        "display(model_dropdown, confirm_button, output)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "F4ALnApVAN9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGkrpEXCfLtZ"
      },
      "source": [
        "This cell introduces and lists the key hyperparameters that control the execution and behavior of the reinforcement learning experiment. Hyperparameters are settings that are not learned by the agent itself but are defined by the user before the training process begins. They significantly influence the learning process and the performance of the agent.\n",
        "\n",
        "`NUM_EPISODES` (e.g., 400): Defines the total number of optimization iterations or training episodes the agent will go through. A higher number allows for more learning but increases computation time.\n",
        "\n",
        "`RENDER_MODE` (e.g., None): Controls how the environment is visualized during execution. Options typically include 'human' (real-time window), 'rgb_array' (returns a pixel array, useful for recording), or None (no visualization, fastest for training).\n",
        "\n",
        "`MAX_TRAJ_COUNT` (e.g., 1000): Sets the maximum size of the Replay Buffer. This buffer stores (policy parameters, reward) pairs, and its size determines how much historical data the LLM has access to when making decisions.\n",
        "\n",
        "`MAX_TRAJ_LENGTH` (e.g., 1000): Specifies the maximum number of steps allowed in a single episode. If the agent doesn't reach a terminal state within these steps, the episode is truncated.\n",
        "\n",
        "`LLM_MODEL_NAME` (e.g., \"gemini-2.5-flash-preview-04-17\"): Specifies which Large Language Model will be used as the optimizer. The comment lists several compatible models from OpenAI and Google.\n",
        "NUM_EVALUATION_EPISODES (e.g., 20): Determines how many times a newly proposed policy is run in the environment to get an average measure of its performance. Averaging helps to reduce variance in the reward signal.\n",
        "\n",
        "`WARMUP_EPISODES` (e.g., 20): Sets the number of initial episodes run with randomly generated policy parameters. This \"warmup\" phase populates the Replay Buffer with some initial data points before the LLM starts optimizing.\n",
        "\n",
        "`SEARCH_STD` (e.g., 1.0): Provides a hint to the LLM regarding the standard deviation or step size it should consider when exploring new parameter values, especially during the initial exploration phase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qhMEUNAfLta",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title **Key Hyperparameters**\n",
        "NUM_EPISODES=200 # Total number of episodes to train for\n",
        "RENDER_MODE=None # Choose from 'human', 'rgb_array', or None\n",
        "MAX_TRAJ_COUNT=1000 # Maximum number of trajectories to store in buffer for prompt\n",
        "MAX_TRAJ_LENGTH=1000 # Maximum number of steps in a trajectory\n",
        "LLM_MODEL_NAME=\"gemini-2.0-flash\" # LLM for optimization, choose from \"o1-preview\", \"gpt-4o\", \"gemini-2.0-flash-exp\", \"gpt-4o-mini\", \"gemini-1.5-flash\", \"gemini-1.5-flash-8b\", \"gemini-1.5-pro\", \"gemini-2.5-pro-preview-05-06\", \"gemini-2.5-flash-preview-04-17\", \"o3-mini-2025-01-31\", \"gpt-4o-2024-11-20\", \"gpt-4o-2024-08-06\", \"claude-3-7-sonnet-20250219\"\n",
        "\n",
        "NUM_EVALUATION_EPISODES=20 # Number of episodes to generate agent rollouts for evaluation\n",
        "WARMUP_EPISODES=20 # Number of randomly generated initial episodes\n",
        "SEARCH_STD=1.0 # Step size for LLM to search for optimal parameters during exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYR4NMPlfLta"
      },
      "source": [
        "The below cell defines the template for the black box optimization prompt. The prompt template uses variables defined in the code for setting the number of parameters required to optimize, the global optimum of the function, step size, current step count and the history of (parameter, reward) tuples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKmx_o8jfLtb",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title **Black Box Optimization Prompt Example**\n",
        "LLM_SI_TEMPLATE_STRING = \"\"\"\n",
        "You are good global optimizer, helping me find the global maximum of a mathematical function f(params).\n",
        "I will give you the function evaluation and the current iteration number at each step.\n",
        "Your goal is to propose input values that efficiently lead us to the global maximum within a limited number of iterations (400).\n",
        "\n",
        "# Regarding the parameters **params**:\n",
        "**params** is an array of {{ rank }} float numbers.\n",
        "**params** values are in the range of [-6.0, 6.0] with 1 decimal place.\n",
        "\n",
        "# Here's how we'll interact:\n",
        "1. I will first provide MAX_STEPS (400) along with a few training examples.\n",
        "2. You will provide your response in the following exact format:\n",
        "    * Line 1: a new input 'params[0]: , params[1]: , params[2]: ,..., params[{{ rank - 1 }}]: ', aiming to maximize the function's value f(params).\n",
        "    Please propose params values in the range of [-6.0, 6.0], with 1 decimal place.\n",
        "    * Line 2: detailed explanation of why you chose that input.\n",
        "3. I will then provide the function's value f(params) at that point, and the current iteration.\n",
        "4. We will repeat steps 2-3 until we reach the maximum number of iterations.\n",
        "\n",
        "# Remember:\n",
        "1. **Do not propose previously seen params.**\n",
        "2. **The global optimum should be around {{ optimum }}.** If you are below that, this is just a local optimum. You should explore instead of exploiting.\n",
        "3. Search both positive and negative values. **During exploration, use search step size of {{ step_size }}**.\n",
        "\n",
        "\n",
        "Next, you will see examples of params and f(params) pairs.\n",
        "{{ episode_reward_buffer_string }}\n",
        "\n",
        "Now you are at iteration {{step_number}} out of 400. Please provide the results in the indicated format. Do not provide any additional texts.\"\"\"\n",
        "\n",
        "\n",
        "llm_si_template = Template(LLM_SI_TEMPLATE_STRING)\n",
        "llm_output_conversion_template = llm_si_template"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAhQnfn5fLtb"
      },
      "source": [
        "## **World**\n",
        "\n",
        "The `ContinualSpaceGeneralWorld` is a wrapper class over the Gymnasium environments to give standardized interface for the agents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9eUd-vPPfLtc",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title **MountainCarContinuous-v0**\n",
        "# https://gymnasium.farama.org/environments/classic_control/mountain_car_continuous/\n",
        "\n",
        "import gymnasium as gym\n",
        "\n",
        "\n",
        "class ContinualSpaceGeneralWorld():\n",
        "    def __init__(\n",
        "        self,\n",
        "        gym_env_name,\n",
        "        render_mode,\n",
        "        max_traj_length=1000,\n",
        "    ):\n",
        "        assert render_mode in [\"human\", \"rgb_array\", None]\n",
        "\n",
        "        if gym_env_name == \"gym_navigation:NavigationTrack-v0\":\n",
        "            self.env = gym.make(\n",
        "                gym_env_name,\n",
        "                render_mode=render_mode,\n",
        "                track_id=1,\n",
        "            )\n",
        "        elif gym_env_name == \"maze-sample-3x3-v0\":\n",
        "            self.env = gym.make(\n",
        "                gym_env_name,\n",
        "                enable_render=render_mode,\n",
        "            )\n",
        "        else:\n",
        "            self.env = gym.make(gym_env_name, render_mode=render_mode)\n",
        "        self.gym_env_name = gym_env_name\n",
        "        self.render_mode = render_mode\n",
        "        self.steps = 0\n",
        "        self.accu_reward = 0\n",
        "        self.max_traj_length = max_traj_length\n",
        "        if isinstance(self.env.action_space, gym.spaces.Discrete):\n",
        "            self.discretize = True\n",
        "        else:\n",
        "            self.discretize = False\n",
        "\n",
        "    def reset(self, new_reward=False):\n",
        "        \"\"\" This method resets the environment to its initial state.\n",
        "        If `new_reward` is True, it initializes the environment with a different reward structure.\n",
        "        \"\"\"\n",
        "        del self.env\n",
        "        if not new_reward:\n",
        "            self.env = gym.make(self.gym_env_name, render_mode=self.render_mode)\n",
        "        else:\n",
        "            self.env = gym.make(self.gym_env_name, render_mode=self.render_mode, healthy_reward=0)\n",
        "\n",
        "        state, _ = self.env.reset()\n",
        "        self.steps = 0\n",
        "        self.accu_reward = 0\n",
        "        return state\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        This method executes a step in the environment with the given action.\n",
        "        It updates the environment state, accumulates the reward, and checks if the episode is done.\n",
        "        \"\"\"\n",
        "        self.steps += 1\n",
        "        action = action[0]\n",
        "        state, reward, terminated, truncated, _ = self.env.step(action)\n",
        "        self.accu_reward += reward\n",
        "\n",
        "        if self.steps >= self.max_traj_length or terminated or truncated:\n",
        "            done = True\n",
        "        else:\n",
        "            done = False\n",
        "\n",
        "        return state, reward, done\n",
        "\n",
        "    def get_accu_reward(self):\n",
        "        \"\"\"\n",
        "        This method returns the accumulated reward for the current episode.\n",
        "        \"\"\"\n",
        "        return self.accu_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4dt8I0DfLte"
      },
      "source": [
        "## **Sub Modules**\n",
        "\n",
        "`EpisodeRewardBufferNoBias`: Store and manage collection of (policy parameters and reward) pairs, acting as the replay buffer.\n",
        "\n",
        "`LinearPolicy`: Implements a linear policy where the action is computed as a dot product of the state and weights, plus a bias term: $a = s^T W + b$.\n",
        "\n",
        "`LinearPolicyNoBias`: Implements a linear policy without a bias term: $a = s^T W$.\n",
        "\n",
        "`LLMBrain`: Coordinates with the LLM to get new parameters for the policy based on existing policy (parameter, reward) pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzC6ZOoqfLte",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title **Details**\n",
        "class EpisodeRewardBufferNoBias:\n",
        "    def __init__(self, max_size):\n",
        "        self.buffer = deque(maxlen=max_size)\n",
        "\n",
        "    def add(self, weights: np.ndarray, reward):\n",
        "        self.buffer.append((weights, reward))\n",
        "\n",
        "    def sort(self):\n",
        "        self.buffer = deque(sorted(self.buffer, key=lambda x: x[1], reverse=False), maxlen=self.buffer.maxlen)\n",
        "\n",
        "    def __str__(self):\n",
        "        buffer_table = \"Parameters | Reward\\n\"\n",
        "        for weights, reward in self.buffer:\n",
        "            buffer_table += f\"{weights.reshape(1, -1)} | {reward}\\n\"\n",
        "        return buffer_table\n",
        "\n",
        "    def load(self, folder):\n",
        "        # Find all episode files\n",
        "        all_files = [os.path.join(folder, x) for x in os.listdir(folder) if x.startswith('warmup_rollout')]\n",
        "        all_files.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
        "\n",
        "        # Load parameters from all episodes\n",
        "        for filename in all_files:\n",
        "            with open(filename, 'r') as f:\n",
        "                lines = f.readlines()\n",
        "                parameters = []\n",
        "                for line in lines:\n",
        "                    if \"parameter ends\" in line:\n",
        "                        break\n",
        "                    try:\n",
        "                        parameters.append([float(x) for x in line.split(',')])\n",
        "                    except:\n",
        "                        continue\n",
        "                parameters = np.array(parameters)\n",
        "\n",
        "                rewards = []\n",
        "                for line in lines:\n",
        "                    if \"Total reward\" in line:\n",
        "                        try:\n",
        "                            rewards.append(float(line.split()[-1]))\n",
        "                        except:\n",
        "                            continue\n",
        "                rewards_mean = np.mean(rewards)\n",
        "                self.add(parameters, rewards_mean)\n",
        "                f.close()\n",
        "        print(self)\n",
        "\n",
        "\n",
        "class LinearPolicy():\n",
        "    \"\"\"\n",
        "    Linear policy for continuous action space. The policy is represented as a (2,1) matrix of weights.\n",
        "    Next action is calculated as the dot product of the state and the weight matrix.\n",
        "    state.T * weight + bias -> action\n",
        "    (1,2) * (2,1) + (1,1) -> (1,1)\n",
        "    \"\"\"\n",
        "    def __init__(self, dim_states, dim_actions):\n",
        "\n",
        "        self.dim_states =dim_states\n",
        "        self.dim_actions = dim_actions\n",
        "\n",
        "        self.weight = np.random.rand(self.dim_states, self.dim_actions)\n",
        "        self.bias = np.random.rand(1, self.dim_actions)\n",
        "\n",
        "    def initialize_policy(self):\n",
        "        self.weight = np.round(np.random.normal(0., 3., size=(self.dim_states, self.dim_actions)), 1)\n",
        "        self.bias = np.round(np.random.normal(0., 3., size=(1, self.dim_actions)), 1)\n",
        "\n",
        "    def get_action(self, state):\n",
        "        state = state.T\n",
        "        return np.matmul(state, self.weight) + self.bias\n",
        "\n",
        "    def __str__(self):\n",
        "        output = \"Weights:\\n\"\n",
        "        for w in self.weight:\n",
        "            output += \", \".join([str(i) for i in w])\n",
        "            output += \"\\n\"\n",
        "\n",
        "        output += \"Bias:\\n\"\n",
        "        for b in self.bias:\n",
        "            output += \", \".join([str(i) for i in b])\n",
        "            output += \"\\n\"\n",
        "\n",
        "        return output\n",
        "\n",
        "    def update_policy(self, weight_and_bias_list):\n",
        "        if weight_and_bias_list is None:\n",
        "            return\n",
        "\n",
        "        weight_and_bias_list = np.array(weight_and_bias_list).reshape(self.dim_states + 1, self.dim_actions)\n",
        "        self.weight = np.array(weight_and_bias_list[:-1])\n",
        "        self.bias = np.expand_dims(np.array(weight_and_bias_list[-1]), axis=0)\n",
        "\n",
        "    def get_parameters(self):\n",
        "        parameters = np.concatenate((self.weight, self.bias), axis=0)\n",
        "        return parameters\n",
        "\n",
        "class LinearPolicyNoBias():\n",
        "    def __init__(self, dim_states, dim_actions):\n",
        "\n",
        "        self.dim_states = dim_states\n",
        "        self.dim_actions = dim_actions\n",
        "\n",
        "        self.weight = np.random.rand(self.dim_states, self.dim_actions)\n",
        "\n",
        "    def initialize_policy(self):\n",
        "        self.weight = np.round((np.random.rand(self.dim_states, self.dim_actions) - 0.5) * 6, 1)\n",
        "\n",
        "    def get_action(self, state):\n",
        "        state = state.T\n",
        "        return np.matmul(state, self.weight)\n",
        "\n",
        "    def __str__(self):\n",
        "        output = \"Weights:\\n\"\n",
        "        for w in self.weight:\n",
        "            output += \", \".join([str(i) for i in w])\n",
        "            output += \"\\n\"\n",
        "\n",
        "        return output\n",
        "\n",
        "    def update_policy(self, weight_and_bias_list):\n",
        "        if weight_and_bias_list is None:\n",
        "            return\n",
        "        self.weight = np.array(weight_and_bias_list)\n",
        "        self.weight = self.weight.reshape(-1)\n",
        "        for i in range(len(self.weight)):\n",
        "            self.weight[i] = Decimal(self.weight[i]).normalize()\n",
        "\n",
        "        self.weight = self.weight.reshape(\n",
        "            self.dim_states, self.dim_actions\n",
        "        )\n",
        "\n",
        "    def get_parameters(self):\n",
        "        return self.weight\n",
        "\n",
        "\n",
        "class LLMBrain:\n",
        "    def __init__(\n",
        "        self,\n",
        "        llm_si_template: Template,\n",
        "        llm_output_conversion_template: Template,\n",
        "        llm_model_name: str,\n",
        "    ):\n",
        "        self.llm_si_template = llm_si_template\n",
        "        self.llm_output_conversion_template = llm_output_conversion_template\n",
        "        self.llm_conversation = []\n",
        "        assert llm_model_name in [\n",
        "            \"gemini-2.0-flash\"\n",
        "        ]\n",
        "        self.llm_model_name = llm_model_name\n",
        "        if \"gemini\" in llm_model_name:\n",
        "            self.model_group = \"gemini\"\n",
        "            self.client = genai.Client(api_key=apikey)\n",
        "        elif \"claude\" in llm_model_name:\n",
        "            self.model_group = \"anthropic\"\n",
        "            self.client = anthropic.Client(api_key=os.environ[\"ANTHROPIC_API_KEY\"])\n",
        "        else:\n",
        "            self.model_group = \"openai\"\n",
        "            self.client = OpenAI()\n",
        "\n",
        "    def reset_llm_conversation(self):\n",
        "        self.llm_conversation = []\n",
        "\n",
        "    def add_llm_conversation(self, text, role):\n",
        "        if self.model_group == \"openai\":\n",
        "            self.llm_conversation.append({\"role\": role, \"content\": text})\n",
        "        elif self.model_group == \"anthropic\":\n",
        "            self.llm_conversation.append({\"role\": role, \"content\": text})\n",
        "        else:\n",
        "            self.llm_conversation.append({\"role\": role, \"parts\": text})\n",
        "\n",
        "    def query_llm(self):\n",
        "        for attempt in range(10):\n",
        "            try:\n",
        "                if self.model_group == \"openai\":\n",
        "                    completion = self.client.chat.completions.create(\n",
        "                        model=self.llm_model_name,\n",
        "                        messages=self.llm_conversation,\n",
        "                    )\n",
        "                    response = completion.choices[0].message.content\n",
        "                elif self.model_group == \"anthropic\":\n",
        "                    message = self.client.messages.create(\n",
        "                        model=self.llm_model_name,\n",
        "                        messages=self.llm_conversation,\n",
        "                        max_tokens=1024,\n",
        "                    )\n",
        "                    response = message.content[0].text\n",
        "                else:\n",
        "                    prompt = self.llm_conversation[-1][\"parts\"]\n",
        "                    response = self.client.models.generate_content(\n",
        "                        model=model_name,\n",
        "                        contents=[prompt]\n",
        "                    )\n",
        "                    response = response.text\n",
        "            except Exception as e:\n",
        "                print(f\"Error: {e}\")\n",
        "                print(\"Retrying...\")\n",
        "                if attempt == 9:\n",
        "                    raise Exception(\"Failed\")\n",
        "                else:\n",
        "                    print(\"Waiting for 60 seconds before retrying...\")\n",
        "                    time.sleep(60)\n",
        "\n",
        "            if self.model_group == \"openai\":\n",
        "                # add the response to self.llm_conversation\n",
        "                self.add_llm_conversation(response, \"assistant\")\n",
        "            else:\n",
        "                self.add_llm_conversation(response, \"model\")\n",
        "\n",
        "            return response\n",
        "\n",
        "\n",
        "    def parse_parameters(self, parameters_string):\n",
        "        new_parameters_list = []\n",
        "\n",
        "        # Update the Q-table based on the new Q-table\n",
        "        for row in parameters_string.split(\"\\n\"):\n",
        "            if row.strip().strip(\",\"):\n",
        "                try:\n",
        "                    parameters_row = [\n",
        "                        float(x.strip().strip(\",\")) for x in row.split(\",\")\n",
        "                    ]\n",
        "                    new_parameters_list.append(parameters_row)\n",
        "                except Exception as e:\n",
        "                    print(e)\n",
        "\n",
        "        return new_parameters_list\n",
        "\n",
        "\n",
        "    def llm_update_parameters_num_optim(\n",
        "        self,\n",
        "        episode_reward_buffer,\n",
        "        parse_parameters,\n",
        "        step_number,\n",
        "        rank=None,\n",
        "        optimum=None,\n",
        "        search_step_size=0.1,\n",
        "        actions=None,\n",
        "    ):\n",
        "        self.reset_llm_conversation()\n",
        "\n",
        "        system_prompt = self.llm_si_template.render(\n",
        "            {\n",
        "                \"episode_reward_buffer_string\": str(episode_reward_buffer),\n",
        "                \"step_number\": str(step_number),\n",
        "                \"rank\": rank,\n",
        "                \"optimum\": str(optimum),\n",
        "                \"step_size\": str(search_step_size),\n",
        "                \"actions\": actions,\n",
        "            }\n",
        "        )\n",
        "\n",
        "        self.add_llm_conversation(system_prompt, \"user\")\n",
        "\n",
        "        api_start_time = time.time()\n",
        "        new_parameters_with_reasoning = self.query_llm()\n",
        "        api_time = time.time() - api_start_time\n",
        "        new_parameters_list = parse_parameters(new_parameters_with_reasoning)\n",
        "\n",
        "        return (\n",
        "            new_parameters_list,\n",
        "            \"system:\\n\"\n",
        "            + system_prompt\n",
        "            + \"\\n\\n\\nLLM:\\n\"\n",
        "            + new_parameters_with_reasoning,\n",
        "            api_time,\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1JlxxvbfLtf"
      },
      "source": [
        "## **Agent**\n",
        "\n",
        "The below cell defines the core agent wrapper. It is responsibe for managing the policy, interacting with the world and coordinating with the LLMBrain to learn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ld4xuBudfLtf",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title **Core Agent Wrapper**\n",
        "class LLMNumOptimAgent:\n",
        "    def __init__(\n",
        "        self,\n",
        "        logdir,\n",
        "        dim_action,\n",
        "        dim_state,\n",
        "        max_traj_count,\n",
        "        max_traj_length,\n",
        "        llm_si_template,\n",
        "        llm_output_conversion_template,\n",
        "        llm_model_name,\n",
        "        num_evaluation_episodes,\n",
        "        bias,\n",
        "        optimum,\n",
        "        search_step_size,\n",
        "    ):\n",
        "        self.start_time = time.process_time()\n",
        "        self.api_call_time = 0\n",
        "        self.total_steps = 0\n",
        "        self.total_episodes = 0\n",
        "        self.dim_action = dim_action\n",
        "        self.dim_state = dim_state\n",
        "        self.bias = bias\n",
        "        self.optimum = optimum\n",
        "        self.search_step_size = search_step_size\n",
        "\n",
        "        if not self.bias:\n",
        "            param_count = dim_action * dim_state\n",
        "        else:\n",
        "            param_count = dim_action * dim_state + dim_action\n",
        "        self.rank = param_count\n",
        "\n",
        "        # Initialize the policy and replay buffer\n",
        "        if not self.bias:\n",
        "            self.policy = LinearPolicyNoBias(\n",
        "                dim_actions=dim_action, dim_states=dim_state\n",
        "            )\n",
        "        else:\n",
        "            self.policy = LinearPolicy(dim_actions=dim_action, dim_states=dim_state)\n",
        "        self.replay_buffer = EpisodeRewardBufferNoBias(max_size=max_traj_count)\n",
        "        self.llm_brain = LLMBrain(\n",
        "            llm_si_template, llm_output_conversion_template, llm_model_name\n",
        "        )\n",
        "        self.logdir = logdir\n",
        "        self.num_evaluation_episodes = num_evaluation_episodes\n",
        "        self.training_episodes = 0\n",
        "\n",
        "        if self.bias:\n",
        "            self.dim_state += 1\n",
        "\n",
        "    def rollout_episode(self, world, logging_file, record=True):\n",
        "        \"\"\"Simulates an episode in the environment using the current policy.\"\"\"\n",
        "        state = world.reset()\n",
        "        state = np.expand_dims(state, axis=0)\n",
        "        logging_file.write(\n",
        "            f\"{', '.join([str(x) for x in self.policy.get_parameters().reshape(-1)])}\\n\"\n",
        "        )\n",
        "        logging_file.write(f\"parameter ends\\n\\n\")\n",
        "        logging_file.write(f\"state | action | reward\\n\")\n",
        "        done = False\n",
        "        step_idx = 0\n",
        "        while not done:\n",
        "            action = self.policy.get_action(state.T)\n",
        "            action = np.reshape(action, (1, self.dim_action))\n",
        "            if world.discretize:\n",
        "                action = np.argmax(action)\n",
        "                action = np.array([action])\n",
        "            next_state, reward, done = world.step(action)\n",
        "            logging_file.write(f\"{state.T[0]} | {action[0]} | {reward}\\n\")\n",
        "            state = next_state\n",
        "            step_idx += 1\n",
        "            self.total_steps += 1\n",
        "        logging_file.write(f\"Total reward: {world.get_accu_reward()}\\n\")\n",
        "        self.total_episodes += 1\n",
        "        if record:\n",
        "            self.replay_buffer.add(\n",
        "                self.policy.get_parameters(), world.get_accu_reward()\n",
        "            )\n",
        "        return world.get_accu_reward()\n",
        "\n",
        "    def random_warmup(self, world, logdir, num_episodes):\n",
        "        for episode in range(num_episodes):\n",
        "            self.policy.initialize_policy()\n",
        "            # Run the episode and collect the trajectory\n",
        "            print(f\"Rolling out warmup episode {episode}...\")\n",
        "            logging_filename = f\"{logdir}/warmup_rollout_{episode}.txt\"\n",
        "            logging_file = open(logging_filename, \"w\")\n",
        "            result = self.rollout_episode(world, logging_file)\n",
        "            print(f\"Result: {result}\")\n",
        "\n",
        "    def train_policy(self, world, logdir):\n",
        "        \"\"\"Core method to train single iteration of the policy using LLM optimization.\"\"\"\n",
        "\n",
        "        def parse_parameters(input_text):\n",
        "            # This regex looks for integers or floating-point numbers (including optional sign)\n",
        "            s = input_text.split(\"\\n\")[0]\n",
        "            print(\"response:\", s)\n",
        "            pattern = re.compile(r\"params\\[(\\d+)\\]:\\s*([+-]?\\d+(?:\\.\\d+)?)\")\n",
        "            matches = pattern.findall(s)\n",
        "\n",
        "            # Convert matched strings to float (or int if you prefer to differentiate)\n",
        "            results = []\n",
        "            for match in matches:\n",
        "                results.append(float(match[1]))\n",
        "            print(results)\n",
        "            assert len(results) == self.rank\n",
        "            return np.array(results).reshape(-1)\n",
        "\n",
        "        def str_nd_examples(replay_buffer: EpisodeRewardBufferNoBias, n):\n",
        "\n",
        "            all_parameters = []\n",
        "            for weights, reward in replay_buffer.buffer:\n",
        "                parameters = weights\n",
        "                all_parameters.append((parameters.reshape(-1), reward))\n",
        "\n",
        "            text = \"\"\n",
        "            for parameters, reward in all_parameters:\n",
        "                l = \"\"\n",
        "                for i in range(n):\n",
        "                    l += f\"params[{i}]: {parameters[i]:.5g}; \"\n",
        "                fxy = reward\n",
        "                l += f\"f(params): {fxy:.2f}\\n\"\n",
        "                text += l\n",
        "            return text\n",
        "\n",
        "        # Update the policy using llm_brain, q_table and replay_buffer\n",
        "        print(\"Updating the policy...\")\n",
        "        new_parameter_list, reasoning, api_time = self.llm_brain.llm_update_parameters_num_optim(\n",
        "            str_nd_examples(self.replay_buffer, self.rank),\n",
        "            parse_parameters,\n",
        "            self.training_episodes,\n",
        "            self.rank,\n",
        "            self.optimum,\n",
        "            self.search_step_size\n",
        "        )\n",
        "        self.api_call_time += api_time\n",
        "\n",
        "        print(self.policy.get_parameters().shape)\n",
        "        print(new_parameter_list.shape)\n",
        "        self.policy.update_policy(new_parameter_list)\n",
        "        print(self.policy.get_parameters().shape)\n",
        "        logging_q_filename = f\"{logdir}/parameters.txt\"\n",
        "        logging_q_file = open(logging_q_filename, \"w\")\n",
        "        logging_q_file.write(str(self.policy))\n",
        "        logging_q_file.close()\n",
        "        q_reasoning_filename = f\"{logdir}/parameters_reasoning.txt\"\n",
        "        q_reasoning_file = open(q_reasoning_filename, \"w\")\n",
        "        q_reasoning_file.write(reasoning)\n",
        "        q_reasoning_file.close()\n",
        "        print(\"Policy updated!\")\n",
        "\n",
        "        # Run the episode and collect the trajectory\n",
        "        print(f\"Rolling out episode {self.training_episodes}...\")\n",
        "        logging_filename = f\"{logdir}/training_rollout.txt\"\n",
        "        logging_file = open(logging_filename, \"w\")\n",
        "        results = []\n",
        "        for idx in range(self.num_evaluation_episodes):\n",
        "            if idx == 0:\n",
        "                result = self.rollout_episode(world, logging_file, record=False)\n",
        "            else:\n",
        "                result = self.rollout_episode(world, logging_file, record=False)\n",
        "            results.append(result)\n",
        "        print(f\"Results: {results}\")\n",
        "        result = np.mean(results)\n",
        "        self.replay_buffer.add(new_parameter_list, result)\n",
        "\n",
        "        self.training_episodes += 1\n",
        "\n",
        "        _cpu_time = time.process_time() - self.start_time\n",
        "        _api_time = self.api_call_time\n",
        "        _total_episodes = self.total_episodes\n",
        "        _total_steps = self.total_steps\n",
        "        _total_reward = result\n",
        "        return _cpu_time, _api_time, _total_episodes, _total_steps, _total_reward\n",
        "\n",
        "\n",
        "    def evaluate_policy(self, world, logdir):\n",
        "        results = []\n",
        "        for idx in range(self.num_evaluation_episodes):\n",
        "            logging_filename = f\"{logdir}/evaluation_rollout_{idx}.txt\"\n",
        "            logging_file = open(logging_filename, \"w\")\n",
        "            result = self.rollout_episode(world, logging_file, record=False)\n",
        "            results.append(result)\n",
        "        return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "en2skYbmfLtg"
      },
      "source": [
        "The below cell orchestrates the entire training process from initialization to completion. The `run_training_loop` function starts with initialization the world, and the agent instances. Then, it creates a set of warmup episodes to pass in as initial replay buffer to the optimizer. The code then runs the training loop for specified number of episodes and optimizes the policy parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJdmb5WLfLtg",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title **Training Loop**\n",
        "def run_training_loop(\n",
        "    num_episodes,\n",
        "    gym_env_name,\n",
        "    render_mode,\n",
        "    logdir,\n",
        "    dim_actions,\n",
        "    dim_states,\n",
        "    max_traj_count,\n",
        "    max_traj_length,\n",
        "    llm_model_name,\n",
        "    num_evaluation_episodes,\n",
        "    warmup_episodes,\n",
        "    warmup_dir,\n",
        "    bias=None,\n",
        "    rank=None,\n",
        "    optimum=100,\n",
        "    search_step_size=SEARCH_STD,\n",
        "):\n",
        "    world = ContinualSpaceGeneralWorld(\n",
        "        gym_env_name,\n",
        "        render_mode,\n",
        "        max_traj_length,\n",
        "    )\n",
        "\n",
        "    agent = LLMNumOptimAgent(\n",
        "        logdir,\n",
        "        dim_actions,\n",
        "        dim_states,\n",
        "        max_traj_count,\n",
        "        max_traj_length,\n",
        "        llm_si_template,\n",
        "        llm_output_conversion_template,\n",
        "        llm_model_name,\n",
        "        num_evaluation_episodes,\n",
        "        bias,\n",
        "        optimum,\n",
        "        search_step_size,\n",
        "    )\n",
        "    print('init done')\n",
        "\n",
        "    if not warmup_dir:\n",
        "        warmup_dir = f\"{logdir}/warmup\"\n",
        "        os.makedirs(warmup_dir, exist_ok=True)\n",
        "        agent.random_warmup(world, warmup_dir, warmup_episodes)\n",
        "    else:\n",
        "        agent.replay_buffer.load(warmup_dir)\n",
        "\n",
        "    overall_log_file = open(f\"{logdir}/overall_log.txt\", \"w\")\n",
        "    overall_log_file.write(\"Iteration, CPU Time, API Time, Total Episodes, Total Steps, Total Reward\\n\")\n",
        "    overall_log_file.flush()\n",
        "    for episode in range(num_episodes):\n",
        "        print(\"-----------------------------------------------------------------------------------------------------------\")\n",
        "        print(f\"Episode: {episode}\")\n",
        "        # create log dir\n",
        "        curr_episode_dir = f\"{logdir}/episode_{episode}\"\n",
        "        print(f\"Creating log directory: {curr_episode_dir}\")\n",
        "        os.makedirs(curr_episode_dir, exist_ok=True)\n",
        "\n",
        "        for trial_idx in range(5):\n",
        "            try:\n",
        "                cpu_time, api_time, total_episodes, total_steps, total_reward = agent.train_policy(world, curr_episode_dir)\n",
        "                overall_log_file.write(f\"{episode + 1}, {cpu_time}, {api_time}, {total_episodes}, {total_steps}, {total_reward}\\n\")\n",
        "                overall_log_file.flush()\n",
        "                print(f\"{trial_idx + 1}th trial attempt succeeded in training\")\n",
        "                break\n",
        "            except Exception as e:\n",
        "                print(\n",
        "                    f\"{trial_idx + 1}th trial attempt failed with error in training: {e}\"\n",
        "                )\n",
        "                traceback.print_exc()\n",
        "\n",
        "                if trial_idx == 4:\n",
        "                    print(f\"All {trial_idx + 1} trials failed. Train terminated\")\n",
        "                    exit(1)\n",
        "                continue\n",
        "    overall_log_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7VvlW34fLtg",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title **Run the Training Loop**\n",
        "run_training_loop(\n",
        "    num_episodes=NUM_EPISODES,\n",
        "    gym_env_name=\"MountainCarContinuous-v0\",\n",
        "    render_mode=RENDER_MODE,\n",
        "    logdir=\"logs/mountaincar_continuous_tutorial\",\n",
        "    dim_actions=1,\n",
        "    dim_states=2,\n",
        "    max_traj_count=MAX_TRAJ_COUNT,\n",
        "    max_traj_length=MAX_TRAJ_LENGTH,\n",
        "    llm_model_name=LLM_MODEL_NAME,\n",
        "    num_evaluation_episodes=NUM_EVALUATION_EPISODES,\n",
        "    warmup_episodes=WARMUP_EPISODES,\n",
        "    warmup_dir=None,\n",
        "    bias=None,\n",
        "    rank=None,\n",
        "    optimum=100,\n",
        "    search_step_size=SEARCH_STD,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qeck4hfHfLtg",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title **Policy Visualization**\n",
        "EPISODE_DIR = \"episode_1\"\n",
        "LOGDIR = \"logs/mountaincar_continuous_tutorial\"\n",
        "\n",
        "def run_policy(\n",
        "    render_mode=\"rgb_array\",\n",
        "    logdir=LOGDIR,\n",
        "    episode_dir=EPISODE_DIR,\n",
        "    save_gif=False,\n",
        "    fast_mode=True,\n",
        "):\n",
        "\n",
        "    if fast_mode:\n",
        "        sleep_time = 0.01\n",
        "        display_interval = 2\n",
        "        downscale_factor = 1\n",
        "    else:\n",
        "        sleep_time = 0.05\n",
        "        display_interval = 1\n",
        "        downscale_factor = 1\n",
        "\n",
        "    world = ContinualSpaceGeneralWorld(\n",
        "        \"MountainCarContinuous-v0\",\n",
        "        render_mode=render_mode,\n",
        "        max_traj_length=MAX_TRAJ_LENGTH,\n",
        "    )\n",
        "\n",
        "    agent = LLMNumOptimAgent(\n",
        "        logdir,\n",
        "        dim_action=1,\n",
        "        dim_state=2,\n",
        "        max_traj_count=MAX_TRAJ_COUNT,\n",
        "        max_traj_length=MAX_TRAJ_LENGTH,\n",
        "        llm_si_template=llm_si_template,\n",
        "        llm_output_conversion_template=llm_output_conversion_template,\n",
        "        llm_model_name=LLM_MODEL_NAME,\n",
        "        num_evaluation_episodes=NUM_EVALUATION_EPISODES,\n",
        "        bias=None,\n",
        "        optimum=100,\n",
        "        search_step_size=SEARCH_STD,\n",
        "    )\n",
        "\n",
        "    param_path = os.path.join(logdir, episode_dir, \"parameters.txt\")\n",
        "    if not os.path.exists(param_path):\n",
        "        raise FileNotFoundError(f\"Could not find parameter file at {param_path}\")\n",
        "\n",
        "    with open(param_path, \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "        weights = [float(x.strip()) for x in lines[1:] if x.strip()]\n",
        "    agent.policy.update_policy([weights])\n",
        "\n",
        "    raw_state = world.reset()\n",
        "    state = np.expand_dims(raw_state, axis=0)\n",
        "    done = False\n",
        "    step_idx = 0\n",
        "    frames = [] if save_gif else None\n",
        "\n",
        "    def get_frame():\n",
        "        try:\n",
        "            img = world.env.render()\n",
        "        except Exception:\n",
        "            try:\n",
        "                img = world.env.render(mode=\"rgb_array\")\n",
        "            except Exception:\n",
        "                img = None\n",
        "        if img is None:\n",
        "            try:\n",
        "                img = world.render()\n",
        "            except Exception:\n",
        "                img = None\n",
        "        return img\n",
        "\n",
        "    img = get_frame()\n",
        "    if img is None:\n",
        "        raise RuntimeError(\"Unable to obtain an RGB frame from the environment.\")\n",
        "\n",
        "    if not isinstance(img, np.ndarray):\n",
        "        img = np.array(img)\n",
        "    if downscale_factor > 1:\n",
        "        img = img[::downscale_factor, ::downscale_factor]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(6, 6))\n",
        "    im = ax.imshow(img)\n",
        "    ax.axis(\"off\")\n",
        "    text_box = ax.text(5, 15, '', color='white', fontsize=12, backgroundcolor='black')\n",
        "    display_id = f\"policy_vis_{int(time.time()*1000)}\"\n",
        "    display(fig, display_id=display_id)\n",
        "\n",
        "    while not done:\n",
        "        img = get_frame()\n",
        "        if img is None:\n",
        "            print(f\"Warning: got None frame at step {step_idx}\")\n",
        "            break\n",
        "\n",
        "        if not isinstance(img, np.ndarray):\n",
        "            img = np.array(img)\n",
        "        if downscale_factor > 1:\n",
        "            img = img[::downscale_factor, ::downscale_factor]\n",
        "\n",
        "        im.set_data(img)\n",
        "        text_box.set_text(f\"Step {step_idx}\")\n",
        "\n",
        "        if step_idx % display_interval == 0:\n",
        "            update_display(fig, display_id=display_id)\n",
        "\n",
        "        if save_gif:\n",
        "            frames.append(img.copy())\n",
        "\n",
        "        try:\n",
        "            action = agent.policy.get_action(state.T)\n",
        "        except Exception as e:\n",
        "            print(f\"Error getting action at step {step_idx}: {e}\")\n",
        "            break\n",
        "\n",
        "        action_np = np.asarray(action)\n",
        "        if action_np.ndim == 0:\n",
        "            action_np = np.reshape(action_np, (1, 1))\n",
        "        elif action_np.ndim == 1:\n",
        "            action_np = action_np.reshape(1, -1)\n",
        "        elif action_np.ndim >= 2:\n",
        "            action_np = action_np[0:1, :]\n",
        "\n",
        "        try:\n",
        "            next_state, reward, done = world.step(action_np)\n",
        "        except Exception as e:\n",
        "            try:\n",
        "                next_state, reward, done = world.step(np.squeeze(action_np))\n",
        "            except Exception as e2:\n",
        "                print(\"Error stepping env:\", e, e2)\n",
        "                break\n",
        "\n",
        "        state = np.expand_dims(next_state, 0) if next_state.ndim == 1 else next_state\n",
        "        step_idx += 1\n",
        "        time.sleep(sleep_time)\n",
        "\n",
        "    plt.close(fig)\n",
        "\n",
        "    if save_gif and frames:\n",
        "      gif_dir = \"output_gif\"\n",
        "      os.makedirs(gif_dir, exist_ok=True)  # <-- create folder if it doesn't exist\n",
        "      gif_path = os.path.join(gif_dir, f\"{episode_dir}.gif\")\n",
        "      imageio.mimsave(gif_path, frames, fps=max(1, int(1 / sleep_time)))\n",
        "      print(f\"Saved visualization → {gif_path}\")\n",
        "\n",
        "    print(f\"Finished visualization.\")\n",
        "\n",
        "run_policy(save_gif=True, fast_mode=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkmFo94UfLth",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title **Episodes Reward Summary**\n",
        "all_succ = []\n",
        "root_folder = \"logs/mountaincar_continuous_tutorial\"\n",
        "\n",
        "all_folders = [os.path.join(root_folder, x) for x in os.listdir(root_folder) if 'episode' in x]\n",
        "all_folders.sort(key=lambda x: int(x.split('_')[-1]))\n",
        "for folder in all_folders:\n",
        "    # read all text files in the folder. Read the last line of each file and extract the total reward. The last line looks like this: \"Total reward: -157.0\"\n",
        "    rewards_succ = []\n",
        "    rewards_fail = []\n",
        "    for filename in os.listdir(folder):\n",
        "        if 'training' in filename:\n",
        "            with open(os.path.join(folder, filename), 'r') as f:\n",
        "                lines = f.readlines()\n",
        "                rewards = []\n",
        "                for line in lines:\n",
        "                    if 'Total reward' in line:\n",
        "                        total_reward = float(line.split()[-1])\n",
        "                        rewards.append(total_reward)\n",
        "                if rewards:  # prevent empty list\n",
        "                    rewards_succ.append(np.mean(rewards))\n",
        "\n",
        "    # Evaluation files (separate loop, not \"else\")\n",
        "    curr_episode_rewards = []\n",
        "    for filename in os.listdir(folder):\n",
        "        if 'evaluation' in filename:\n",
        "            with open(os.path.join(folder, filename), 'r') as f:\n",
        "                lines = f.readlines()\n",
        "                curr_rewards = []\n",
        "                for line in lines[1:]:\n",
        "                    try:\n",
        "                        curr_rewards.append(float(line.split('|')[-1]))\n",
        "                    except ValueError:\n",
        "                        continue  # skip malformed lines\n",
        "                if curr_rewards:\n",
        "                    curr_episode_rewards.append(np.sum(curr_rewards))\n",
        "    if curr_episode_rewards:\n",
        "        rewards_succ.append(np.mean(curr_episode_rewards))\n",
        "\n",
        "    # print(rewards_succ)\n",
        "    # print(rewards_fail)\n",
        "\n",
        "\n",
        "    all_rewards = rewards_succ + rewards_fail\n",
        "\n",
        "    print(\"Average reward for all episodes:\", np.mean(all_rewards))\n",
        "    print(\"Standard deviation of reward for all episodes:\", np.std(all_rewards))\n",
        "    print(\"------------------\")\n",
        "\n",
        "\n",
        "    if 'descent' in root_folder:\n",
        "        all_succ.append(1500 - np.mean(all_rewards))\n",
        "    else:\n",
        "        all_succ.append(np.mean(all_rewards))\n",
        "# print(all_succ)\n",
        "print(max(all_succ))\n",
        "for i in range(len(all_succ)):\n",
        "    if all_succ[i] >= max(all_succ) * 0.95:\n",
        "        print(i + 1)\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Reward Curve**\n",
        "episodes = list(range(1, len(all_succ) + 1))\n",
        "\n",
        "# Creating the plot\n",
        "plt.figure(figsize=(15, 6))\n",
        "\n",
        "# Plot the main line with better styling\n",
        "plt.plot(episodes, all_succ, linewidth=2, color='red', label=\"MountainCar LLM Optimization Tutorial - Learning Curve\")\n",
        "\n",
        "plt.xlabel(\"Episodes\", fontsize=12)\n",
        "plt.ylabel(\"Reward\", fontsize=12)\n",
        "plt.legend(fontsize=10)\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.tight_layout(pad=3.0)\n",
        "\n",
        "os.makedirs('results_curves', exist_ok=True)\n",
        "plt.savefig(f'results_curves/{root_folder.split(\"/\")[1]}.png', dpi=300)\n",
        "%matplotlib inline\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PGpVr0sLwstO",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `Reward Curve` plotted over the training episodes illustrates the progressive improvement of the learned policy. Typically, in early stages of training, rewards fluctuate due to random initialization and broad exploration. As the LLM repeatedly refines the parameters, the curve begins to rise steadily, indicating that the model is discovering more effective control strategies. Eventually, the reward curve stabilizes, reflecting convergence toward a locally optimal linear policy for the environment."
      ],
      "metadata": {
        "id": "utQ7jUNAOVVl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Summary**\n",
        "This demo illustrates how Large Language Models, within the Prompted Policy Search (ProPS) framework, can act as central reasoning engines for reinforcement learning by jointly leveraging linguistic and numerical cues. Rather than relying on gradient-based updates or analytic models of the environment, the LLM incrementally improves a linear control policy for the MountainCar environment using only the replayed history of policy parameters and their resulting rewards. By embedding the optimization goal, the replay buffer contents, and instructions for balancing exploration and exploitation directly into the prompt, the LLM is encouraged to infer the relationship between policy weights and episode returns—treating the environment as an opaque black-box function. Over successive episodes, the model uses in-context learning to identify promising parameter directions, refining its estimates purely from observed outcomes and natural-language guidance.\n",
        "\n",
        "Beyond the MountainCar example, the ProPS methodology generalizes to a broad class of reinforcement learning problems. Because policy updates are communicated and reasoned about through prompt structure rather than backpropagation or explicit gradient computation, LLMs can flexibly integrate reward feedback, natural-language constraints, and high-level instructions in a unified optimization loop. This demonstration highlights how ProPS enables LLMs to participate directly in policy search, showing how linguistic reasoning can complement numerical reasoning to drive self-improvement in simulated control environments.\n",
        "\n",
        "## **Conclusion**\n",
        "This demonstration shows that LLMs can serve as effective policy optimizers in reinforcement learning when guided through structured prompts, capturing the core insight of the ProPS framework. Without access to transition dynamics, reward functions, or gradient information, the LLLM learns to propose increasingly effective policy parameters by interpreting replay-buffer histories and adapting its search strategy over time. Although the LLM is not performing classical RL computation internally, its ability to approximate improved policy updates based solely on contextual feedback validates the ProPS claim that language-based reasoning can drive policy search.\n",
        "\n",
        "ProPS provides an interpretable and flexible mechanism for integrating LLMs into reinforcement-learning loops. Through carefully designed prompts and iterative interaction with the environment, the LLM progressively discovers high-reward policies—demonstrating that linguistic and numerical reasoning, when fused inside a single model, offer a practical pathway for optimizing control behavior in continuous-action domains.\n",
        "\n",
        "<br>\n",
        "\n",
        "For more details, please refer to the [ProPS Project Page](https://props-llm.github.io/) and the associated research paper.\n",
        "\n",
        "\n",
        "## **References**\n",
        "Zhou, Y., Grover, S., El Mistiri, M., Kalirathnam, K., Kerhalkar, P., Mishra, S., Kumar, N., Gaurav, S., Aran, O., & Ben Amor, H. (2025). Prompted Policy Search: Reinforcement Learning through Linguistic and Numerical Reasoning in LLMs, Advances in Neural Information Processing Systems (NeurIPS 2025)."
      ],
      "metadata": {
        "id": "gtMHW9DSwfyr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h3czh6HLItFK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}