{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction to In-Context Learning Demo: PID (Proportional-Integral-Derivative) Tuning**\n",
        "This notebook presents a systematic demonstration of how Large Language Models (LLMs) can perform PID controller tuning through in-context learning (ICL). Motivated by recent advances in LLM-based numerical optimization, this experiment investigates whether providing Gemini with examples of suboptimally tuned controllers enables it to refine tuning parameters and improve closed-loop performance. Performance is quantified through three standard metrics: Mean Square Error (MSE) between the controlled variable ($y$) and setpoint ($r$), variance in the controlled variable ($y_{var}$), and variance in the manipulated variable ($u_{var}$).\n",
        "\n",
        "The objective is to demonstrate that LLMs, when equipped with appropriate in-context examples, can iteratively reason through control design problems and function as effective control engineering assistants.\n",
        "\n",
        "## **Tutorial Overview**\n",
        "\n",
        "This interactive demonstration guides you through the following methodological steps:\n",
        "\n",
        "1. **Controller Construction**: Implement three classical controller structures (PI, PID, PID with filter) for a first-order time-invariant dynamic system with delay.\n",
        "2. **Example Curation**: Prepare representative tuning examples to facilitate in-context learning.\n",
        "3. **Prompt Engineering**: Construct ICL prompts to instantiate a tuning assistant.\n",
        "4. **Iterative Optimization**: Query the LLM (Gemini 2.0/2.5 Flash) to progressively refine controller parameters.\n",
        "\n",
        "The resulting framework illustrates the potential of LLMs to serve as optimization assistants in control systems engineering.\n",
        "\n",
        "---\n",
        "\n",
        "# **Background**\n",
        "\n",
        "This demonstration focuses on leveraging LLMs as optimizers for controller tuning. Before proceeding with the implementation, we establish the fundamental control problem and relevant theoretical background.\n",
        "\n",
        "## **The Feedback Control Paradigm**\n",
        "\n",
        "Control systems engineering addresses the automation of dynamic processes ($P$) to regulate controlled variables ($y(t) = P(u(t), d(t))$) to desired setpoints ($r(t)$) despite disturbances ($d(t)$). This is achieved by designing a controller ($C$) that manipulates process inputs ($u(t)$) to minimize tracking error $e(t) = r(t) - y(t)$. For this tutorial, we consider a feedback-only architecture with additive disturbances that do not affect process dynamics ($y(t) = P(u(t)) + d(t)$), as depicted below.\n",
        "\n",
        "<p style=\"text-align:center;\">\n",
        "<img src=\"https://github.com/MohamedElMistiri/LLM-PID-Tuning-Demo/blob/main/images/closed-loop-block-diagram.png?raw=true\" alt=\"Closed-loop feedback control block diagram\" style=\"max-width: 600px;\">\n",
        "</p>\n",
        "\n",
        "Controllers may take numerous forms, from classical PID to optimal control (LQR, MPC) or reinforcement learning policies. This tutorial focuses on PID variants as representative examples.\n",
        "\n",
        "## **PID Controller Fundamentals**\n",
        "\n",
        "PID controllers combine three control actions based on the error signal:\n",
        "\n",
        "- **Proportional (P)**: Provides corrective action proportional to the instantaneous error\n",
        "- **Integral (I)**: Eliminates steady-state offset by integrating historical error\n",
        "- **Derivative (D)**: Anticipates future error trends through derivative action\n",
        "\n",
        "The continuous-time representation is:\n",
        "\n",
        "$$u(t) = C(e(t)) = K_c\\left(e(t) + \\frac{1}{\\tau_I} \\int_0^t e(\\tau) d\\tau + \\tau_D \\frac{de(t)}{dt}\\right)$$\n",
        "\n",
        "where $K_c$ is the proportional gain, $\\tau_I$ the integral time constant, and $\\tau_D$ the derivative time constant. The equivalent transfer function representation is:\n",
        "\n",
        "$$C(s) = \\frac{u(s)}{e(s)} = K_c\\left(1 + \\frac{1}{\\tau_I s}  + \\tau_D s \\right)$$\n",
        "\n",
        "This tutorial examines three PID architectures:\n",
        "\n",
        "1. **PI Controller**: Proportional and integral actions only ($\\tau_D = 0$). Two tunable parameters: $K_c$ and $\\tau_I$.\n",
        "2. **PID Controller**: Full three-term implementation. Three tunable parameters: $K_c$, $\\tau_I$, and $\\tau_D$.\n",
        "3. **PID with Filter (PIDwF)**: Includes a first-order filter on the error signal to attenuate high-frequency noise and render the derivative term physically realizable:\n",
        "$$C(s) = K_c\\left(1 + \\frac{1}{\\tau_I s}  + \\tau_D s \\right) \\left(\\frac{1}{\\tau_F s + 1}\\right)$$ This formulation introduces a fourth tuning parameter, the filter time constant $\\tau_F$.\n",
        "\n",
        "## **Process Model: First-Order System**\n",
        "\n",
        "We evaluate controller performance on a first-order linear time-invariant system:\n",
        "\n",
        "$$\\tau\\frac{dy(t)}{dt} + y(t) = k_p u(t)$$\n",
        "\n",
        "This ordinary differential equation describes numerous physical processes, where $\\tau$ is the system time constant (response speed) and $k_p$ is the steady-state gain (input-output sensitivity). The transfer function representation is:\n",
        "\n",
        "$$P(s) = \\frac{y(s)}{u(s)} = \\frac{k_p}{\\tau s + 1}$$\n",
        "\n",
        "To concretize these representations, consider the following canonical examples from each engineering domain:\n",
        "\n",
        "### **1. Thermal System: Room Temperature Dynamics**\n",
        "\n",
        "Energy balance for a room with thermal capacitance $C_{th}$ and resistance $R_{th}$:\n",
        "\n",
        "$$C_{th}\\frac{dT_{room}(t)}{dt} = \\dot{Q}_{heater}(t) - \\frac{T_{room}(t) - T_{amb}}{R_{th}}$$\n",
        "\n",
        "In standard form with $y(t) = T_{room}$ and $u(t) = \\dot{Q}_{heater}$:\n",
        "\n",
        "$$\\tau\\frac{dT_{room}(t)}{dt} + T_{room}(t) = k_p u(t) + T_{amb}$$\n",
        "\n",
        "**Parameters:**\n",
        "- $\\tau = R_{th}C_{th}$ [minutes]: Thermal time constant\n",
        "- $k_p = R_{th}$ [°C/kW]: Steady-state temperature rise per unit heating power\n",
        "- $T_{amb}$ [°C]: Ambient temperature (disturbance)\n",
        "- $T_{room}$ [°C]: Room temperature (controlled variable)\n",
        "\n",
        "**Laplace domain (deviation variables):**\n",
        "\n",
        "$$P(s) = \\frac{T_{room}(s)}{\\dot{Q}_{heater}(s)} = \\frac{k_p}{\\tau s + 1}$$\n",
        "\n",
        "### **2. Electrical System: RC Low-Pass Filter**\n",
        "\n",
        "Series RC circuit with input voltage $V_{in}(t)$ and output voltage $V_{out}(t)$ across the capacitor:\n",
        "\n",
        "$$RC\\frac{dV_{out}(t)}{dt} + V_{out}(t) = V_{in}(t)$$\n",
        "\n",
        "**Parameters:**\n",
        "- $R$ [Ω]: Resistance\n",
        "- $C$ [F]: Capacitance\n",
        "- $\\tau = RC$ [seconds]: Electrical time constant\n",
        "- $k_p = 1$ [V/V]: Unity gain\n",
        "\n",
        "**Transfer function:**\n",
        "\n",
        "$$P(s) = \\frac{V_{out}(s)}{V_{in}(s)} = \\frac{1}{\\tau s + 1}$$\n",
        "\n",
        "### **3. Mechanical System: Velocity of Mass-Damper System**\n",
        "\n",
        "Mass $m$ moving with velocity $v(t)$ under applied force $F(t)$ and viscous friction coefficient $b$:\n",
        "\n",
        "$$m\\frac{dv(t)}{dt} + bv(t) = F(t)$$\n",
        "\n",
        "In standard form:\n",
        "\n",
        "$$\\tau\\frac{dv(t)}{dt} + v(t) = k_p F(t)$$\n",
        "\n",
        "**Parameters:**\n",
        "- $\\tau = m/b$ [seconds]: Mechanical time constant\n",
        "- $k_p = 1/b$ [(m/s)/N]: Steady-state velocity per unit force\n",
        "- $v(t)$ [m/s]: Velocity (controlled variable)\n",
        "- $F(t)$ [N]: Applied force (manipulated variable)\n",
        "\n",
        "**Laplace domain:**\n",
        "\n",
        "$$P(s) = \\frac{v(s)}{F(s)} = \\frac{k_p}{\\tau s + 1}$$\n",
        "\n",
        "### **4. Chemical System: Liquid Level in Cylindrical Tank**\n",
        "\n",
        "Cylindrical tank of cross-sectional area $A$ with inlet flow $Q_{in}(t)$ and outlet flow proportional to hydrostatic head:\n",
        "\n",
        "$$A\\frac{dh(t)}{dt} = Q_{in}(t) - k_{out}h(t)$$\n",
        "\n",
        "In standard form:\n",
        "\n",
        "$$\\tau\\frac{dh(t)}{dt} + h(t) = k_p Q_{in}(t)$$\n",
        "\n",
        "**Parameters:**\n",
        "- $A$ [m²]: Tank cross-sectional area\n",
        "- $k_{out}$ [m²/s]: Outlet flow coefficient\n",
        "- $\\tau = A/k_{out}$ [seconds]: Hydraulic time constant\n",
        "- $k_p = 1/k_{out}$ [m/(m³/s)]: Steady-state level per unit inlet volumtric flow\n",
        "- $h(t)$ [m]: Liquid level (controlled variable)\n",
        "- $Q_{in}(t)$ [m³/s]: Inlet flow rate (manipulated variable)\n",
        "\n",
        "**Transfer function:**\n",
        "\n",
        "$$P(s) = \\frac{h(s)}{Q_{in}(s)} = \\frac{k_p}{\\tau s + 1}$$\n",
        "\n",
        "These diverse physical systems share an identical mathematical structure, underscoring the universal applicability of first-order system analysis and controller design methodology.\n",
        "\n",
        "### **5. Incorporating Time Delay**\n",
        "\n",
        "The preceding examples represent idealized dynamics. Physical systems often exhibit **time delay** (dead time, $\\theta$) due to transport phenomena, measurement latency, or computational lag. This delay between applying a control action and observing its effect significantly complicates tuning and is a critical consideration in practice.\n",
        "\n",
        "The time-delayed first-order system becomes:\n",
        "\n",
        "$$\\tau\\frac{dy(t)}{dt} + y(t) = k_p \\, u(t-\\theta)$$\n",
        "\n",
        "In the Laplace domain, delay introduces an irrational exponential term:\n",
        "\n",
        "$$P(s) = \\frac{y(s)}{u(s)} = \\frac{k_p}{\\tau s + 1} e^{-\\theta s}$$\n",
        "\n",
        "This tutorial includes time delay as a tunable parameter to reflect realistic process dynamics and assess the LLM's ability to handle this added complexity.\n",
        "\n",
        "---\n",
        "\n",
        "# **LLM-Guided PID Tuning via In-Context Learning**\n",
        "\n",
        "This demonstration frames PID tuning as an optimization problem solved through iterative LLM interaction. The approach leverages in-context learning to transform the language model into a control engineering assistant capable of reasoning about parameter selection and closed-loop performance.\n",
        "\n",
        "## **Methodology**\n",
        "\n",
        "The tuning workflow proceeds as follows:\n",
        "\n",
        "1. **Initialization**: Provide the LLM with two in-context examples comprising randomly generated PID parameters and their corresponding performance metrics (MSE, output variance, and input variance) from closed-loop simulations.\n",
        "\n",
        "2. **Generation**: The LLM proposes a new set of tuning parameters, accompanied by explicit reasoning for its recommendations.\n",
        "\n",
        "3. **Evaluation**: The suggested parameters are implemented in a closed-loop simulation, generating updated performance metrics.\n",
        "\n",
        "4. **Iteration**: The simulation results are appended to the prompt history, creating an enriched context for subsequent LLM queries. This feedback loop continues until either the MSE converges to a minimum or a predefined iteration limit is reached.\n",
        "\n",
        "## **Performance Assessment**\n",
        "\n",
        "To contextualize the LLM-tuned controller performance, we benchmark results against the Internal Model Control (IMC) tuning method—a well-established model-based approach that derives PID parameters directly from process characteristics (time constant, gain, and time delay). IMC provides a theoretically grounded baseline for evaluating the efficacy of LLM-driven optimization. For a comprehensive treatment of IMC tuning principles, refer to [Rivera et al. (1986)](https://skoge.folk.ntnu.no/publications/1986/Rivera86/Rivera86.pdf).\n",
        "\n",
        "This comparative framework enables quantitative assessment of whether in-context learning can achieve performance comparable to traditional model-based tuning while requiring only minimal process knowledge and no explicit optimization algorithms.\n",
        "\n",
        "## **Code Overview**\n",
        "\n",
        "The implementation is organized into a modular structure, with each component responsible for a different stage of the PID tuning pipeline. This design separates system configuration, closed-loop simulation, prompt construction, LLM inference, and performance evaluation, making the system easy to understand, modify, and extend for different process dynamics or controller types."
      ],
      "metadata": {
        "id": "kU4CXddeoZfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Install ```control``` mondule**\n",
        "!pip install control"
      ],
      "metadata": {
        "id": "-luXYuCOpsfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1BQqwhIoUEl"
      },
      "outputs": [],
      "source": [
        "#@title **Import Necessary Libraries**\n",
        "\n",
        "import control as ct\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import os\n",
        "import dotenv\n",
        "import datetime\n",
        "import json\n",
        "from google import genai\n",
        "import time\n",
        "import re\n",
        "import getpass\n",
        "import ipywidgets as widgets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Before Running the Demo, Follow the Instructions Below**\n",
        "To run the full experiment:\n",
        "1. Ensure all dependencies are imported and installed.\n",
        "2.\tVisit Google AI Studio (https://aistudio.google.com/) to obtain your Gemini API key.\n",
        "3.\tOnce API key is generated, copy and paste it into the demo when prompted.\n",
        "\n",
        "#### **If having trouble creating an API Key, follow the link below for instructions:**\n",
        "* #### **[Instructions on How to Obtain a Gemini API Key](https://docs.google.com/document/d/17pgikIpvZCBgcRh4NNrcee-zp3Id0pU0vq2SG52KIGE/edit?usp=sharing)**\n",
        "\n"
      ],
      "metadata": {
        "id": "vNamP9yf63lq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Setting Up Gemini Client**\n",
        "apikey = getpass.getpass(\"Enter your Gemini API Key: \")"
      ],
      "metadata": {
        "id": "AFQuMxk2oaLb",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Choose a Model**\n",
        "model_dropdown = widgets.Dropdown(\n",
        "    options=[\n",
        "        (\"Gemini 2.5 Flash\", \"gemini-2.5-flash\"),\n",
        "        (\"Gemini 2.0 Flash\", \"gemini-2.0-flash\")\n",
        "    ],\n",
        "    description=\"Model:\",\n",
        "    value=\"gemini-2.5-flash\",\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "confirm_button = widgets.Button(\n",
        "    description=\"Confirm Selection\"\n",
        ")\n",
        "\n",
        "output = widgets.Output()\n",
        "\n",
        "model_name = None\n",
        "\n",
        "def on_confirm_click(b):\n",
        "    global model_name, batch_size\n",
        "\n",
        "    model_name = model_dropdown.value\n",
        "\n",
        "    with output:\n",
        "        output.clear_output()\n",
        "        print(f\"\\nSelected model: {model_name}\")\n",
        "\n",
        "confirm_button.on_click(on_confirm_click)\n",
        "\n",
        "display(model_dropdown, confirm_button, output)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "uZJQHyV_7wIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Utility Functions**\n",
        "\n",
        "**Response Parser**  \n",
        "Extracts numerical parameter values from the LLM's textual output, converting recommendations like \"$K_c$ = 2.5, $\\tau_I$ = 15.2\" into an ordered list of floats for direct simulation use.\n",
        "\n",
        "**Performance Metric: MSE**  \n",
        "Computes the mean squared error between the setpoint trajectory and controlled variable output, serving as the primary optimization objective that guides the LLM's tuning iterations.\n",
        "\n",
        "**Example Formatter**  \n",
        "Generates a structured text string that encapsulates a tuning attempt's parameters (e.g., $K_c$, $\\tau_I$) and resulting performance metrics (MSE, output variance, input variance) for inclusion in the LLM's prompt history.\n",
        "\n",
        "**Data Persistence**  \n",
        "Saves the complete experimental record—comprising parameters, metrics, and LLM responses at each iteration—to a JSON file for post-analysis and reproducibility"
      ],
      "metadata": {
        "id": "jRRRboHYGWLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Utility Functions**\n",
        "def parse_response(input_text):\n",
        "    \"\"\"Extract all numbers from a string as floats.\"\"\"\n",
        "    pattern = r\"[-+]?\\d+(?:\\.\\d+)?\"\n",
        "    return [float(match) for match in re.findall(pattern, input_text)]\n",
        "\n",
        "def calculate_mse(x, y):\n",
        "    \"\"\"Mean squared error between two arrays.\"\"\"\n",
        "    return np.mean((x - y)**2)\n",
        "\n",
        "def generate_example_str(params, mse, i, y_var, u_var):\n",
        "    \"\"\"Generalized example string for any controller type.\"\"\"\n",
        "    param_str = ', '.join([f\"{k} = {v}\" for k, v in params.items()])\n",
        "    return f\"Example {i}: Tuning Parameters: {param_str}, produced an error of: MSE = {mse}, output variance: {y_var}, manipulated variable variance: {u_var}\"\n",
        "\n",
        "def save_json(data, filename):\n",
        "    \"\"\"Save data to a JSON file safely, even if it contains non-serializable objects like 'Content'.\"\"\"\n",
        "    def default_serializer(obj):\n",
        "        # Try to convert known object types to a serializable form\n",
        "        if hasattr(obj, \"__dict__\"):\n",
        "            return obj.__dict__  # convert custom objects to dict\n",
        "        return str(obj)  # fallback to string\n",
        "\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(data, f, default=default_serializer, indent=2)"
      ],
      "metadata": {
        "id": "fSoUWyTHodfm",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Controller Configuration**\n",
        "\n",
        "This function calculates baseline tuning parameters using the Internal Model Control (IMC) method—a classic, proven approach that derives controller settings directly from your process characteristics. Think of IMC as the \"traditional expert\" whose performance we'll compare against our AI assistant.\n",
        "\n",
        "The function supports three controller types:\n",
        "- **PI**: Two parameters for basic control (proportional + integral)\n",
        "- **PID**: Three parameters for more aggressive control (adds derivative action)\n",
        "- **PIDwFilter**: Four parameters that include a noise-filtering term for smoother operation\n",
        "\n",
        "You simply specify which controller you want (via `case`), provide your process model values (time constant `tau`, gain `kp`, and delay `theta`), and the function returns scientifically-grounded parameters that serve as both a starting point and performance benchmark for the LLM optimization."
      ],
      "metadata": {
        "id": "76doxY-AG9R9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Controller Configuration**\n",
        "def get_controller_params(case, alpha, tau, theta, kp):\n",
        "    \"\"\"Return controller parameters and names for PI, PID, or PIDf.\"\"\"\n",
        "    lamb = alpha * (tau + 0.5 * theta)\n",
        "    if case == 1:  # PI\n",
        "        params = dict(Kc=(2*tau+theta)/(kp*2*lamb),\n",
        "                      tau_I=tau + theta/2,\n",
        "                      tau_D=0,\n",
        "                      tau_F=0)\n",
        "        names = ['Kc', 'tau_I']\n",
        "        label = \"PI\"\n",
        "    elif case == 2:  # PID\n",
        "        params = dict(Kc=(2*tau+theta)/(kp*(2*lamb+theta)),\n",
        "                      tau_I=tau + theta/2,\n",
        "                      tau_D=(tau*theta)/(2*tau+theta),\n",
        "                      tau_F=0.01)\n",
        "        names = ['Kc', 'tau_I', 'tau_D']\n",
        "        label = \"PID\"\n",
        "    elif case == 3:  # PIDwfilter\n",
        "        params = dict(Kc=(2*tau+theta)/(kp*(2*lamb+2*theta)),\n",
        "                      tau_I=tau + theta/2,\n",
        "                      tau_D=(tau*theta)/(2*tau+theta),\n",
        "                      tau_F=(lamb*theta)/(2*lamb+2*theta))\n",
        "        names = ['Kc', 'tau_I', 'tau_D', 'tau_F']\n",
        "        label = \"PIDwfilter\"\n",
        "    return params, names, label"
      ],
      "metadata": {
        "id": "kDaQi_Mioi_7",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- ## **Closed-Loop Simulation**\n",
        "\n",
        "This function is the experimental testbed where we evaluate how well a given set of PID parameters actually controls our process. Think of it as a virtual laboratory: we build a digital twin of our real-world system, connect our controller to it, run through realistic scenarios, and measure the results.\n",
        "\n",
        "**Here's what happens, step by step:**\n",
        "\n",
        "**1. Build the Plant Model**  \n",
        "We create a mathematical representation of our process (like a heater, motor, or tank) using its key characteristics: gain (`kp`) and time constant (`tau`). This is the \"thing\" we're trying to control.\n",
        "\n",
        "**2. Add Real-World Delay**  \n",
        "Physical systems don't respond instantly. We model the transport delay (`theta`)—the time between taking an action and seeing its effect—using a Padé approximation. This makes our simulation much more realistic.\n",
        "\n",
        "**3. Assemble the Controller**  \n",
        "We construct the PID controller as a transfer function using the tuning parameters (`Kc`, `tau_I`, `tau_D`, `tau_F`). This is the \"brain\" that will make decisions.\n",
        "\n",
        "**4. Wire Everything Together**  \n",
        "We connect the plant, controller, and disturbance signals into a complete **feedback loop**: the controller reads the error (difference between desired and actual output), calculates a corrective action, applies it to the plant, and the cycle continues. We also add a summing junction to inject disturbances.\n",
        "\n",
        "**5. Create Test Scenarios**  \n",
        "We generate realistic test signals:\n",
        "- A **setpoint change** at time `t_setpoint` (e.g., \"increase room temperature to 22°C\")\n",
        "- A **disturbance** at time `t_disturbance` (e.g., someone opens a window)\n",
        "\n",
        "**6. Run the Virtual Experiment**  \n",
        "The simulation calculates how the system behaves every millisecond over the time horizon, showing us exactly how the controlled variable (`y`) and manipulated variable (`u`) respond.\n",
        "\n",
        "**7. Measure Performance**  \n",
        "We compute three critical metrics:\n",
        "- **Mean Squared Error (MSE)**: How far did the actual output drift from the setpoint? Lower is better.\n",
        "- **Output Variance (`y_var`)**: How steady was the controlled variable? We want smooth, not oscillatory.\n",
        "- **Input Variance (`u_var`)**: How aggressively did the controller act? We want efficient control, not \"jumpy\" actuator movements.\n",
        "\n",
        "**8. Optional Visualization**  \n",
        "If requested, we plot the time trajectories of both the process variable and controller output so you can literally *see* how the system behaves.\n",
        "\n",
        "**What you get back**: The function returns all performance metrics plus the full time-series data (`response`), giving us everything needed to evaluate whether the LLM's suggested parameters are actually improving control. -->\n",
        "## **Closed-Loop Simulation Module**\n",
        "\n",
        "This function constitutes the core computational framework for evaluating controller performance. It constructs a complete feedback control system in the Laplace domain, simulates its dynamic response to realistic operational scenarios, and quantifies performance through standardized metrics.\n",
        "\n",
        "**The simulation proceeds through four primary phases:**\n",
        "\n",
        "**1. Process Model Assembly**  \n",
        "The function first defines the plant dynamics as a first-order transfer function characterized by process gain (`kp`) and time constant (`tau`). To capture real-world behavior, transport delay (`theta`) is incorporated via a 20th-order Padé approximation, ensuring high-fidelity representation of latency effects inherent in physical systems.\n",
        "\n",
        "**2. Controller Realization**  \n",
        "The PID control law is implemented as a transfer function using the provided tuning parameters. The general structure accommodates proportional, integral, and derivative actions, with an optional first-order filter (`tau_F`) for derivative term noise attenuation. This flexible formulation supports PI, PID, and filtered-PID configurations through a unified implementation.\n",
        "\n",
        "**3. Closed-Loop System Construction**  \n",
        "The plant, controller, and disturbance injection pathways are integrated into a complete feedback architecture using `control.InterconnectedSystem`. This creates a rigorous mathematical representation where:\n",
        "- The controller acts on measured error (setpoint minus process output)\n",
        "- Control actions propagate through the delayed plant dynamics\n",
        "- Additive disturbances can be injected at the plant input\n",
        "- Both controlled and manipulated variables are accessible for analysis\n",
        "\n",
        "**4. Performance Evaluation**  \n",
        "The system undergoes forced-response simulation through a standardized test protocol:\n",
        "- **Setpoint step**: A reference change at `t_setpoint` evaluates tracking performance\n",
        "- **Disturbance step**: A load disturbance at `t_disturbance` assesses regulatory capability\n",
        "\n",
        "Performance is rigorously quantified via three complementary metrics:\n",
        "- **Mean Squared Error (MSE)**: Aggregated tracking accuracy during both transient and steady-state phases\n",
        "- **Output Variance**: Statistical dispersion of the controlled variable, indicating oscillation tendency\n",
        "- **Input Variance**: Statistical dispersion of the manipulated variable, reflecting control effort and actuator wear\n",
        "\n",
        "When visualization is enabled, the function generates time-domain trajectories of both controlled and manipulated variables, providing intuitive insight into closed-loop behavior. The complete time-series data and performance metrics are returned, enabling downstream analysis and iterative optimization."
      ],
      "metadata": {
        "id": "XztLwWZHHYQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Simulation**\n",
        "def closed_loop_simulation(config):\n",
        "    \"\"\"Run closed-loop simulation and return performance metrics.\"\"\"\n",
        "    num = [config['kp']]\n",
        "    den = [config['tau'], 1]\n",
        "    plant = ct.TransferFunction(num, den, name=\"plant\", inputs=\"u\", outputs=\"y\")\n",
        "    delay_num, delay_den = ct.pade(config['theta'], 20)\n",
        "    delay_tf = ct.TransferFunction(delay_num, delay_den)\n",
        "    plant_delayed = plant * delay_tf\n",
        "    plant_delayed.name = \"plant\"\n",
        "\n",
        "    controller_num = [config['Kc']*config['tau_I']*config['tau_D'],\n",
        "                      config['Kc']*config['tau_I'],\n",
        "                      config['Kc']]\n",
        "    controller_den = [config['tau_I']*config['tau_F'],\n",
        "                      config['tau_I'],\n",
        "                      0]\n",
        "    controller = ct.TransferFunction(controller_num, controller_den, name=\"controller\", inputs=\"y\", outputs=\"u\")\n",
        "    add_disturbance = ct.summing_junction(inputs=['u', 'd'], output=\"u\")\n",
        "    add_disturbance.name = \"add_disturbance\"\n",
        "    cl_sys = ct.InterconnectedSystem(\n",
        "        [plant_delayed, controller, add_disturbance], name=\"cl_sys\",\n",
        "        connections=[\n",
        "            [\"controller.y\", \"-plant.y\"], [\"add_disturbance.u\", \"controller.u\"],\n",
        "            [\"plant.u\", \"add_disturbance.u\"]\n",
        "        ],\n",
        "        inplist=[\"controller.y\", \"add_disturbance.d\"],\n",
        "        inputs=[\"y_ref\", \"d\"],\n",
        "        outlist=[\"plant.y\", \"controller.u\"],\n",
        "        outputs=[\"y\", \"u\"]\n",
        "    )\n",
        "\n",
        "    Ts = config['t_end'] / config['n_samples']\n",
        "    T = np.linspace(config['t_start'], config['t_end'], config['n_samples'])\n",
        "    y_ref = np.zeros(T.shape)\n",
        "    n_setpoint = int(config['t_setpoint'] / Ts)\n",
        "    y_ref[n_setpoint:] = config['setpoint']\n",
        "    d = np.zeros(T.shape)\n",
        "    n_disturbance = int(config['t_disturbance'] / Ts)\n",
        "    d[n_disturbance:] = config['disturbance']\n",
        "\n",
        "    response = ct.forced_response(cl_sys, T, [y_ref, d])\n",
        "\n",
        "    if config.get('plot_results', False):\n",
        "        plt.figure()\n",
        "        plt.subplot(211)\n",
        "        plt.plot(response.t, response.y[0], label=\"controlled variable (y)\")\n",
        "        plt.plot(response.t, response.u[0], linestyle='--', label=\"setpoint (r)\")\n",
        "        plt.xlim((response.t[0], response.t[-1]))\n",
        "        plt.grid(True)\n",
        "        plt.ylabel('y')\n",
        "        plt.legend()\n",
        "        plt.subplot(212)\n",
        "        plt.plot(response.t, response.y[1], label='manipulated variable ($u$)')\n",
        "        plt.plot(response.t, response.u[1], linestyle='--', label='disturbance (d)')\n",
        "        plt.xlim((response.t[0], response.t[-1]))\n",
        "        plt.ylim([-2, 2])\n",
        "        plt.grid(True)\n",
        "        plt.ylabel('u')\n",
        "        plt.xlabel('Time [s]')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "    setpoint_tracking_mse = calculate_mse(response.y[0][n_setpoint:n_disturbance], response.u[0][n_setpoint:n_disturbance])\n",
        "    disturbance_rejection_mse = calculate_mse(response.y[0][n_disturbance:], response.u[0][n_disturbance])\n",
        "    mse_overall = calculate_mse(response.y[0], response.u[0])\n",
        "    y_var = np.var(response.y[0][n_setpoint:])\n",
        "    u_var = np.var(response.y[1][n_setpoint:])\n",
        "    return setpoint_tracking_mse, disturbance_rejection_mse, mse_overall, y_var, u_var, response"
      ],
      "metadata": {
        "id": "YHZoFTRSojpG",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **LLM Interaction and Iterative Optimization Module**\n",
        "\n",
        "This function serves as the orchestration layer for the in-context learning workflow, managing the conversational state with Gemini and driving the iterative parameter refinement process. It transforms the language model into an active participant in the control design optimization, rather than a static advisor.\n",
        "\n",
        "**The function executes a structured iterative protocol:**\n",
        "\n",
        "**1. Conversation Initialization**  \n",
        "Establishes a persistent chat session with the Gemini model, sending the initial ICL prompt that contains:\n",
        "- Controller tuning objectives and constraints\n",
        "- Representative examples of suboptimal parameters with their performance metrics\n",
        "- Explicit formatting requirements for parameter extraction\n",
        "\n",
        "**2. Iterative Refinement Loop**  \n",
        "For each iteration, the system:\n",
        "- **Parses** the LLM's response to extract numerical parameter recommendations using regex pattern matching\n",
        "- **Updates** the simulation configuration with the proposed tuning values\n",
        "- **Evaluates** the new parameters through the closed-loop simulation, yielding objective performance metrics (MSE, variances)\n",
        "- **Enriches context** by appending the empirical results to the conversation history via the iteration prompt template\n",
        "- **Re-queries** the LLM, now armed with additional performance data to inform subsequent recommendations\n",
        "\n",
        "This creates a feedback loop where each iteration's outcomes become in-context examples for the next, enabling the model to learn from its own suggestions' successes and failures.\n",
        "\n",
        "**3. Exception Handling and Resilience**  \n",
        "The loop incorporates robust error handling: if parameter parsing fails or simulation errors occur, the system logs the exception, implements a cooldown period (60-second sleep), and continues the optimization process rather than terminating abruptly.\n",
        "\n",
        "**4. Termination and Documentation**  \n",
        "Upon reaching `max_iterations`, the function performs a final evaluation of the LLM's last recommendation and requests a concluding response. It returns:\n",
        "- **Complete conversation history**: Full transcript of the LLM's reasoning trajectory\n",
        "- **Evaluation archive**: Time-series record of all parameter sets and their associated performance metrics\n",
        "- **Final simulation response**: Time-domain data from the ultimate controller configuration for comparative visualization\n",
        "\n",
        "This architecture enables systematic exploration of the LLM's ability to navigate the parameter space through linguistic reasoning, transforming controller tuning from a numerical optimization problem into a conversational design process, combining linguistic and numerical reasoning."
      ],
      "metadata": {
        "id": "mppFEf-QIsVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Gemini Interaction**\n",
        "def prompt_gemini(prompt, iteration_prompt, max_iterations, config, param_names):\n",
        "    \"\"\"Interact with Gemini for iterative controller tuning.\"\"\"\n",
        "    client = genai.Client(api_key=apikey)\n",
        "    chat_session = client.chats.create(model=model_name)\n",
        "    print(\"\\n\\nGemini is prompted\")\n",
        "    response = chat_session.send_message(prompt)\n",
        "    res = parse_response(response.text)\n",
        "    evaluations = []\n",
        "    for iteration in range(max_iterations):\n",
        "        try:\n",
        "          if iteration == 0:\n",
        "            print(\"Initiation prompt response:\")\n",
        "            print(response.text)\n",
        "          else:\n",
        "            print(f\"Iteration {iteration-1} response:\")\n",
        "            print(response.text)\n",
        "          # Update parameters based on the response\n",
        "          for idx, name in enumerate(param_names):\n",
        "              config[name] = res[idx]\n",
        "          config[\"plot_results\"] = False\n",
        "          _, _, mse_overall_, y_var_, u_var_, _ = closed_loop_simulation(config)\n",
        "          print(f\"iteration {iteration}: \" + \", \".join([f\"{name} = {config[name]}\" for name in param_names]) +\n",
        "                f\", mse = {mse_overall_}, y_var = {y_var_}, u_var = {u_var_}\\n\\n\")\n",
        "          evaluations.append([config[name] for name in param_names] + [mse_overall_, y_var_, u_var_])\n",
        "          response = chat_session.send_message(\n",
        "              iteration_prompt.format(i=iteration, **{name: config[name] for name in param_names},\n",
        "                                      MSE=mse_overall_, y_var=y_var_, u_var=u_var_))\n",
        "          res = parse_response(response.text)\n",
        "        except Exception as e:\n",
        "            print(\"Error:\", e)\n",
        "            time.sleep(60)\n",
        "    # Final evaluation\n",
        "    for idx, name in enumerate(param_names):\n",
        "        config[name] = res[idx]\n",
        "    config[\"plot_results\"] = False\n",
        "    _, _, mse_overall_, y_var_, u_var_, response_llm = closed_loop_simulation(config)\n",
        "    evaluations.append([config[name] for name in param_names] + [mse_overall_, y_var_, u_var_])\n",
        "    print(f\"Iteration {iteration} response:\")\n",
        "    print(response.text)\n",
        "    print(f\"iteration {iteration+1}: \" + \", \".join([f\"{name} = {config[name]}\" for name in param_names]) +\n",
        "                f\", mse = {mse_overall_}, y_var = {y_var_}, u_var = {u_var_}\\n\\n\")\n",
        "    return chat_session.get_history(), evaluations, response_llm"
      ],
      "metadata": {
        "id": "9HDPVGMOoj1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Plotting Results**\n",
        "def plot_results(evaluations, param_names, imc_params, mse_overall, y_var, u_var, save_directory, model_label, d):\n",
        "    \"\"\"Generalized plotting for controller parameters and performance.\"\"\"\n",
        "    n_params = len(param_names)\n",
        "    plt.figure()\n",
        "    for i, name in enumerate(param_names):\n",
        "      if \"tau\" in name:\n",
        "        name = \"\\\\\" + name\n",
        "      plt.subplot(n_params, 1, i+1)\n",
        "      plt.plot(range(len(evaluations)), np.array(evaluations)[:,i], label=f\"${name}$\")\n",
        "      plt.axhline(imc_params[i], color='r', linestyle='--', label=\"IMC tuning\")\n",
        "      plt.xlim((0, len(evaluations)-1))\n",
        "      plt.grid(True)\n",
        "      plt.legend()\n",
        "      plt.ylabel(f\"${name}$\")\n",
        "      plt.xlabel(\"Iteration\")\n",
        "    plt.savefig(f\"./Results/{save_directory}/{model_label}-tuning_parameters-max-iterations-{len(evaluations)}-{d}.png\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.subplot(211)\n",
        "    plt.plot(range(len(evaluations)), np.array(evaluations)[:,n_params])\n",
        "    plt.axhline(mse_overall, color='r', linestyle='--', label=\"IMC tuning\")\n",
        "    plt.xlim((0, len(evaluations)-1))\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.ylabel(\"Mean Squared Error (MSE)\")\n",
        "    plt.subplot(212)\n",
        "    plt.plot(range(len(evaluations)), np.array(evaluations)[:,n_params+1], label='$y_{var}$')\n",
        "    plt.plot(range(len(evaluations)), np.array(evaluations)[:,n_params+2], linestyle=\"--\", label='$u_{var}$')\n",
        "    plt.xlim((0, len(evaluations)-1))\n",
        "    plt.axhline(u_var, color='g', label=\"$u_{var}$ IMC tuning\")\n",
        "    plt.axhline(y_var, color='r', linestyle='--', label=\"$y_{var}$ IMC tuning\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.ylabel(\"variance\")\n",
        "    plt.xlabel(\"Iteration\")\n",
        "    plt.savefig(f\"./Results/{save_directory}/{model_label}-performance-max-iterations-{len(evaluations)}-{d}.png\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "F9npZhjPoj_M",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<style>\n",
        "pre {\n",
        "  white-space: pre-wrap;       /* Wrap long lines */\n",
        "  word-wrap: break-word;       /* Break long words */\n",
        "  overflow-x: auto;            /* Optional: allow horizontal scroll if needed */\n",
        "}\n",
        "</style>\n",
        "\n",
        "## **Experiment Implementation: End-to-End LLM Tuning Workflow**\n",
        "\n",
        "This is the master control function that coordinates the entire in-context learning experiment from initialization through final analysis. It systematically executes each phase of the comparative study, pitting the LLM's iterative reasoning against classical IMC tuning.\n",
        "\n",
        "**The workflow consists of seven sequential stages:**\n",
        "\n",
        "**1. Baseline Establishment**  \n",
        "The function first generates IMC tuning parameters using the `get_controller_params` function. It then executes a closed-loop simulation with these IMC parameters, creating a performance benchmark measured through MSE, output variance, and input variance. This establishes the \"gold standard\" that the LLM must meet or exceed.\n",
        "\n",
        "**2. In-Context Example Generation**  \n",
        "To prime the LLM for effective learning, the function initiates two deliberately suboptimal tuning examples. These cases, featuring conservative and aggressive parameter choices, are simulated to produce their corresponding performance metrics. The examples are formatted into natural language descriptions that become the foundation of the ICL prompt, essentially teaching the LLM \"what not to do\" before it begins optimization.\n",
        "\n",
        "**3. Prompt Engineering Architecture**  \n",
        "The function maintains a dictionary of controller-specific prompt templates for PI, PID, and PIDwF variants. Each template is carefully engineered to:\n",
        "- Establish the LLM's persona as a \"smart and resourceful control systems engineer\"\n",
        "- Define explicit optimization objectives and parameter constraints\n",
        "- Specify mandatory response formatting for reliable parsing\n",
        "- Embed the suboptimal examples as contextual grounding\n",
        "\n",
        "**Crucially, we intentionally refrain from passing the true model parameters (`kp`, `tau`, `theta`) to the LLM in our prompt.** This deliberate omission ensures that the LLM's tuning recommendations emerge purely from iterative in-context learning and performance feedback, rather than leveraging its nascent knowledge of classical control theory or pre-existing tuning rules. The LLM must discover effective parameter relationships through conversational reasoning alone.\n",
        "\n",
        "**Prompt Example**\n",
        "\n",
        "*Initiation Prompt:*\n",
        "\n",
        "```\n",
        "You are a smart and resourceful control systems engineer helping me tune a PID controller to minimize the Mean Squared\n",
        "Error (MSE) between the controlled output variable (y) and the reference (r). The plant system to control is a first-order\n",
        "system with a transport delay. The controller in use is PID controller with three tuning parameters (Kc, tau_I, tau_D).\n",
        "Your goal is to adjust the tuning parameters: Kc in range {Kc_range}, tau_I in range {tau_I_range}, and tau_D in\n",
        "range {tau_D_range} to minimize the MSE, as well as variance in the output controlled variable and the manipulated variable.\n",
        "Next, you will see examples of the tuning parameters (Kc, tau_I, tau_D) and their corresponding MSE values.\n",
        "    \n",
        "Based on MSE and\n",
        "variance values, please provide a new set of tuning parameters you think will help in minimizing the MSE and reduce variance\n",
        "in both the manipulated and output variables.\n",
        "You will provide your response in the following exact format:\n",
        "    * new tuning parameters (Kc, tau_I, tau_D) aiming to minimize the mean square error.\n",
        "    * a brief (one sentence) explanation of why you chose that input, considering the current iteration.\n",
        "    \n",
        "Here are the examples:\n",
        "{examples}\n",
        "\n",
        "```\n",
        "\n",
        "*Iteration Prompt:*\n",
        "```\n",
        "iteration {i}:\\n\\nTuning Parameters: Kc = {Kc}, tau_I = {tau_I}, tau_D = {tau_D} produced an error of: MSE = {MSE},\n",
        "output variance: y_var = {y_var}, manipulated variable variance: u_var = {u_var}\n",
        "\n",
        "Based on the MSE value, please provide a new set of tuning parameters to minimize the MSE.\n",
        "Please provide the results in the indicated format. Do not provide any additional texts.\n",
        "\n",
        "```\n",
        "**4. Parameter Space Definition**  \n",
        "For each controller type, permissible ranges for each tuning parameter are codified. These bounds prevent the LLM from proposing physically unrealistic values (e.g., negative time constants) while granting sufficient freedom for meaningful exploration.\n",
        "\n",
        "**5. Iterative Optimization Execution**  \n",
        "The prepared prompt, iteration template, and configuration are passed to the `prompt_gemini` function, initiating the LLM-based optimization loop. This starts the iterative LLM interactions for the prescribed number of iterations.\n",
        "\n",
        "**6. Persistent Data Archival**  \n",
        "All experimental results are systematically preserved in timestamped JSON files within a dedicated `./Results/` directory structure:\n",
        "- **Conversation transcripts**: Complete LLM dialogue including reasoning, parameter justifications, and decision-making rationale\n",
        "- **Performance trajectories**: Per-iteration metrics capturing the optimization path and convergence behavior\n",
        "\n",
        "This ensures full reproducibility and enables detailed post-hoc analysis of the LLM's reasoning process.\n",
        "\n",
        "**7. Comparative Visualization**  \n",
        "The function culminates by generating comparitive plots, overlaying the IMC and LLM-tuned controller responses in the same figure. It also delegates to the `plot_results` function for parameter convergence analysis, producing comprehensive figures that intuitively demonstrate whether LLM-based optimization can match or surpass traditional model-based tuning.\n",
        "\n",
        "This modular structure ensures that each experimental run is self-contained, reproducible, and yields comprehensive documentation of both the LLM's reasoning process and its quantitative performance outcomes."
      ],
      "metadata": {
        "id": "VvXafFfXGEMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Main Experiment Loop**\n",
        "# --- Main Experiment Loop ---\n",
        "def run_experiment(case, alpha, tau, theta, kp, t_start, t_end, n_samples,\n",
        "                   t_setpoint, setpoint, t_disturbance, disturbance, max_iterations):\n",
        "    params, param_names, controller_type = get_controller_params(case, alpha, tau, theta, kp)\n",
        "    config = dict(\n",
        "        kp=kp, tau=tau, theta=theta,\n",
        "        t_start=t_start, t_end=t_end, n_samples=n_samples,\n",
        "        t_setpoint=t_setpoint, setpoint=setpoint,\n",
        "        t_disturbance=t_disturbance, disturbance=disturbance,\n",
        "        plot_results=True, **params\n",
        "    )\n",
        "    print(\"********************* Baseline: IMC Tuning *********************\")\n",
        "    print(\"IMC params:\", [(name, params[name]) for name in param_names])\n",
        "    setpoint_tracking_mse, disturbance_rejection_mse, mse_overall, y_var, u_var, response_imc = closed_loop_simulation(config)\n",
        "\n",
        "    print(\"\\n\\n********************* LLM Tuning *********************\")\n",
        "    print(\"Controller type:\", controller_type)\n",
        "    print(\"Generate examples:\")\n",
        "    # --- Generate example data ---\n",
        "    example_params = [\n",
        "        {name: val for name, val in zip(param_names, [0.9, 8] + ([1.5] if case > 1 else []) + ([0.5] if case == 3 else []))},\n",
        "        {name: val for name, val in zip(param_names, [0.8, 10] + ([0.3] if case > 1 else []) + ([0.8] if case == 3 else []))}\n",
        "    ]\n",
        "    examples = []\n",
        "    for i, ex_params in enumerate(example_params, 1):\n",
        "        config.update(ex_params)\n",
        "        _, _, mse, yv, uv, _ = closed_loop_simulation(config)\n",
        "        examples.append(generate_example_str(ex_params, mse, i, yv, uv))\n",
        "    examples_str = \"\\n\".join(examples)\n",
        "\n",
        "    # --- Prompt templates ---\n",
        "    prompt_templates = {\n",
        "        1: (\"PI\",\n",
        "            \"\"\"You are a smart and resourceful control systems engineer helping me tune a PI controller to minimize the Mean Squared Error (MSE) between the controlled output variable (y) and the reference (r). The plant system to control is a first-order system with a transport delay. The controller in use is PI controller with two tuning parameters (Kc and tau_I). Your goal is to adjust the tuning parameters: Kc in range {Kc_range} and tau_I in range {tau_I_range} to minimize the MSE, as well as variance in the output controlled variable and the manipulated variable. Next, you will see examples of the tuning parameters (Kc and tau_I) and their corresponding MSE values.\\n\\nBased on MSE and variance values, please provide a new set of tuning parameters you think will help in minimizing the MSE and reduce variance in both the manipulated and output variables.\\nYou will provide your response in the following exact format:\\n    * new tuning parameters (Kc, tau_I) aiming to minimize the mean square error.\\n    * a brief (one sentence) explanation of why you chose that input, considering the current iteration.\\n\\nHere are the examples:\\n{examples}\\n\"\"\",\n",
        "            \"iteration {i}:\\n\\nTuning Parameters: Kc = {Kc}, tau_I = {tau_I}, produced an error of: MSE = {MSE}, output variance: y_var = {y_var}, manipulated variable variance: u_var = {u_var}\\n\\nBased on the MSE value, please provide a new set of tuning parameters to minimize the MSE.\\nPlease provide the results in the indicated format. Do not provide any additional texts.\\n\"),\n",
        "\n",
        "        2: (\"PID\",\n",
        "            \"\"\"You are a smart and resourceful control systems engineer helping me tune a PID controller to minimize the Mean Squared Error (MSE) between the controlled output variable (y) and the reference (r). The plant system to control is a first-order system with a transport delay. The controller in use is PID controller with three tuning parameters (Kc, tau_I, tau_D). Your goal is to adjust the tuning parameters: Kc in range {Kc_range}, tau_I in range {tau_I_range}, and tau_D in range {tau_D_range} to minimize the MSE, as well as variance in the output controlled variable and the manipulated variable. Next, you will see examples of the tuning parameters (Kc, tau_I, tau_D) and their corresponding MSE values.\\n\\nBased on MSE and variance values, please provide a new set of tuning parameters you think will help in minimizing the MSE and reduce variance in both the manipulated and output variables.\\nYou will provide your response in the following exact format:\\n    * new tuning parameters (Kc, tau_I, tau_D) aiming to minimize the mean square error.\\n    * a brief (one sentence) explanation of why you chose that input, considering the current iteration.\\n\\nHere are the examples:\\n{examples}\\n\"\"\",\n",
        "            \"iteration {i}:\\n\\nTuning Parameters: Kc = {Kc}, tau_I = {tau_I}, tau_D = {tau_D} produced an error of: MSE = {MSE}, output variance: y_var = {y_var}, manipulated variable variance: u_var = {u_var}\\n\\nBased on the MSE value, please provide a new set of tuning parameters to minimize the MSE.\\nPlease provide the results in the indicated format. Do not provide any additional texts.\\n\"),\n",
        "\n",
        "        3: (\"PIDwfilter\",\n",
        "            \"\"\"You are a smart and resourceful control systems engineer helping me tune a filtered PID controller to minimize the Mean Squared Error (MSE) between the controlled output variable (y) and the reference (r). The plant system to control is a first-order system with a transport delay. The controller in use is PID with a filter controller with 4 tuning parameters (Kc, tau_I, tau_D, tau_F). Your goal is to adjust the tuning parameters: Kc in range {Kc_range}, tau_I in range {tau_I_range}, tau_D in range {tau_D_range}, and tau_F {tau_F_range} to minimize the MSE, as well as variance in the output controlled variable and the manipulated variable. Next, you will see examples of the tuning parameters (Kc, tau_I, tau_D, tau_F) and their corresponding MSE values.\\n\\nBased on MSE and variance values, please provide a new set of tuning parameters you think will help in minimizing the MSE and reduce variance in both the manipulated and output variables.\\nYou will provide your response in the following exact format:\\n    * new tuning parameters (Kc, tau_I, tau_D, tau_F) aiming to minimize the mean square error.\\n    * a brief (one sentence) explanation of why you chose that input, considering the current iteration.\\n\\nHere are the examples:\\n{examples}\\n\"\"\",\n",
        "            \"iteration {i}:\\n\\nTuning Parameters: Kc = {Kc}, tau_I = {tau_I}, tau_D = {tau_D}, tau_F = {tau_F} produced an error of: MSE = {MSE}, output variance: y_var = {y_var}, manipulated variable variance: u_var = {u_var}\\n\\nBased on the MSE value, please provide a new set of tuning parameters to minimize the MSE.\\nPlease provide the results in the indicated format. Do not provide any additional texts.\\n\")\n",
        "    }\n",
        "\n",
        "    _, prompt_template, iteration_template = prompt_templates[case]\n",
        "\n",
        "    param_ranges = {\n",
        "        1: dict(Kc_range=[0, 1], tau_I_range=[1, 20]),\n",
        "        2: dict(Kc_range=[0, 1], tau_I_range=[1, 20], tau_D_range=[0.2, 2]),\n",
        "        3: dict(Kc_range=[0, 1], tau_I_range=[1, 20], tau_D_range=[0.2, 2], tau_F_range=[0.1, 1])\n",
        "    }\n",
        "\n",
        "    prompt = prompt_template.format(**param_ranges[case], examples=examples_str)\n",
        "    save_directory = f\"closed_loop_1st_order/{controller_type}\"\n",
        "    os.makedirs(\"./Results/\" + save_directory, exist_ok=True)\n",
        "\n",
        "    model_label = model_name\n",
        "    conversation, evaluations, response_llm = prompt_gemini(prompt, iteration_template, max_iterations, config, param_names)\n",
        "\n",
        "    conversation_serializable = [\n",
        "        {\"role\": getattr(c, \"role\", \"unknown\"), \"text\": getattr(c, \"text\", str(c))}\n",
        "        if not isinstance(c, (dict, str)) else c\n",
        "        for c in conversation\n",
        "    ]\n",
        "\n",
        "    d = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "    save_json(conversation_serializable, f\"./Results/{save_directory}/conversation_{model_label}_max-iterations_{len(evaluations)}_{d}.json\")\n",
        "    save_json(evaluations, f\"./Results/{save_directory}/evaluations_{model_label}_max-iterations_{len(evaluations)}_{d}.json\")\n",
        "\n",
        "    plt.figure()\n",
        "    plt.subplot(211)\n",
        "    plt.plot(response_imc.t, response_imc.y[0], label=\"controlled variable IMC tuning (y)\")\n",
        "    plt.plot(response_llm.t, response_llm.y[0], label=\"controlled variable LLM tuning (y)\")\n",
        "    plt.plot(response_imc.t, response_imc.u[0], linestyle='--', label=\"setpoint (r)\")\n",
        "    plt.xlim((response_imc.t[0], response_imc.t[-1]))\n",
        "    plt.grid(True)\n",
        "    plt.ylabel('y')\n",
        "    plt.legend()\n",
        "    plt.subplot(212)\n",
        "    plt.plot(response_imc.t, response_imc.y[1], label='manipulated variable IMC ($u$)')\n",
        "    plt.plot(response_llm.t, response_llm.y[1], label='manipulated variable LLM ($u$)')\n",
        "    plt.plot(response_imc.t, response_imc.u[1], linestyle='--', label='disturbance (d)')\n",
        "    plt.xlim((response_imc.t[0], response_imc.t[-1]))\n",
        "    plt.ylim([-2, 2])\n",
        "    plt.grid(True)\n",
        "    plt.ylabel('u')\n",
        "    plt.xlabel('Time [s]')\n",
        "    plt.legend()\n",
        "    plt.savefig(f\"./Results/{save_directory}/{model_label}-IMC-vs-LLM-{len(evaluations)}-{d}.png\")\n",
        "    plt.show()\n",
        "\n",
        "    plot_results(evaluations, param_names, [params[name] for name in param_names],\n",
        "                 mse_overall, y_var, u_var, save_directory, model_label, d)"
      ],
      "metadata": {
        "id": "QiN7XYPpokHj",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Comparative Study Execution**\n",
        "\n",
        "This cell serves as the main entry point for executing a comprehensive comparative study across multiple controller architectures. When run, it systematically evaluates the LLM's tuning capabilities for PI, PID, and PIDwF controllers under identical process conditions.\n",
        "\n",
        "**The execution sequence establishes a controlled experimental environment:**\n",
        "\n",
        "**1. Reproducibility Framework**  \n",
        "The random seed is fixed and NumPy formatting is standardized to ensure that every experimental run produces deterministic, comparable results. This scientific rigor allows for precise validation of the LLM's performance across multiple trials.\n",
        "\n",
        "**2. Process Characterization**  \n",
        "A unified set of plant parameters defines the system to be controlled: process gain (`kp = 2.5`), time constant (`tau = 3.5`), and time delay (`theta = 2`). These values represent a moderately challenging first-order plus delay system typical of many industrial processes. The simulation horizon, setpoint profile, and disturbance timing are also standardized to create a consistent test protocol. Feel free to play around with these parameters and perform your own experiments!\n",
        "\n",
        "**3. Architectural Comparison Loop**  \n",
        "The function iterates sequentially through the three controller configurations:\n",
        "- **Case 1 (PI)**: 20 iterations of proportional-integral tuning\n",
        "- **Case 2 (PID)**: 20 iterations with derivative action enabled\n",
        "- **Case 3 (PIDwFilter)**: 20 iterations with derivative filtering\n",
        "\n",
        "Each experiment runs independently, with the LLM starting fresh for each controller type, enabling direct comparison of how architectural complexity affects the model's ability to optimize performance.\n",
        "\n",
        "**4. Automated Data Collection**  \n",
        "As each experiment completes, the `run_experiment` function automatically produces and saves:\n",
        "- Timestamped conversation transcripts capturing the LLM's reasoning evolution\n",
        "- Performance metrics tracking convergence behavior\n",
        "- Comparative visualizations of IMC versus LLM-tuned responses\n",
        "\n",
        "**Executing this cell initiates the full experiment**, typically requiring several minutes to complete depending on API response times. Upon completion, the `./Results/` directory will contain a complete dataset documenting the LLM's tuning performance across all controller architectures, ready for analysis and interpretation."
      ],
      "metadata": {
        "id": "szAWffoY0seM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Run All Experiments**\n",
        "if __name__ == \"__main__\":\n",
        "    # Set up environment and random seed\n",
        "    env_file_path = \"keys.env\"\n",
        "    pwd = os.path.dirname(os.getcwd())\n",
        "    env_file_path = os.path.abspath(os.path.join(pwd, env_file_path))\n",
        "    np.random.seed(20)\n",
        "    float_formatter = \"{:.3f}\".format\n",
        "    np.set_printoptions(formatter={'float_kind': float_formatter})\n",
        "\n",
        "    # Common plant and simulation parameters\n",
        "    kp = 2.5\n",
        "    tau = 3.5\n",
        "    theta = 2\n",
        "    alpha = 0.8\n",
        "    t_start = 0\n",
        "    t_end = 100\n",
        "    n_samples = 10000\n",
        "    t_setpoint = 5\n",
        "    setpoint = 1\n",
        "    t_disturbance = 50\n",
        "    disturbance = 1\n",
        "\n",
        "    # Run for PI, PID, PIDwfilter\n",
        "    evaluations = []\n",
        "    chat_history = {}\n",
        "    for case, max_iterations in zip([1, 2, 3], [20, 20, 20]):\n",
        "        run_experiment(case, alpha, tau, theta, kp, t_start, t_end, n_samples, t_setpoint, setpoint, t_disturbance, disturbance, max_iterations)\n"
      ],
      "metadata": {
        "id": "qciypKgVokat",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Summary**\n",
        "\n",
        "This demonstration illustrates how Large Language Models, utilizing an in-context learning framework, can perform iterative numerical optimization of PID controller parameters using only execution traces and performance feedback embedded within the prompt. By presenting the LLM with a small set of initial tuning parameter combinations and their corresponding closed-loop performance metrics (MSE, output variance, and manipulated variable variance), the model is encouraged to infer underlying parameter-performance relationships and propose improved values over time—without being given the process model equations or classical tuning rules. Each iteration provides updated context from actual simulation outcomes, allowing the LLM to adjust its recommendations and progressively refine controller performance through pattern recognition rather than explicit gradient computation, effectively taking on the role of a **control systems engineering assistant**.\n",
        "\n",
        "We purposely refrain from passing the true model parameters ($k_p$, $\\tau$, $\\theta$) to the LLM in our prompt, ensuring that the tuning parameters result purely from iterative conversational interactions, rather than leveraging nascent knowledge of classical control theory. Through this setup, we observe how the LLM implicitly constructs a \"textual gradient,\" behaving as an optimizer driven by linguistic reasoning and contextual feedback.\n",
        "\n",
        "Our results show that the LLM-tuned PID controller achieves **performance comparable to the well-established IMC method**, and in some configurations even surpasses it based on the provided quantitative metrics. However, as control systems engineers recognize, MSE and variance represent only a subset of closed-loop performance criteria. **Oscillatory behavior, rise time, settling time, robustness margins, and other temporal characteristics** are equally critical depending on the application. This work demonstrates the LLM's capability to optimize multi-objective problems, a flexibility that can be extended to incorporate these additional criteria as needed.\n",
        "\n",
        "**Future Directions:** Beyond this PID tuning example, we plan to explore the generalizability of this approach to a broader range of control systems engineering challenges. By leveraging the multimodal capabilities of modern LLMs—embedding closed-loop step responses, frequency-domain characteristics, or constraint violations directly within the prompt as plots or data traces—we believe LLMs can be guided to refine controller parameters, adapt to process changes, or even suggest structural modifications for cascade and multivariable systems. These extensions would further highlight the flexibility of in-context learning for enabling self-improvement loops in control engineering.\n",
        "\n",
        "## **Conclusion**\n",
        "\n",
        "This tutorial demonstrates that LLMs can act as effective, gradient-free numerical optimizers for PID controller tuning using only the information supplied in the prompt—capturing the core idea of in-context learning in a classical control systems setting. Without receiving process model parameters, analytical derivatives, or traditional tuning methodologies, the LLM learns from the pattern of tuning parameters and their measured performance to generate increasingly effective PID settings over successive iterations. While the LLM is not performing true dynamic simulation internally, its ability to discover improved controller parameters purely from contextual feedback validates a key insight: **LLMs can acting as an optimizer and drive classical controllers fine-tuning through structured conversational prompting alone, taking the role of an control systems engineering assistant**.\n",
        "\n",
        "Our implementation successfully tunes PI, PID, and PIDwF controllers for a first-order system with delay, achieving performance that can be quantitatively compared against the established IMC baseline. The preservation of complete conversation transcripts—including the LLM's one-sentence reasoning at each iteration—provides an unprecedented level of **explainability in automated controller tuning**. This work demonstrates a practical and interpretable pathway for integrating LLMs into control engineering workflows, where language becomes the substrate for iterative design refinement and knowledge transfer from simulation to actionable tuning recommendations. By refraining from providing the LLM with underlying model parameters, we validate that the optimization emerges genuinely from the conversational interaction, rather than pre-existing control theory memorization—representing a true advancement in AI-assisted control systems engineering.\n",
        "\n",
        "## **References**\n",
        "\n",
        "Rivera, D. E., Morari, M., & Skogestad, S. (1986). Internal model control: PID controller design. Industrial & engineering chemistry process design and development, 25(1), 252-265."
      ],
      "metadata": {
        "id": "Xau9YNcm4Wez"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mOAv7fR4raSz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}