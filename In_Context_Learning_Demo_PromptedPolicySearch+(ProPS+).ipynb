{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhX16i8VfLtT"
      },
      "source": [
        "# **Prompted Policy Search + Envrionment Description (ProPS+): Reinforcement Learning through Linguistic and Numerical Reasoning in LLMs**\n",
        "\n",
        "This notebook serves as a detailed tutorial on ProPS+, an extension of the *[Prompted Policy Search (ProPS)](https://props-llm.github.io/)* (Zhou et al., 2025) reinfocement learning method that unifies numerical and linguistic reasoning within a single framework. ProPS places a large language model (LLM) at the center of the policy optimization loop; directly proposing policy updates based on both reward feedback and natural language input.\n",
        "\n",
        "ProPS+ extends this framework by bridging the gap between numerical optimization and semantic understanding. **It extends ProPS by adding rich, task-specific and contextual knowledge via semantically-informed prompts**, allowing the LLM to perform \"Linguistic Reasoning\" alongside numerical optimization.\n",
        "\n",
        "In this tutorial, we utilize a Large Language Model (LLM), specifically the Gemini 2.0/2.5 Flash, to perform Policy Search for a linear continuous control policy within an OpenAI Gym reinforcement learning environment. Our focus will be on the Swimmer environment, where we aim to employ the Gemini family of models to discover the optimal parameters for a linear policy that enables the agent to successfully perform forward locomotion.\n",
        "\n",
        "## **Environment: State and Action Variables**\n",
        "<img src=\"https://raw.githubusercontent.com/intro-to-icl/intro-to-icl.github.io/refs/heads/master/static/images/swimmer.gif\" height=\"200\"><img src=\"https://raw.githubusercontent.com/intro-to-icl/intro-to-icl.github.io/refs/heads/master/static/images/swimmer_parameters.png\" height=\"200\">\n",
        "\n",
        "\n",
        "The Swimmer-v5 environment presents a challenging continuous control problem. The agent is a simple swimmer composed of three rigid links connected by two actuated rotational joints (rotors). This chain-like structure is simulated in a viscous fluid. The primary objective for the swimmer is to move forward (typically along the positive x-axis) as quickly as possible by applying torques to its two rotors. The interaction with the fluid and the multi-link dynamics make this a non-trivial control task. This environment can be modeled as a Markov Decision Process (MDP), where the next state and reward are determined by the current state and the action taken. Mujoco environments are generally deterministic given the same initial conditions and actions.\n",
        "\n",
        "At each timestep, the agent receives an observation of the environment's current state. For Swimmer-v5, this state is represented by an 8-dimensional continuous vector:\n",
        "\n",
        "$$ S^{T} = [q_{tip}, q_{rotor1}, q_{rotor2}, v_x, v_y, \\omega_{tip}, \\omega_{rotor1}, \\omega_{rotor2} ] $$\n",
        "\n",
        "Where:\n",
        "*   $q_{tip}$: Angle of the front tip (the first link).\n",
        "*   $q_{rotor1}$: Angle of the first rotor.\n",
        "*   $q_{rotor2}$: Angle of the second rotor.\n",
        "*   $v_x$: Velocity of the tip along the x-axis (forward direction).\n",
        "*   $v_y$: Velocity of the tip along the y-axis.\n",
        "*   $\\omega_{tip}$: Angular velocity of the front tip.\n",
        "*   $\\omega_{rotor1}$: Angular velocity of the first rotor.\n",
        "*   $\\omega_{rotor2}$: Angular velocity of the second rotor.\n",
        "\n",
        "Control over the swimmer is exerted by applying torques to its two rotors. The action $A$ is a 2-dimensional continuous vector:\n",
        "$$ A = [ \\tau_1, \\tau_2 ] $$\n",
        "\n",
        "Here, $\\tau_1$ is the torque applied to the first rotor, and $\\tau_2$ is the torque applied to the second rotor. Both torque values are typically clipped within the range [-1, 1]. The reward function in Swimmer-v5 is primarily based on forward locomotion - encouraging the agent to swim quickly in the target direction, and a control penalty for taking actions too large.\n",
        "\n",
        "## **Policy Representation**\n",
        "In reinforcement learning, the agent's behavior is dictated by a \"policy,\" which essentially maps observed states to appropriate actions. For this specific problem, we adopt a straightforward **linear policy**. This implies that the action is calculated as a linear combination of the current state variables (position and velocity). The action (torques) will be a linear combination of the observed state variables. Given the 8 state variables and 2 action variables, the policy - π will be parameterized by a parameter matrix $θ$ of shape 8x2:\n",
        "$$ π_θ = \\begin{bmatrix}\n",
        "θ_{1,1} & θ_{1,2} \\\\\n",
        "θ_{2,1} & θ_{2,2} \\\\\n",
        "\\vdots & \\vdots \\\\\n",
        "θ_{8,1} & θ_{8,2}\n",
        "\\end{bmatrix} $$\n",
        "\n",
        "Given a state vector $S$ (an 8x1 vector), the action vector $A$ (a 2x1 vector) is computed as:\n",
        "\n",
        "$$ A = S^T θ $$\n",
        "\n",
        "Each element $A_j = \\sum_{i=1}^{8} S_i \\cdot θ_{i,j}$. The parameters $θ_{i,j}$ determine the influence of the $i$-th state variable on the $j$-th action (torque). The optimization goal is to find the 16 parameters in $θ$ that maximize the total reward accumulated over an episode.\n",
        "\n",
        "## **Optimization Strategy: LLM-Driven Policy Search**\n",
        "The core objective is to identify the optimal policy parameters $π_θ$ that maximize the cumulative reward $R$ gathered over a complete episode, which consists of a sequence of steps from the start until termination (either reaching the goal or hitting the maximum step limit, e.g., 1000). This task is formally known as **Policy Search**. Mathematically, we seek to solve:\n",
        "$$ \\max_{θ} \\mathbb{E}\\left[ \\sum_{t=0}^{T} r_t \\right]$$\n",
        "\n",
        "where $r_t$ is the reward at timestep $t$ and $T$ is the episode length.\n",
        "\n",
        "To facilitate this optimization, we utilize a \"Replay Buffer.\" After each episode concludes, having been run with the current parameters $θ$, the total reward $R$ is calculated. This $(θ, R)$ pair is then stored in the buffer. The optimizer, which is the LLM in our case, observes the $(θ, R)$ pair, and the domain knowledge of the environment.\n",
        "\n",
        "\n",
        "## **LLM as the Optimizer**\n",
        "\n",
        "We harness the capabilities of the Gemini models to conduct this optimization. The LLM is instructed via a prompt to function as an optimization assistant. Furthermore, the prompt is augmented with the domain knowledge including the description of the environment, detailed definitions of the parameter types, specifications of the policy structure, and human provided hints regarding optimal behavior in the Swimmer environment. This allows the LLM to use common knowledge about the environment, driving faster convergence.\n",
        "</br>\n",
        "</br>\n",
        "\n",
        "##### ProPS+ Prompt Augmentation\n",
        "```\n",
        "The swimmers consist of three or more segments (’links’) and one less articulation joints (’rotors’) - one rotor joint connects exactly two links to form a linear chain. The swimmer is suspended in a two-dimensional pool and always starts in the same position (subject to some deviation drawn from a uniform distribution), and the goal is to move as fast as possible towards the right by applying torque to the rotors and using fluid friction.\n",
        "The state is a vector of 8 elements, representing the following:\n",
        "- state[0] angle of the front tip (-inf to inf rad)\n",
        "- state[1] angle of the first rotor (-inf to inf rad)\n",
        "- state[2] angle of the second rotor (-inf to inf rad)\n",
        "- state[3] velocity of the tip along the x-axis (-inf to inf m/s)\n",
        "- state[4] velocity of the tip along the y-axis (-inf to inf m/s)\n",
        "- state[5] angular velocity of front tip (-inf to inf rad/s)\n",
        "- state[6] angular velocity of first rotor (-inf to inf rad/s)\n",
        "- state[7] angular velocity of second rotor (-inf to inf rad/s)\n",
        "The action space is a vector of 2 float numbers, representing the torques applied between the links (-1 to 1 N).\n",
        "The policy is a linear policy with 5 parameters and works as follows:\n",
        "action = argmax(state @ W + B), where\n",
        "state = [state[0], state[1], state[2], state[3], state[4], state[5], state[6], state[7]]\n",
        "W = [[params[0], params[1]],\n",
        "    [params[2], params[3]],\n",
        "    [params[4], params[5]],\n",
        "    [params[6], params[7]],\n",
        "    [params[8], params[9]],\n",
        "    [params[10], params[11]],\n",
        "    [params[12], params[13]],\n",
        "    [params[14], params[15]]]\n",
        "b = [params[16], params[17]]\n",
        "The goal is to try to move forward. However, in the meantime, the control cost should also be minimized. The reward function is as follows:\n",
        "reward = x-velocity - 1e-4 * action^2.\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "The process begins with a \"warmup\" phase, where several episodes are run using randomly selected parameters $π_θ$. The resulting $(θ, R)$ pairs populate the Replay Buffer, providing initial data. Subsequently, the LLM is presented with a detailed prompt containing the optimization goal, the historical data from the Replay Buffer, output format instructions, and guidance on balancing exploration (trying novel parameters) versus exploitation (refining promising parameters), adapting this balance as the optimization progresses. Based on this prompt and the historical context (enabling in-context learning), the LLM proposes a new set of parameters $θ$ anticipated to yield improved rewards.\n",
        "\n",
        "<br />\n",
        "<p style=\"text-align:center;\">\n",
        "<img src=\"https://github.com/k-pratyush/props-llm-examples/blob/main/static/approach_overview_props_plus.png?raw=1\" alt=\"image\" width=350>\n",
        "</p>\n",
        "Figure (a): Props+ Optimization Approach\n",
        "\n",
        "<br />\n",
        "<br />\n",
        "\n",
        "The agent's policy is then updated with these suggested parameters, and one or more evaluation episodes are executed in the environment. The cumulative reward obtained from these evaluations is recorded, and the new $(π_θ, R)$ pair is added to the Replay Buffer. This cycle of prompting the LLM, receiving parameter suggestions, evaluating the updated policy, and updating the buffer is repeated for a predetermined number of episodes (e.g., 400), allowing the LLM to iteratively refine the policy parameters towards optimality. The prompt design treats the task purely as optimizing an unknown function $f(θ_1, θ_2) = R$, guiding the LLM with hints on step size and search ranges but without revealing the underlying simulation details.\n",
        "\n",
        "\n",
        "## **Code Overview**\n",
        "\n",
        "The implementation follows a modular design. The **World** component (`ContinualSpaceGeneralWorld`) wraps the standard Gymnasium environment, managing state transitions, reward calculations, and episode termination. The **Agent** component (`LLMNumOptimAgent`) integrates the learning elements. It includes the **`LinearPolicy`** module, which stores the policy parameter matrix $θ$ and computes actions based on states. It also contains the **`EpisodeRewardBufferNoBias`** module, responsible for maintaining the Replay Buffer of ($θ$ , $R$) pairs. Finally, the **`LLMBrain`** module orchestrates all interactions with the LLM, including prompt generation using Jinja2 templates, API communication (handling both OpenAI and Gemini models), and parsing the LLM's responses to extract the suggested new parameters.\n",
        "\n",
        "## **Hyperparameters**\n",
        "\n",
        "Several hyperparameters govern the experiment's execution. `NUM_EPISODES` (e.g., 400) sets the total number of optimization iterations. `RENDER_MODE` controls environment visualization. `MAX_TRAJ_COUNT` (e.g., 1000) defines the Replay Buffer size, influencing the historical context available to the LLM. `MAX_TRAJ_LENGTH` (e.g., 1000) sets the maximum steps per episode. `LLM_MODEL_NAME` specifies the LLM used. `NUM_EVALUATION_EPISODES` (e.g., 20) determines how many runs are averaged to evaluate a new policy. `WARMUP_EPISODES` (e.g., 20) sets the number of initial random runs. `SEARCH_STD` (e.g., 1.0) provides a hint to the LLM regarding the step size for parameter exploration.\n",
        "\n",
        "## **Training Loop**\n",
        "<p style=\"text-align:center;\">\n",
        "<img src=\"https://raw.githubusercontent.com/intro-to-icl/intro-to-icl.github.io/refs/heads/master/static/images/swimmer_loop.gif\" alt=\"image\" height=\"350\">\n",
        "</p>\n",
        "Figure (b): Code Overview\n",
        "\n",
        "<br />\n",
        "<br />\n",
        "\n",
        "The `run_training_loop` function orchestrates the process. It initializes the World and Agent components. It performs the initial warmup runs if necessary, populating the replay buffer. Then, it enters the main loop, iterating `NUM_EPISODES` times. In each iteration, it interacts with the LLM (`agent.train_policy`) to get updated policy parameters based on the replay buffer history. It then evaluates the performance of this new policy over `NUM_EVALUATION_EPISODES` (`agent.evaluate_policy`), calculates the cumulative reward, and adds the new (parameters, cumulative reward) pair back into the replay buffer. Logging occurs at each step.\n",
        "\n",
        "## **Output Structure**\n",
        "\n",
        "The training process generates structured logs. A main log directory contains subdirectories for each episode (`episode_*`) and potentially a `warmup/` directory. Each episode directory stores logs of evaluation trajectories, the parameters suggested by the LLM for that episode (`parameters.txt`), and the full LLM interaction including its reasoning (`parameters_reasoning.txt`). The final notebook cells typically include code for visualizing the learned policy in action and plotting the reward curve over episodes, illustrating the learning progress."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Df4h5KlfLtW"
      },
      "source": [
        "## **Before Running the Demo, Follow the Instructions Below**\n",
        "To run the full experiment:\n",
        "1. Ensure all dependencies are imported and installed.\n",
        "2.\tVisit Google AI Studio (https://aistudio.google.com/) to obtain your Gemini API key.\n",
        "3.\tOnce API key is generated, copy and paste it into the demo when prompted.\n",
        "\n",
        "#### **If having trouble creating an API Key, follow the link below for instructions:**\n",
        "* #### **[Instructions on How to Obtain a Gemini API Key](https://docs.google.com/document/d/17pgikIpvZCBgcRh4NNrcee-zp3Id0pU0vq2SG52KIGE/edit?usp=sharing)**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "uH1ovM2YfLtY"
      },
      "outputs": [],
      "source": [
        "#@title **Import and Install Necessary Libraries**\n",
        "!pip install gymnasium[mujoco]\n",
        "\n",
        "import re\n",
        "import os\n",
        "import time\n",
        "from decimal import Decimal\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "import traceback\n",
        "import numpy as np\n",
        "from IPython.display import display, update_display\n",
        "import imageio.v2 as imageio\n",
        "\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from jinja2 import Template\n",
        "from google import genai\n",
        "import getpass\n",
        "import ipywidgets as widgets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Rjl9Zcyo7Mk1"
      },
      "outputs": [],
      "source": [
        "#@title **Setting Up Gemini Client**\n",
        "apikey = getpass.getpass(\"Enter your Gemini API Key: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "QJLqzj72AMZ4"
      },
      "outputs": [],
      "source": [
        "#@title **Choose a Model**\n",
        "model_dropdown = widgets.Dropdown(\n",
        "    options=[\n",
        "        (\"Gemini 2.5 Flash\", \"gemini-2.5-flash\"),\n",
        "        (\"Gemini 2.0 Flash\", \"gemini-2.0-flash\")\n",
        "    ],\n",
        "    description=\"Model:\",\n",
        "    value=\"gemini-2.5-flash\",\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "confirm_button = widgets.Button(\n",
        "    description=\"Confirm Selection\"\n",
        ")\n",
        "\n",
        "output = widgets.Output()\n",
        "\n",
        "model_name = None\n",
        "\n",
        "def on_confirm_click(b):\n",
        "    global model_name, batch_size\n",
        "\n",
        "    model_name = model_dropdown.value\n",
        "\n",
        "    with output:\n",
        "        output.clear_output()\n",
        "        print(f\"\\nSelected model: {model_name}\")\n",
        "\n",
        "confirm_button.on_click(on_confirm_click)\n",
        "\n",
        "display(model_dropdown, confirm_button, output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGkrpEXCfLtZ"
      },
      "source": [
        "This cell introduces and lists the key hyperparameters that control the execution and behavior of the reinforcement learning experiment. Hyperparameters are settings that are not learned by the agent itself but are defined by the user before the training process begins. They significantly influence the learning process and the performance of the agent.\n",
        "\n",
        "`NUM_EPISODES` (e.g., 400): Defines the total number of optimization iterations or training episodes the agent will go through. A higher number allows for more learning but increases computation time.\n",
        "\n",
        "`RENDER_MODE` (e.g., None): Controls how the environment is visualized during execution. Options typically include 'human' (real-time window), 'rgb_array' (returns a pixel array, useful for recording), or None (no visualization, fastest for training).\n",
        "\n",
        "`MAX_TRAJ_COUNT` (e.g., 1000): Sets the maximum size of the Replay Buffer. This buffer stores (policy parameters, reward) pairs, and its size determines how much historical data the LLM has access to when making decisions.\n",
        "\n",
        "`MAX_TRAJ_LENGTH` (e.g., 1000): Specifies the maximum number of steps allowed in a single episode. If the agent doesn't reach a terminal state within these steps, the episode is truncated.\n",
        "\n",
        "`LLM_MODEL_NAME` (e.g., \"gemini-2.5-flash-preview-04-17\"): Specifies which Large Language Model will be used as the optimizer. The comment lists several compatible models from OpenAI and Google.\n",
        "NUM_EVALUATION_EPISODES (e.g., 20): Determines how many times a newly proposed policy is run in the environment to get an average measure of its performance. Averaging helps to reduce variance in the reward signal.\n",
        "\n",
        "`WARMUP_EPISODES` (e.g., 20): Sets the number of initial episodes run with randomly generated policy parameters. This \"warmup\" phase populates the Replay Buffer with some initial data points before the LLM starts optimizing.\n",
        "\n",
        "`SEARCH_STD` (e.g., 1.0): Provides a hint to the LLM regarding the standard deviation or step size it should consider when exploring new parameter values, especially during the initial exploration phase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1qhMEUNAfLta"
      },
      "outputs": [],
      "source": [
        "#@title **Key Hyperparameters**\n",
        "NUM_EPISODES=200 # Total number of episodes to train for\n",
        "RENDER_MODE=None # Choose from 'human', 'rgb_array', or None\n",
        "MAX_TRAJ_COUNT=1000 # Maximum number of trajectories to store in buffer for prompt\n",
        "MAX_TRAJ_LENGTH=1000 # Maximum number of steps in a trajectory\n",
        "LLM_MODEL_NAME=model_name\n",
        "\n",
        "NUM_EVALUATION_EPISODES=20 # Number of episodes to generate agent rollouts for evaluation\n",
        "WARMUP_EPISODES=20 # Number of randomly generated initial episodes\n",
        "SEARCH_STD=1.0 # Step size for LLM to search for optimal parameters during exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYR4NMPlfLta"
      },
      "source": [
        "The below cell defines the template for the black box optimization prompt. The prompt template uses variables defined in the code for setting the number of parameters required to optimize, the global optimum of the function, step size, current step count and the history of (parameter, reward) tuples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mKmx_o8jfLtb"
      },
      "outputs": [],
      "source": [
        "#@title **Black Box Optimization Prompt Example**\n",
        "LLM_SI_TEMPLATE_STRING = \"\"\"\n",
        "You are good global RL policy optimizer, helping me find the global optimal policy in the following environment:\n",
        "\n",
        "# Environment: {{ env_description }}\n",
        "\n",
        "# Regarding the parameters **params**: **params** is an array of rank float numbers.\n",
        "**params** values are in the range of [-6.0, 6.0] with 1 decimal place. params represent a\n",
        "linear policy. f(params) is the episodic reward of the policy.\n",
        "\n",
        "# Here's how we'll interact:\n",
        "1. I will first provide MAX_STEPS (200) along with a few training examples.\n",
        "2. You will provide your response in the following exact format:\n",
        "    * Line 1: a new input 'params[0]: , params[1]: , params[2]: ,..., params[{{ rank - 1 }}]: ', aiming to maximize the function's value f(params).\n",
        "    Please propose params values in the range of [-6.0, 6.0], with 1 decimal place.\n",
        "    * Line 2: detailed explanation of why you chose that input.\n",
        "3. I will then provide the function's value f(params) at that point, and the current iteration.\n",
        "4. We will repeat steps 2-3 until we reach the maximum number of iterations.\n",
        "\n",
        "# Remember:\n",
        "1. **Do not propose previously seen params.**\n",
        "2. **The global optimum should be around {{ optimum }}.** If you are below that, this is just a local optimum. You should explore instead of exploiting.\n",
        "3. Search both positive and negative values. **During exploration, use search step size of {{ step_size }}**.\n",
        "\n",
        "\n",
        "Next, you will see examples of params and their episodic reward f(params) pairs.\n",
        "{{ episode_reward_buffer_string }}\n",
        "\n",
        "Now you are at iteration {{step_number}} out of 200. Please provide the results in the indicated format. Do not provide any additional texts.\"\"\"\n",
        "\n",
        "\n",
        "llm_si_template = Template(LLM_SI_TEMPLATE_STRING)\n",
        "llm_output_conversion_template = llm_si_template"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAhQnfn5fLtb"
      },
      "source": [
        "## **World**\n",
        "\n",
        "The `ContinualSpaceGeneralWorld` is a wrapper class over the Gymnasium environments to give standardized interface for the agents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9eUd-vPPfLtc"
      },
      "outputs": [],
      "source": [
        "#@title **Swimmer-v5**\n",
        "import gymnasium as gym\n",
        "\n",
        "\n",
        "class ContinualSpaceGeneralWorld():\n",
        "    def __init__(\n",
        "        self,\n",
        "        gym_env_name,\n",
        "        render_mode,\n",
        "        max_traj_length=1000,\n",
        "    ):\n",
        "        assert render_mode in [\"human\", \"rgb_array\", None]\n",
        "\n",
        "        if gym_env_name == \"gym_navigation:NavigationTrack-v0\":\n",
        "            self.env = gym.make(\n",
        "                gym_env_name,\n",
        "                render_mode=render_mode,\n",
        "                track_id=1,\n",
        "            )\n",
        "        elif gym_env_name == \"maze-sample-3x3-v0\":\n",
        "            self.env = gym.make(\n",
        "                gym_env_name,\n",
        "                enable_render=render_mode,\n",
        "            )\n",
        "        else:\n",
        "            self.env = gym.make(gym_env_name, render_mode=render_mode)\n",
        "        self.gym_env_name = gym_env_name\n",
        "        self.render_mode = render_mode\n",
        "        self.steps = 0\n",
        "        self.accu_reward = 0\n",
        "        self.max_traj_length = max_traj_length\n",
        "        if isinstance(self.env.action_space, gym.spaces.Discrete):\n",
        "            self.discretize = True\n",
        "        else:\n",
        "            self.discretize = False\n",
        "\n",
        "    def reset(self, new_reward=False):\n",
        "        \"\"\" This method resets the environment to its initial state.\n",
        "        If `new_reward` is True, it initializes the environment with a different reward structure.\n",
        "        \"\"\"\n",
        "        del self.env\n",
        "        if not new_reward:\n",
        "            self.env = gym.make(self.gym_env_name, render_mode=self.render_mode)\n",
        "        else:\n",
        "            self.env = gym.make(self.gym_env_name, render_mode=self.render_mode, healthy_reward=0)\n",
        "\n",
        "        state, _ = self.env.reset()\n",
        "        self.steps = 0\n",
        "        self.accu_reward = 0\n",
        "        return state\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        This method executes a step in the environment with the given action.\n",
        "        It updates the environment state, accumulates the reward, and checks if the episode is done.\n",
        "        \"\"\"\n",
        "        self.steps += 1\n",
        "        action = np.asarray(action).reshape(-1)\n",
        "        state, reward, terminated, truncated, _ = self.env.step(action)\n",
        "        self.accu_reward += reward\n",
        "\n",
        "        if self.steps >= self.max_traj_length or terminated or truncated:\n",
        "            done = True\n",
        "        else:\n",
        "            done = False\n",
        "\n",
        "        return state, reward, done\n",
        "\n",
        "    def get_accu_reward(self):\n",
        "        \"\"\"\n",
        "        This method returns the accumulated reward for the current episode.\n",
        "        \"\"\"\n",
        "        return self.accu_reward\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4dt8I0DfLte"
      },
      "source": [
        "## **Sub Modules**\n",
        "\n",
        "`EpisodeRewardBufferNoBias`: Store and manage collection of (policy parameters and reward) pairs, acting as the replay buffer.\n",
        "\n",
        "`LinearPolicy`: Implements a linear policy where the action is computed as a dot product of the state and weights, plus a bias term: $a = s^T W + b$.\n",
        "\n",
        "`LinearPolicyNoBias`: Implements a linear policy without a bias term: $a = s^T W$.\n",
        "\n",
        "`LLMBrain`: Coordinates with the LLM to get new parameters for the policy based on existing policy (parameter, reward) pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xzC6ZOoqfLte"
      },
      "outputs": [],
      "source": [
        "#@title **Details**\n",
        "class EpisodeRewardBufferNoBias:\n",
        "    def __init__(self, max_size):\n",
        "        self.buffer = deque(maxlen=max_size)\n",
        "\n",
        "    def add(self, weights: np.ndarray, reward):\n",
        "        self.buffer.append((weights, reward))\n",
        "\n",
        "    def sort(self):\n",
        "        self.buffer = deque(sorted(self.buffer, key=lambda x: x[1], reverse=False), maxlen=self.buffer.maxlen)\n",
        "\n",
        "    def __str__(self):\n",
        "        buffer_table = \"Parameters | Reward\\n\"\n",
        "        for weights, reward in self.buffer:\n",
        "            buffer_table += f\"{weights.reshape(1, -1)} | {reward}\\n\"\n",
        "        return buffer_table\n",
        "\n",
        "    def load(self, folder):\n",
        "        # Find all episode files\n",
        "        all_files = [os.path.join(folder, x) for x in os.listdir(folder) if x.startswith('warmup_rollout')]\n",
        "        all_files.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
        "\n",
        "        # Load parameters from all episodes\n",
        "        for filename in all_files:\n",
        "            with open(filename, 'r') as f:\n",
        "                lines = f.readlines()\n",
        "                parameters = []\n",
        "                for line in lines:\n",
        "                    if \"parameter ends\" in line:\n",
        "                        break\n",
        "                    try:\n",
        "                        parameters.append([float(x) for x in line.split(',')])\n",
        "                    except:\n",
        "                        continue\n",
        "                parameters = np.array(parameters)\n",
        "\n",
        "                rewards = []\n",
        "                for line in lines:\n",
        "                    if \"Total reward\" in line:\n",
        "                        try:\n",
        "                            rewards.append(float(line.split()[-1]))\n",
        "                        except:\n",
        "                            continue\n",
        "                rewards_mean = np.mean(rewards)\n",
        "                self.add(parameters, rewards_mean)\n",
        "                f.close()\n",
        "        print(self)\n",
        "\n",
        "\n",
        "class LinearPolicy():\n",
        "    \"\"\"\n",
        "    Linear policy for continuous action space. The policy is represented as a (2,1) matrix of weights.\n",
        "    Next action is calculated as the dot product of the state and the weight matrix.\n",
        "    state.T * weight + bias -> action\n",
        "    (1,2) * (2,1) + (1,1) -> (1,1)\n",
        "    \"\"\"\n",
        "    def __init__(self, dim_states, dim_actions):\n",
        "\n",
        "        self.dim_states =dim_states\n",
        "        self.dim_actions = dim_actions\n",
        "\n",
        "        self.weight = np.random.rand(self.dim_states, self.dim_actions)\n",
        "        self.bias = np.random.rand(1, self.dim_actions)\n",
        "\n",
        "    def initialize_policy(self):\n",
        "        self.weight = np.round(np.random.normal(0., 3., size=(self.dim_states, self.dim_actions)), 1)\n",
        "        self.bias = np.round(np.random.normal(0., 3., size=(1, self.dim_actions)), 1)\n",
        "\n",
        "    def get_action(self, state):\n",
        "        state = state.T\n",
        "        return np.matmul(state, self.weight) + self.bias\n",
        "\n",
        "    def __str__(self):\n",
        "        output = \"Weights:\\n\"\n",
        "        for w in self.weight:\n",
        "            output += \", \".join([str(i) for i in w])\n",
        "            output += \"\\n\"\n",
        "\n",
        "        output += \"Bias:\\n\"\n",
        "        for b in self.bias:\n",
        "            output += \", \".join([str(i) for i in b])\n",
        "            output += \"\\n\"\n",
        "\n",
        "        return output\n",
        "\n",
        "    def update_policy(self, weight_and_bias_list):\n",
        "        if weight_and_bias_list is None:\n",
        "            return\n",
        "\n",
        "        weight_and_bias_list = np.array(weight_and_bias_list).reshape(self.dim_states + 1, self.dim_actions)\n",
        "        self.weight = np.array(weight_and_bias_list[:-1])\n",
        "        self.bias = np.expand_dims(np.array(weight_and_bias_list[-1]), axis=0)\n",
        "\n",
        "    def get_parameters(self):\n",
        "        parameters = np.concatenate((self.weight, self.bias), axis=0)\n",
        "        return parameters\n",
        "\n",
        "class LinearPolicyNoBias():\n",
        "    def __init__(self, dim_states, dim_actions):\n",
        "\n",
        "        self.dim_states = dim_states\n",
        "        self.dim_actions = dim_actions\n",
        "\n",
        "        self.weight = np.random.rand(self.dim_states, self.dim_actions)\n",
        "\n",
        "    def initialize_policy(self):\n",
        "        self.weight = np.round((np.random.rand(self.dim_states, self.dim_actions) - 0.5) * 6, 1)\n",
        "\n",
        "    def get_action(self, state):\n",
        "        state = state.T\n",
        "        return np.matmul(state, self.weight)\n",
        "\n",
        "    def __str__(self):\n",
        "        output = \"Weights:\\n\"\n",
        "        for w in self.weight:\n",
        "            output += \", \".join([str(i) for i in w])\n",
        "            output += \"\\n\"\n",
        "\n",
        "        return output\n",
        "\n",
        "    def update_policy(self, weight_and_bias_list):\n",
        "        if weight_and_bias_list is None:\n",
        "            return\n",
        "        self.weight = np.array(weight_and_bias_list)\n",
        "        self.weight = self.weight.reshape(-1)\n",
        "        for i in range(len(self.weight)):\n",
        "            self.weight[i] = Decimal(self.weight[i]).normalize()\n",
        "\n",
        "        self.weight = self.weight.reshape(\n",
        "            self.dim_states, self.dim_actions\n",
        "        )\n",
        "\n",
        "    def get_parameters(self):\n",
        "        return self.weight\n",
        "\n",
        "\n",
        "class LLMBrain:\n",
        "    def __init__(\n",
        "        self,\n",
        "        llm_si_template: Template,\n",
        "        llm_output_conversion_template: Template,\n",
        "        llm_model_name: str,\n",
        "    ):\n",
        "        self.llm_si_template = llm_si_template\n",
        "        self.llm_output_conversion_template = llm_output_conversion_template\n",
        "        self.llm_conversation = []\n",
        "        assert llm_model_name in [\n",
        "            \"gemini-2.0-flash\"\n",
        "        ]\n",
        "        self.llm_model_name = llm_model_name\n",
        "        if \"gemini\" in llm_model_name:\n",
        "            self.model_group = \"gemini\"\n",
        "            self.client = genai.Client(api_key=apikey)\n",
        "        elif \"claude\" in llm_model_name:\n",
        "            self.model_group = \"anthropic\"\n",
        "            self.client = anthropic.Client(api_key=os.environ[\"ANTHROPIC_API_KEY\"])\n",
        "        else:\n",
        "            self.model_group = \"openai\"\n",
        "            self.client = OpenAI()\n",
        "\n",
        "    def reset_llm_conversation(self):\n",
        "        self.llm_conversation = []\n",
        "\n",
        "    def add_llm_conversation(self, text, role):\n",
        "        if self.model_group == \"openai\":\n",
        "            self.llm_conversation.append({\"role\": role, \"content\": text})\n",
        "        elif self.model_group == \"anthropic\":\n",
        "            self.llm_conversation.append({\"role\": role, \"content\": text})\n",
        "        else:\n",
        "            self.llm_conversation.append({\"role\": role, \"parts\": text})\n",
        "\n",
        "    def query_llm(self):\n",
        "        for attempt in range(10):\n",
        "            try:\n",
        "                if self.model_group == \"openai\":\n",
        "                    completion = self.client.chat.completions.create(\n",
        "                        model=self.llm_model_name,\n",
        "                        messages=self.llm_conversation,\n",
        "                    )\n",
        "                    response = completion.choices[0].message.content\n",
        "                elif self.model_group == \"anthropic\":\n",
        "                    message = self.client.messages.create(\n",
        "                        model=self.llm_model_name,\n",
        "                        messages=self.llm_conversation,\n",
        "                        max_tokens=1024,\n",
        "                    )\n",
        "                    response = message.content[0].text\n",
        "                else:\n",
        "                    prompt = self.llm_conversation[-1][\"parts\"]\n",
        "                    response = self.client.models.generate_content(\n",
        "                        model=model_name,\n",
        "                        contents=[prompt]\n",
        "                    )\n",
        "                    response = response.text\n",
        "            except Exception as e:\n",
        "                print(f\"Error: {e}\")\n",
        "                print(\"Retrying...\")\n",
        "                if attempt == 9:\n",
        "                    raise Exception(\"Failed\")\n",
        "                else:\n",
        "                    print(\"Waiting for 60 seconds before retrying...\")\n",
        "                    time.sleep(60)\n",
        "\n",
        "            if self.model_group == \"openai\":\n",
        "                # add the response to self.llm_conversation\n",
        "                self.add_llm_conversation(response, \"assistant\")\n",
        "            else:\n",
        "                self.add_llm_conversation(response, \"model\")\n",
        "\n",
        "            return response\n",
        "\n",
        "\n",
        "    def parse_parameters(self, parameters_string):\n",
        "        new_parameters_list = []\n",
        "\n",
        "        # Update the Q-table based on the new Q-table\n",
        "        for row in parameters_string.split(\"\\n\"):\n",
        "            if row.strip().strip(\",\"):\n",
        "                try:\n",
        "                    parameters_row = [\n",
        "                        float(x.strip().strip(\",\")) for x in row.split(\",\")\n",
        "                    ]\n",
        "                    new_parameters_list.append(parameters_row)\n",
        "                except Exception as e:\n",
        "                    print(e)\n",
        "\n",
        "        return new_parameters_list\n",
        "\n",
        "\n",
        "    def llm_update_parameters_num_optim(\n",
        "        self,\n",
        "        episode_reward_buffer,\n",
        "        parse_parameters,\n",
        "        step_number,\n",
        "        rank=None,\n",
        "        optimum=None,\n",
        "        search_step_size=0.1,\n",
        "        actions=None,\n",
        "        env_description=None,\n",
        "    ):\n",
        "        self.reset_llm_conversation()\n",
        "\n",
        "        system_prompt = self.llm_si_template.render(\n",
        "            {\n",
        "                \"episode_reward_buffer_string\": str(episode_reward_buffer),\n",
        "                \"step_number\": str(step_number),\n",
        "                \"rank\": rank,\n",
        "                \"optimum\": str(optimum),\n",
        "                \"step_size\": str(search_step_size),\n",
        "                \"actions\": actions,\n",
        "                \"env_description\": env_description,\n",
        "            }\n",
        "        )\n",
        "\n",
        "        self.add_llm_conversation(system_prompt, \"user\")\n",
        "\n",
        "        api_start_time = time.time()\n",
        "        new_parameters_with_reasoning = self.query_llm()\n",
        "        api_time = time.time() - api_start_time\n",
        "        new_parameters_list = parse_parameters(new_parameters_with_reasoning)\n",
        "\n",
        "        return (\n",
        "            new_parameters_list,\n",
        "            \"system:\\n\"\n",
        "            + system_prompt\n",
        "            + \"\\n\\n\\nLLM:\\n\"\n",
        "            + new_parameters_with_reasoning,\n",
        "            api_time,\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1JlxxvbfLtf"
      },
      "source": [
        "### **Agent**\n",
        "\n",
        "The below cell defines the core agent wrapper. It is responsibe for managing the policy, interacting with the world and coordinating with the LLMBrain to learn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ld4xuBudfLtf"
      },
      "outputs": [],
      "source": [
        "#@title **Core Agent Wrapper**\n",
        "class LLMNumOptimAgent:\n",
        "    def __init__(\n",
        "        self,\n",
        "        logdir,\n",
        "        dim_action,\n",
        "        dim_state,\n",
        "        max_traj_count,\n",
        "        max_traj_length,\n",
        "        llm_si_template,\n",
        "        llm_output_conversion_template,\n",
        "        llm_model_name,\n",
        "        num_evaluation_episodes,\n",
        "        bias,\n",
        "        optimum,\n",
        "        search_step_size,\n",
        "        env_description,\n",
        "    ):\n",
        "        self.start_time = time.process_time()\n",
        "        self.api_call_time = 0\n",
        "        self.total_steps = 0\n",
        "        self.total_episodes = 0\n",
        "        self.dim_action = dim_action\n",
        "        self.dim_state = dim_state\n",
        "        self.bias = bias\n",
        "        self.optimum = optimum\n",
        "        self.search_step_size = search_step_size\n",
        "        self.env_description = env_description\n",
        "\n",
        "        if not self.bias:\n",
        "            param_count = dim_action * dim_state\n",
        "        else:\n",
        "            param_count = dim_action * dim_state + dim_action\n",
        "        self.rank = param_count\n",
        "\n",
        "        # Initialize the policy and replay buffer\n",
        "        if not self.bias:\n",
        "            self.policy = LinearPolicyNoBias(\n",
        "                dim_actions=dim_action, dim_states=dim_state\n",
        "            )\n",
        "        else:\n",
        "            self.policy = LinearPolicy(dim_actions=dim_action, dim_states=dim_state)\n",
        "        self.replay_buffer = EpisodeRewardBufferNoBias(max_size=max_traj_count)\n",
        "        self.llm_brain = LLMBrain(\n",
        "            llm_si_template, llm_output_conversion_template, llm_model_name\n",
        "        )\n",
        "        self.logdir = logdir\n",
        "        self.num_evaluation_episodes = num_evaluation_episodes\n",
        "        self.training_episodes = 0\n",
        "\n",
        "        if self.bias:\n",
        "            self.dim_state += 1\n",
        "\n",
        "    def rollout_episode(self, world, logging_file, record=True):\n",
        "        \"\"\"Simulates an episode in the environment using the current policy.\"\"\"\n",
        "        state = world.reset()\n",
        "        state = np.expand_dims(state, axis=0)\n",
        "        logging_file.write(\n",
        "            f\"{', '.join([str(x) for x in self.policy.get_parameters().reshape(-1)])}\\n\"\n",
        "        )\n",
        "        logging_file.write(f\"parameter ends\\n\\n\")\n",
        "        logging_file.write(f\"state | action | reward\\n\")\n",
        "        done = False\n",
        "        step_idx = 0\n",
        "        while not done:\n",
        "            action = self.policy.get_action(state.T)\n",
        "            action = np.reshape(action, (1, self.dim_action))\n",
        "            if world.discretize:\n",
        "                action = np.argmax(action)\n",
        "                action = np.array([action])\n",
        "            next_state, reward, done = world.step(action)\n",
        "            logging_file.write(f\"{state.T[0]} | {action[0]} | {reward}\\n\")\n",
        "            state = next_state\n",
        "            step_idx += 1\n",
        "            self.total_steps += 1\n",
        "        logging_file.write(f\"Total reward: {world.get_accu_reward()}\\n\")\n",
        "        self.total_episodes += 1\n",
        "        if record:\n",
        "            self.replay_buffer.add(\n",
        "                self.policy.get_parameters(), world.get_accu_reward()\n",
        "            )\n",
        "        return world.get_accu_reward()\n",
        "\n",
        "    def random_warmup(self, world, logdir, num_episodes):\n",
        "        for episode in range(num_episodes):\n",
        "            self.policy.initialize_policy()\n",
        "            # Run the episode and collect the trajectory\n",
        "            print(f\"Rolling out warmup episode {episode}...\")\n",
        "            logging_filename = f\"{logdir}/warmup_rollout_{episode}.txt\"\n",
        "            logging_file = open(logging_filename, \"w\")\n",
        "            result = self.rollout_episode(world, logging_file)\n",
        "            print(f\"Result: {result}\")\n",
        "\n",
        "    def train_policy(self, world, logdir):\n",
        "        \"\"\"Core method to train single iteration of the policy using LLM optimization.\"\"\"\n",
        "\n",
        "        def parse_parameters(input_text):\n",
        "            # This regex looks for integers or floating-point numbers (including optional sign)\n",
        "            s = input_text.split(\"\\n\")[0]\n",
        "            print(\"response:\", s)\n",
        "            pattern = re.compile(r\"params\\[(\\d+)\\]:\\s*([+-]?\\d+(?:\\.\\d+)?)\")\n",
        "            matches = pattern.findall(s)\n",
        "\n",
        "            # Convert matched strings to float (or int if you prefer to differentiate)\n",
        "            results = []\n",
        "            for match in matches:\n",
        "                results.append(float(match[1]))\n",
        "            print(results)\n",
        "            assert len(results) == self.rank\n",
        "            return np.array(results).reshape(-1)\n",
        "\n",
        "        def str_nd_examples(replay_buffer: EpisodeRewardBufferNoBias, n):\n",
        "\n",
        "            all_parameters = []\n",
        "            for weights, reward in replay_buffer.buffer:\n",
        "                parameters = weights\n",
        "                all_parameters.append((parameters.reshape(-1), reward))\n",
        "\n",
        "            text = \"\"\n",
        "            for parameters, reward in all_parameters:\n",
        "                l = \"\"\n",
        "                for i in range(n):\n",
        "                    l += f\"params[{i}]: {parameters[i]:.5g}; \"\n",
        "                fxy = reward\n",
        "                l += f\"f(params): {fxy:.2f}\\n\"\n",
        "                text += l\n",
        "            return text\n",
        "\n",
        "        # Update the policy using llm_brain, q_table and replay_buffer\n",
        "        print(\"Updating the policy...\")\n",
        "        new_parameter_list, reasoning, api_time = self.llm_brain.llm_update_parameters_num_optim(\n",
        "            str_nd_examples(self.replay_buffer, self.rank),\n",
        "            parse_parameters,\n",
        "            self.training_episodes,\n",
        "            self.rank,\n",
        "            self.optimum,\n",
        "            self.search_step_size,\n",
        "            env_description = self.env_description,\n",
        "        )\n",
        "        self.api_call_time += api_time\n",
        "\n",
        "        print(self.policy.get_parameters().shape)\n",
        "        print(new_parameter_list.shape)\n",
        "        self.policy.update_policy(new_parameter_list)\n",
        "        print(self.policy.get_parameters().shape)\n",
        "        logging_q_filename = f\"{logdir}/parameters.txt\"\n",
        "        logging_q_file = open(logging_q_filename, \"w\")\n",
        "        logging_q_file.write(str(self.policy))\n",
        "        logging_q_file.close()\n",
        "        q_reasoning_filename = f\"{logdir}/parameters_reasoning.txt\"\n",
        "        q_reasoning_file = open(q_reasoning_filename, \"w\")\n",
        "        q_reasoning_file.write(reasoning)\n",
        "        q_reasoning_file.close()\n",
        "        print(\"Policy updated!\")\n",
        "\n",
        "        # Run the episode and collect the trajectory\n",
        "        print(f\"Rolling out episode {self.training_episodes}...\")\n",
        "        logging_filename = f\"{logdir}/training_rollout.txt\"\n",
        "        logging_file = open(logging_filename, \"w\")\n",
        "        results = []\n",
        "        for idx in range(self.num_evaluation_episodes):\n",
        "            if idx == 0:\n",
        "                result = self.rollout_episode(world, logging_file, record=False)\n",
        "            else:\n",
        "                result = self.rollout_episode(world, logging_file, record=False)\n",
        "            results.append(result)\n",
        "        print(f\"Results: {results}\")\n",
        "        result = np.mean(results)\n",
        "        self.replay_buffer.add(new_parameter_list, result)\n",
        "\n",
        "        self.training_episodes += 1\n",
        "\n",
        "        _cpu_time = time.process_time() - self.start_time\n",
        "        _api_time = self.api_call_time\n",
        "        _total_episodes = self.total_episodes\n",
        "        _total_steps = self.total_steps\n",
        "        _total_reward = result\n",
        "        return _cpu_time, _api_time, _total_episodes, _total_steps, _total_reward\n",
        "\n",
        "\n",
        "    def evaluate_policy(self, world, logdir):\n",
        "        results = []\n",
        "        for idx in range(self.num_evaluation_episodes):\n",
        "            logging_filename = f\"{logdir}/evaluation_rollout_{idx}.txt\"\n",
        "            logging_file = open(logging_filename, \"w\")\n",
        "            result = self.rollout_episode(world, logging_file, record=False)\n",
        "            results.append(result)\n",
        "        return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "en2skYbmfLtg"
      },
      "source": [
        "The below cell orchestrates the entire training process from initialization to completion. The `run_training_loop` function starts with initialization the world, and the agent instances. Then, it creates a set of warmup episodes to pass in as initial replay buffer to the optimizer. The code then runs the training loop for specified number of episodes and optimizes the policy parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "iJdmb5WLfLtg"
      },
      "outputs": [],
      "source": [
        "#@title **Training Loop**\n",
        "def ordinal(n):\n",
        "    if 10 <= n % 100 <= 20:\n",
        "        suffix = \"th\"\n",
        "    else:\n",
        "        suffix = {1: \"st\", 2: \"nd\", 3: \"rd\"}.get(n % 10, \"th\")\n",
        "    return f\"{n}{suffix}\"\n",
        "\n",
        "def run_training_loop(\n",
        "    num_episodes,\n",
        "    gym_env_name,\n",
        "    render_mode,\n",
        "    logdir,\n",
        "    dim_actions,\n",
        "    dim_states,\n",
        "    max_traj_count,\n",
        "    max_traj_length,\n",
        "    llm_model_name,\n",
        "    num_evaluation_episodes,\n",
        "    warmup_episodes,\n",
        "    warmup_dir,\n",
        "    bias=None,\n",
        "    rank=None,\n",
        "    optimum=100,\n",
        "    search_step_size=SEARCH_STD,\n",
        "    env_description=None,\n",
        "):\n",
        "    world = ContinualSpaceGeneralWorld(\n",
        "        gym_env_name,\n",
        "        render_mode,\n",
        "        max_traj_length,\n",
        "    )\n",
        "\n",
        "    agent = LLMNumOptimAgent(\n",
        "        logdir,\n",
        "        dim_actions,\n",
        "        dim_states,\n",
        "        max_traj_count,\n",
        "        max_traj_length,\n",
        "        llm_si_template,\n",
        "        llm_output_conversion_template,\n",
        "        llm_model_name,\n",
        "        num_evaluation_episodes,\n",
        "        bias,\n",
        "        optimum,\n",
        "        search_step_size,\n",
        "        env_description,\n",
        "    )\n",
        "    print('init done')\n",
        "\n",
        "    if not warmup_dir:\n",
        "        warmup_dir = f\"{logdir}/warmup\"\n",
        "        os.makedirs(warmup_dir, exist_ok=True)\n",
        "        agent.random_warmup(world, warmup_dir, warmup_episodes)\n",
        "    else:\n",
        "        agent.replay_buffer.load(warmup_dir)\n",
        "\n",
        "    overall_log_file = open(f\"{logdir}/overall_log.txt\", \"w\")\n",
        "    overall_log_file.write(\"Iteration, CPU Time, API Time, Total Episodes, Total Steps, Total Reward\\n\")\n",
        "    overall_log_file.flush()\n",
        "    for episode in range(num_episodes):\n",
        "        print(\"-----------------------------------------------------------------------------------------------------------\")\n",
        "        print(f\"Episode: {episode}\")\n",
        "        # create log dir\n",
        "        curr_episode_dir = f\"{logdir}/episode_{episode}\"\n",
        "        print(f\"Creating log directory: {curr_episode_dir}\")\n",
        "        os.makedirs(curr_episode_dir, exist_ok=True)\n",
        "\n",
        "        for trial_idx in range(5):\n",
        "            try:\n",
        "                cpu_time, api_time, total_episodes, total_steps, total_reward = agent.train_policy(world, curr_episode_dir)\n",
        "                overall_log_file.write(f\"{episode + 1}, {cpu_time}, {api_time}, {total_episodes}, {total_steps}, {total_reward}\\n\")\n",
        "                overall_log_file.flush()\n",
        "                print(f\"{ordinal(trial_idx + 1)} trial attempt succeeded in training\")\n",
        "                break\n",
        "            except Exception as e:\n",
        "                print(f\"{ordinal(trial_idx + 1)} trial attempt failed with error in training: {e}\")\n",
        "                traceback.print_exc()\n",
        "\n",
        "                if trial_idx == 4:\n",
        "                    print(f\"All {trial_idx + 1} trials failed. Train terminated\")\n",
        "                    exit(1)\n",
        "                continue\n",
        "    overall_log_file.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "pX6X6LbwfLti"
      },
      "outputs": [],
      "source": [
        "#@title **Run the Training Loop**\n",
        "LLM_MODEL_NAME = \"gemini-2.0-flash\"\n",
        "\n",
        "swimmer_description = \"\"\"\n",
        "The swimmers consist of three or more segments (’links’) and one less articulation joints (’rotors’) - one rotor joint connects exactly two links to form a linear chain. The swimmer is suspended in a two-dimensional pool and always starts in the same position (subject to some deviation drawn from a uniform distribution), and the goal is to move as fast as possible towards the right by applying torque to the rotors and using fluid friction.\n",
        "The state is a vector of 8 elements, representing the following:\n",
        "- state[0] angle of the front tip (-inf to inf rad)\n",
        "- state[1] angle of the first rotor (-inf to inf rad)\n",
        "- state[2] angle of the second rotor (-inf to inf rad)\n",
        "- state[3] velocity of the tip along the x-axis (-inf to inf m/s)\n",
        "- state[4] velocity of the tip along the y-axis (-inf to inf m/s)\n",
        "- state[5] angular velocity of front tip (-inf to inf rad/s)\n",
        "- state[6] angular velocity of first rotor (-inf to inf rad/s)\n",
        "- state[7] angular velocity of second rotor (-inf to inf rad/s)\n",
        "The action space is a vector of 2 float numbers, representing the torques applied between the links (-1 to 1 N).\n",
        "The policy is a linear policy with 5 parameters and works as follows:\n",
        "action = argmax(state @ W + B), where\n",
        "state = [state[0], state[1], state[2], state[3], state[4], state[5], state[6], state[7]]\n",
        "W = [[params[0], params[1]],\n",
        "    [params[2], params[3]],\n",
        "    [params[4], params[5]],\n",
        "    [params[6], params[7]],\n",
        "    [params[8], params[9]],\n",
        "    [params[10], params[11]],\n",
        "    [params[12], params[13]],\n",
        "    [params[14], params[15]]]\n",
        "b = [params[16], params[17]]\n",
        "The goal is to try to move forward. However, in the meantime, the control cost should also be minimized. The reward function is as follows:\n",
        "reward = x-velocity - 1e-4 * action^2.\n",
        "\"\"\"\n",
        "\n",
        "run_training_loop(\n",
        "    num_episodes=NUM_EPISODES,\n",
        "    gym_env_name=\"Swimmer-v5\", # https://gymnasium.farama.org/environments/mujoco/swimmer/\n",
        "    render_mode=RENDER_MODE,\n",
        "    logdir=\"logs/mujoco_swimmer_tutorial\",\n",
        "    dim_actions=2,\n",
        "    dim_states=8,\n",
        "    max_traj_count=MAX_TRAJ_COUNT,\n",
        "    max_traj_length=MAX_TRAJ_LENGTH,\n",
        "    llm_model_name=LLM_MODEL_NAME,\n",
        "    num_evaluation_episodes=NUM_EVALUATION_EPISODES,\n",
        "    warmup_episodes=WARMUP_EPISODES,\n",
        "    warmup_dir=None,\n",
        "    bias=None,\n",
        "    rank=None,\n",
        "    optimum=250,\n",
        "    search_step_size=SEARCH_STD,\n",
        "    env_description=swimmer_description,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "JvgFYhbn6Y0f",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title **Policy Visualization (Swimmer-v5)**\n",
        "\n",
        "EPISODE_DIR = \"episode_197\"   # keep your existing episode folder name\n",
        "LOGDIR = \"logs/mujoco_swimmer_tutorial\"\n",
        "\n",
        "import os\n",
        "# Use EGL on most GPU-backed Linux servers; try \"osmesa\" if EGL isn't available\n",
        "os.environ.setdefault(\"MUJOCO_GL\", \"egl\")   # alternatives: \"osmesa\", \"glfw\"\n",
        "\n",
        "def run_policy(\n",
        "    render_mode=\"rgb_array\",\n",
        "    logdir=LOGDIR,\n",
        "    episode_dir=EPISODE_DIR,\n",
        "    save_gif=False,\n",
        "    fast_mode=True,\n",
        "):\n",
        "\n",
        "    # Visualization speed settings\n",
        "    if fast_mode:\n",
        "        sleep_time = 0.0\n",
        "        display_interval = 1\n",
        "        downscale_factor = 1\n",
        "    else:\n",
        "        sleep_time = 0.05\n",
        "        display_interval = 1\n",
        "        downscale_factor = 1\n",
        "\n",
        "    ENV_NAME = \"Swimmer-v5\"\n",
        "    DIM_STATE = 8\n",
        "    DIM_ACTION = 2\n",
        "\n",
        "    world = ContinualSpaceGeneralWorld(\n",
        "        ENV_NAME,\n",
        "        render_mode=render_mode,\n",
        "        max_traj_length=MAX_TRAJ_LENGTH,\n",
        "    )\n",
        "\n",
        "    agent = LLMNumOptimAgent(\n",
        "        logdir,\n",
        "        dim_action=DIM_ACTION,\n",
        "        dim_state=DIM_STATE,\n",
        "        max_traj_count=MAX_TRAJ_COUNT,\n",
        "        max_traj_length=MAX_TRAJ_LENGTH,\n",
        "        llm_si_template=llm_si_template,\n",
        "        llm_output_conversion_template=llm_output_conversion_template,\n",
        "        llm_model_name=LLM_MODEL_NAME,\n",
        "        num_evaluation_episodes=NUM_EVALUATION_EPISODES,\n",
        "        bias=False,\n",
        "        optimum=100,\n",
        "        search_step_size=SEARCH_STD,\n",
        "        env_description=swimmer_description,\n",
        "    )\n",
        "\n",
        "    # Load trained parameters safely\n",
        "    param_path = os.path.join(logdir, episode_dir, \"parameters.txt\")\n",
        "    if not os.path.exists(param_path):\n",
        "        raise FileNotFoundError(f\"Could not find parameter file at {param_path}\")\n",
        "\n",
        "    weights = []\n",
        "    with open(param_path, \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    for line in lines[1:]:  # skip header\n",
        "        line = line.strip()\n",
        "        if not line or not any(c.isdigit() or c in \"-.\" for c in line):\n",
        "            continue\n",
        "        try:\n",
        "            weights.append(float(line))\n",
        "        except ValueError:\n",
        "            # Handle \"Bias: 0.2\" or similar\n",
        "            parts = line.replace(\",\", \" \").split()\n",
        "            for p in parts:\n",
        "                try:\n",
        "                    weights.append(float(p))\n",
        "                except ValueError:\n",
        "                    continue\n",
        "\n",
        "    agent.policy.update_policy([weights])\n",
        "\n",
        "    # Rollout\n",
        "    raw_state = world.reset()\n",
        "    state = np.expand_dims(raw_state, axis=0)\n",
        "    done = False\n",
        "    step_idx = 0\n",
        "    frames = [] if save_gif else None\n",
        "\n",
        "    def get_frame():\n",
        "        \"\"\"Safely obtain an RGB frame across Gymnasium/Gym versions.\"\"\"\n",
        "        # Gymnasium (>=0.26): render_mode set at make-time; call with no args\n",
        "        try:\n",
        "            img = world.env.render()\n",
        "            if img is not None:\n",
        "                return img\n",
        "        except TypeError:\n",
        "            # Old Gym fallback path\n",
        "            pass\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        # Legacy Gym fallbacks\n",
        "        for call in (lambda: world.env.render(\"rgb_array\"),\n",
        "                    lambda: world.env.render(mode=\"rgb_array\")):\n",
        "            try:\n",
        "                img = call()\n",
        "                if img is not None:\n",
        "                    return img\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "        return None\n",
        "\n",
        "    # First frame setup\n",
        "    img = get_frame()\n",
        "    if img is None:\n",
        "        raise RuntimeError(\"Unable to obtain an RGB frame from the environment.\")\n",
        "    if not isinstance(img, np.ndarray):\n",
        "        img = np.array(img)\n",
        "    if downscale_factor > 1:\n",
        "        img = img[::downscale_factor, ::downscale_factor]\n",
        "\n",
        "    # Matplotlib setup\n",
        "    fig, ax = plt.subplots(figsize=(6, 6))\n",
        "    im = ax.imshow(img)\n",
        "    ax.axis(\"off\")\n",
        "    text_box = ax.text(5, 15, '', color='white', fontsize=12, backgroundcolor='black')\n",
        "    display_id = f\"policy_vis_{int(time.time()*1000)}\"\n",
        "    display(fig, display_id=display_id)\n",
        "\n",
        "    # Rollout loop\n",
        "    while not done:\n",
        "        img = get_frame()\n",
        "        if img is None:\n",
        "            print(f\"Warning: got None frame at step {step_idx}\")\n",
        "            break\n",
        "\n",
        "        if not isinstance(img, np.ndarray):\n",
        "            img = np.array(img)\n",
        "        if downscale_factor > 1:\n",
        "            img = img[::downscale_factor, ::downscale_factor]\n",
        "\n",
        "        im.set_data(img)\n",
        "        text_box.set_text(f\"Step {step_idx}\")\n",
        "\n",
        "        if step_idx % display_interval == 0:\n",
        "            update_display(fig, display_id=display_id)\n",
        "\n",
        "        if save_gif:\n",
        "            frames.append(img.copy())\n",
        "\n",
        "        # Get action\n",
        "        try:\n",
        "            action = agent.policy.get_action(state.T)\n",
        "        except Exception as e:\n",
        "            print(f\"Error getting action at step {step_idx}: {e}\")\n",
        "            break\n",
        "\n",
        "        action_np = np.asarray(action)\n",
        "        if action_np.ndim == 0:\n",
        "            action_np = np.reshape(action_np, (1, 1))\n",
        "        elif action_np.ndim == 1:\n",
        "            action_np = action_np.reshape(1, -1)\n",
        "        elif action_np.ndim >= 2:\n",
        "            action_np = action_np[0:1, :]\n",
        "\n",
        "        # Environment step\n",
        "        try:\n",
        "            next_state, reward, done = world.step(action_np)\n",
        "        except Exception as e:\n",
        "            try:\n",
        "                next_state, reward, done = world.step(np.squeeze(action_np))\n",
        "            except Exception as e2:\n",
        "                print(\"Error stepping env:\", e, e2)\n",
        "                break\n",
        "\n",
        "        state = np.expand_dims(next_state, 0) if next_state.ndim == 1 else next_state\n",
        "        step_idx += 1\n",
        "        time.sleep(sleep_time)\n",
        "\n",
        "    plt.close(fig)\n",
        "\n",
        "    # Save visualization\n",
        "    if save_gif and frames:\n",
        "        gif_dir = \"output_gif\"\n",
        "        os.makedirs(gif_dir, exist_ok=True)\n",
        "        gif_path = os.path.join(gif_dir, f\"{episode_dir}.gif\")\n",
        "        imageio.mimsave(gif_path, frames, fps=25)\n",
        "        print(f\"Saved visualization → {gif_path}\")\n",
        "\n",
        "    print(\"Finished visualization.\")\n",
        "\n",
        "# Run visualization\n",
        "run_policy(save_gif=True, fast_mode=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "MkmFo94UfLth"
      },
      "outputs": [],
      "source": [
        "#@title **Episodes Reward Summary**\n",
        "all_succ = []\n",
        "root_folder = \"logs/mujoco_swimmer_tutorial\"\n",
        "\n",
        "all_folders = [os.path.join(root_folder, x) for x in os.listdir(root_folder) if 'episode' in x]\n",
        "all_folders.sort(key=lambda x: int(x.split('_')[-1]))\n",
        "for folder in all_folders:\n",
        "    # read all text files in the folder. Read the last line of each file and extract the total reward. The last line looks like this: \"Total reward: -157.0\"\n",
        "    rewards_succ = []\n",
        "    rewards_fail = []\n",
        "    for filename in os.listdir(folder):\n",
        "        if 'training' in filename:\n",
        "            with open(os.path.join(folder, filename), 'r') as f:\n",
        "                lines = f.readlines()\n",
        "                rewards = []\n",
        "                for line in lines:\n",
        "                    if 'Total reward' in line:\n",
        "                        total_reward = float(line.split()[-1])\n",
        "                        rewards.append(total_reward)\n",
        "                if rewards:  # prevent empty list\n",
        "                    rewards_succ.append(np.mean(rewards))\n",
        "\n",
        "    # Evaluation files (separate loop, not \"else\")\n",
        "    curr_episode_rewards = []\n",
        "    for filename in os.listdir(folder):\n",
        "        if 'evaluation' in filename:\n",
        "            with open(os.path.join(folder, filename), 'r') as f:\n",
        "                lines = f.readlines()\n",
        "                curr_rewards = []\n",
        "                for line in lines[1:]:\n",
        "                    try:\n",
        "                        curr_rewards.append(float(line.split('|')[-1]))\n",
        "                    except ValueError:\n",
        "                        continue  # skip malformed lines\n",
        "                if curr_rewards:\n",
        "                    curr_episode_rewards.append(np.sum(curr_rewards))\n",
        "    if curr_episode_rewards:\n",
        "        rewards_succ.append(np.mean(curr_episode_rewards))\n",
        "\n",
        "    print(rewards_succ)\n",
        "    # print(rewards_fail)\n",
        "\n",
        "\n",
        "    all_rewards = rewards_succ + rewards_fail\n",
        "\n",
        "    print(\"Average reward for all episodes:\", np.mean(all_rewards))\n",
        "    print(\"Standard deviation of reward for all episodes:\", np.std(all_rewards))\n",
        "    print(\"------------------\")\n",
        "\n",
        "    if 'descent' in root_folder:\n",
        "        all_succ.append(1500 - np.mean(all_rewards))\n",
        "    else:\n",
        "        all_succ.append(np.mean(all_rewards))\n",
        "# print(all_succ)\n",
        "print(max(all_succ))\n",
        "for i in range(len(all_succ)):\n",
        "    if all_succ[i] >= max(all_succ) * 0.95:\n",
        "        print(i + 1)\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Reward Curve**\n",
        "episodes = list(range(1, len(all_succ) + 1))\n",
        "\n",
        "# Creating the plot\n",
        "plt.figure(figsize=(15, 6))\n",
        "\n",
        "# Plot the main line with better styling\n",
        "plt.plot(episodes, all_succ, linewidth=2, color='red', label=\"Swimmer-v5 LLM Optimization Tutorial - Learning Curve\")\n",
        "\n",
        "plt.xlabel(\"Episodes\", fontsize=12)\n",
        "plt.ylabel(\"Reward\", fontsize=12)\n",
        "plt.legend(fontsize=10)\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.tight_layout(pad=3.0)\n",
        "\n",
        "os.makedirs('results_curves', exist_ok=True)\n",
        "plt.savefig(f'results_curves/{root_folder.split(\"/\")[1]}.png', dpi=300)\n",
        "%matplotlib inline\n",
        "plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "fSuL4dfaxao8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `Reward Curve` plotted over the training episodes illustrates the progressive improvement of the learned policy. Typically, in early stages of training, rewards fluctuate due to random initialization and broad exploration. As the LLM repeatedly refines the parameters, the curve begins to rise steadily, indicating that the model is discovering more effective control strategies. Eventually, the reward curve stabilizes, reflecting convergence toward a locally optimal linear policy for the environment."
      ],
      "metadata": {
        "id": "YmzKrH8wOgfo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Summary**\n",
        "This demo illustrates how ProPS+, an extension of the Prompted Policy Search (ProPS) framework, integrates numerical optimization with semantically informed, task-specific knowledge to accelerate LLM-driven reinforcement learning. While ProPS relies solely on replay-buffer data linking policy parameters to rewards, ProPS+ augments this with rich environment descriptions, structural knowledge, and human-provided behavioral hints, enabling the LLM to perform both numerical reasoning and linguistic reasoning during policy optimization. In the context of the Swimmer-v5 locomotion task, the LLM receives not only the replayed history of policy pairs but also an explicit description of the agent’s morphology, state variables, actuator roles, and reward structure. This added semantic grounding guides the model toward more purposeful exploration and more informed parameter refinement, helping it infer how different policy weights influence forward motion, torque efficiency, and swimming dynamics.\n",
        "\n",
        "Beyond this Swimmer demonstration, ProPS+ generalizes to a wide range of continuous-control and decision-making tasks where environment semantics play a meaningful role. By embedding structural descriptions, domain knowledge, or task constraints directly inside the prompt, ProPS+ enables LLMs to reason about optimal behavior at a conceptual level while still performing numerical optimization over policy parameters. This hybrid reasoning capability illustrates the flexibility of the ProPS+ framework, showing how linguistic priors can complement in-context numerical learning to produce more sample-efficient and interpretable policy-improvement loops.\n",
        "\n",
        "## **Conclusion**\n",
        "This demonstration shows that LLMs, when equipped with semantically enriched prompts, can serve as powerful policy optimizers—capturing the central insight of the ProPS+ framework. Without gradients, environment internals, or model-based planners, the LLM learns to propose improved policy parameters by synthesizing replay-buffer outcomes with detailed knowledge about the Swimmer’s physics, morphology, and control objectives. While the model does not internally simulate Mujoco dynamics, its ability to leverage both contextual numerical patterns and linguistic domain knowledge enables it to approximate effective policy updates that steadily increase forward locomotion reward.\n",
        "\n",
        "This highlights how ProPS+ advances the original ProPS approach by making LLM-driven reinforcement learning more efficient, interpretable, and guided. The LLM not only reacts to past performance but also reasons about the meaning of state variables, the role of torques, and the physical constraints of swimming. This synergy between language-based reasoning and numerical optimization demonstrates a practical and compelling path for using LLMs as unified RL optimizers—capable of learning complex continuous control behavior through structured prompts alone.\n",
        "\n",
        "<br>\n",
        "\n",
        "For more details, please refer to the [ProPS Project Page](https://props-llm.github.io/) and the associated research paper.\n",
        "\n",
        "## **References**\n",
        "Zhou, Y., Grover, S., El Mistiri, M., Kalirathnam, K., Kerhalkar, P., Mishra, S., Kumar, N., Gaurav, S., Aran, O., & Ben Amor, H. (2025). Prompted Policy Search: Reinforcement Learning through Linguistic and Numerical Reasoning in LLMs, Advances in Neural Information Processing Systems (NeurIPS 2025)."
      ],
      "metadata": {
        "id": "sOwf2lMUxh0e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2SJWLVwuzoW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}