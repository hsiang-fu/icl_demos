{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction to In-Context Learning Demo: Image Classification**\n",
        "\n",
        "## **Overview**\n",
        "This notebook presents a step-by-step walkthrough of an interactive demo that explores how Large Language Models (LLMs) can perform image-based image classification purely through in-context learning. Inspired by the idea that LLMs can function as general pattern recognizers, this experiment uses small image snippets of weld defects, labeled with their respective defect—to examine how effectively Gemini can internalize visual defect patterns and reproduce consistent judgments on new samples.\n",
        "\n",
        "The goal is to illustrate how an LLM—without explicit training, fine-tuning, or feature engineering—can infer quality cues (defects) from a few examples and generalize to other images.\n",
        "\n",
        "## **Background**\n",
        "This demo focuses on real weld defect images categorized into 6 classes:\n",
        "- Weld Cracks\n",
        "- Burn Through\n",
        "- Lack of Fusion\n",
        "- Slag Inclusion\n",
        "- Weld Splatter\n",
        "- Surface Porosity\n",
        "\n",
        "Each example image serves as an example pairing:\n",
        "- A compact visual representation\n",
        "- A category label from one of the 6 classes\n",
        "\n",
        "What makes this setup particularly compelling:\n",
        "- The data was checked before utilization to make sure it couldn't classify the industrial defect correctly before ICL.\n",
        "- The model receives visual examples only through prompt context\n",
        "- No training or gradient updates occur—classification arises from pattern matching\n",
        "- Prompts can include descriptions, annotations, or multi-step chains of examples\n",
        "- The test images require generalization, not memorization\n",
        "- Predictions are generated sample-by-sample, mimicking standard evaluation flows\n",
        "\n",
        "This setting provides a clear benchmark for understanding how well LLMs can perform visual classification tasks when guided only through carefully constructed prompts.\n",
        "\n",
        "## **Let's Take a Look at an Example**\n",
        "\n",
        "The illustration below shows an example of weld defects. Given a set of labeled samples in context, the LLM must detect the defect for new, unseen defect images during evaluation.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/intro-to-icl/intro-to-icl.github.io/refs/heads/master/static/images/weld_defects.png\" height=\"500\">\n",
        "\n",
        "## **LLM as the Classifier**\n",
        "\n",
        "In this demo, we use Gemini 2.0/2.5 Flash in non-reasoning mode, prompting the model to rely on direct pattern recognition rather than symbolic explanation or analytic reasoning.\n",
        "\n",
        "The workflow proceeds as follows:\n",
        "- Provide 9 total images of weld defects (1-2 for each defect) as in-context examples\n",
        "- Send a new unlabeled image through the prompt to classify\n",
        "- Ask the model to output the category of the new unlabeled image\n",
        "\n",
        "The LLM functions as a lightweight, prompt-driven classifier—absorbing visual differences, structural patterns, and defect signatures from the in-context examples.\n",
        "\n",
        "## **Evaluation**\n",
        "Finally, we compare the model’s predicted labels against ground-truth labels and compute accuracy, which provides insight into how effectively a LLM can approximate visual quality-control decisions through in-context learning alone—without any dedicated training pipeline.\n",
        "\n",
        "## **Code Overview**\n",
        "The implementation is structured modularly, with each component handling a distinct stage of the ICL classification pipeline. This separation makes the system easy to modify, extend, and reuse:\n",
        "- Data loading & preprocessing: Read images, convert to model-compatible format\n",
        "- Visualization: Display sets of good/bad examples\n",
        "- Prompt construction: Insert labeled samples into few-shot prompts\n",
        "- LLM inference: Retrieve predictions one image at a time\n",
        "\n",
        "## **Before Running the Demo, Follow the Instructions Below**\n",
        "To run the full experiment:\n",
        "1. Ensure all dependencies are imported and installed.\n",
        "2.\tVisit Google AI Studio (https://aistudio.google.com/) to obtain your Gemini API key.\n",
        "3.\tOnce API key is generated, copy and paste it into the demo when prompted.\n",
        "\n",
        "#### **If having trouble creating an API Key, follow the link below for instructions:**\n",
        "* #### **[Instructions on How to Obtain a Gemini API Key](https://docs.google.com/document/d/17pgikIpvZCBgcRh4NNrcee-zp3Id0pU0vq2SG52KIGE/edit?usp=sharing)**\n",
        "\n"
      ],
      "metadata": {
        "id": "JWBl0iMOvbE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Import Necessary Libraries**\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "import re\n",
        "import itertools\n",
        "import math\n",
        "import ast\n",
        "import json\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "import os\n",
        "import getpass\n",
        "from IPython.display import Image\n",
        "import time"
      ],
      "metadata": {
        "cellView": "form",
        "id": "qDV03CJIve4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lh6d1fWygOfe"
      },
      "outputs": [],
      "source": [
        "#@title **Setting Up Gemini Client**\n",
        "apikey = getpass.getpass(\"Enter your Gemini API Key: \")\n",
        "client = genai.Client(api_key=apikey)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Choose a Model**\n",
        "model_dropdown = widgets.Dropdown(\n",
        "    options=[\n",
        "        (\"Gemini 2.5 Flash\", \"gemini-2.5-flash\"),\n",
        "        (\"Gemini 2.0 Flash\", \"gemini-2.0-flash\")\n",
        "    ],\n",
        "    description=\"Model:\",\n",
        "    value=\"gemini-2.5-flash\",\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "confirm_button = widgets.Button(\n",
        "    description=\"Confirm Selection\"\n",
        ")\n",
        "\n",
        "output = widgets.Output()\n",
        "\n",
        "model_name = None\n",
        "\n",
        "def on_confirm_click(b):\n",
        "    global model_name, batch_size\n",
        "\n",
        "    model_name = model_dropdown.value\n",
        "\n",
        "    with output:\n",
        "        output.clear_output()\n",
        "        print(f\"\\nSelected model: {model_name}\")\n",
        "\n",
        "confirm_button.on_click(on_confirm_click)\n",
        "\n",
        "display(model_dropdown, confirm_button, output)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "FyQ7yQQV7qvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Download Data from GitHub**\n",
        "if not os.path.exists(\"intro_to_icl_data\"):\n",
        "    !git clone https://github.com/hsiang-fu/intro_to_icl_data.git"
      ],
      "metadata": {
        "id": "6UMMqb9zX20A",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cell `ICL Image Classification` below  is responsible for running ICL weld defect image classification. It loads labeled training examples, constructs the few-shot prompt, sends the prompt to the LLM for each test image, parses the prediction, evaluates correctness, and finally reports overall accuracy.\n",
        "\n",
        "Each test image is displayed, and the model’s prediction and the ground truth label are printed. Running each cell will perform inference across all test images.\n",
        "\n",
        "After the cell runs, it performs several steps. First, it builds the ICL training examples by loading labled training images. Each example is converted into a Gemini Part so it can be embedded directly into the prompt. These form the annotated few-shot demonstrations the model uses to learn the classification pattern. Next, it constructs the full ICL prompt for each test item by including the instruction, all labeled example images, the unlabeled test image, and a rule specifying that the model should respond only with the labels of the weld defect. Then it loads the unseen test images and their ground-truth labels. For each test image, the cell displays the image, sends the entire ICL prompt to Gemini, reads the model’s label prediction, compares it to the ground truth, and stores the results. After all images are processed, the cell computes summary metrics such as accuracy, total number of correct predictions, and incorrect predictions.\n",
        "\n",
        "Each evaluation cycle outputs the test hazelnut image, the model’s predicted label, the true label, and whether the prediction was correct. At the end, the code prints a performance summary showing the model’s accuracy across all ten test images."
      ],
      "metadata": {
        "id": "NkxcjLfcvzzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **ICL Image Classification**\n",
        "test_labels = [\n",
        "    \"Burn Through\",\n",
        "    \"Cracks\",\n",
        "    \"Lack of Fusion\",\n",
        "    \"Slag Inclusion\",\n",
        "    \"Splatter\",\n",
        "    \"Surface Porosity\",\n",
        "    \"Cracks\",\n",
        "    \"Cracks\",\n",
        "    \"Lack of Fusion\",\n",
        "    \"Splatter\",\n",
        "    \"Surface Porosity\"\n",
        "    ]\n",
        "\n",
        "def load_part(path):\n",
        "    with open(path, \"rb\") as f:\n",
        "        return types.Part.from_bytes(\n",
        "            data=f.read(),\n",
        "            mime_type=\"image/jpeg\"\n",
        "        )\n",
        "\n",
        "train_paths = [f\"intro_to_icl_data/industrial_defects/train{i}.jpg\" for i in range(1, 10)]\n",
        "train_labels = [\"Cracks\", \"Surface Porosity\", \"Slag Inclusion\", \"Splatter\", \"Burn Through\", \"Lack of Fusion\", \"Splatter\", \"Slag Inclusion\", \"Surface Porosity\", \"Burn Through\", \"Burn Through\"]\n",
        "\n",
        "train_parts = [load_part(p) for p in train_paths]\n",
        "\n",
        "def classify_image(label, index):\n",
        "    test_path = f\"intro_to_icl_data/industrial_defects/test{index}.jpg\"\n",
        "    test_part = load_part(test_path)\n",
        "\n",
        "    contents = [\"You are an expert in detecting industrial defects. \"\n",
        "                \"By only using the provided examples, classify the defect.\\n\"]\n",
        "\n",
        "    for i, (tlabel, tpart) in enumerate(zip(train_labels, train_parts)):\n",
        "        contents.append(f\"Example {i+1}: {tlabel}\")\n",
        "        contents.append(tpart)\n",
        "\n",
        "    contents.extend([\n",
        "        \"What is the defect in the test image? Only return the label.\",\n",
        "        test_part,\n",
        "    ])\n",
        "\n",
        "\n",
        "    try:\n",
        "      response = client.models.generate_content(\n",
        "          model=model_name,\n",
        "          contents=contents,\n",
        "      )\n",
        "    except Exception as e:\n",
        "      print(\"Waiting 60 seconds for quota limit reset\")\n",
        "      time.sleep(60)\n",
        "      response = client.models.generate_content(\n",
        "          model=\"gemini-2.5-flash\",\n",
        "          contents=contents,\n",
        "          config=types.GenerateContentConfig(\n",
        "                thinking_config=types.ThinkingConfig(thinking_budget=0)\n",
        "          )\n",
        "      )\n",
        "\n",
        "    return response.text.strip(), test_path\n",
        "\n",
        "correct = 0\n",
        "results = []\n",
        "print(\"Starting Image Classification for Welding Defects\\n\")\n",
        "for i, label in enumerate(test_labels, start=1):\n",
        "    pred, path = classify_image(label, i)\n",
        "\n",
        "    display(Image(filename=path, height = 300))\n",
        "\n",
        "    print(f\"\\nGround Truth: {label}\")\n",
        "    print(f\"Model Output: {pred}\\n\")\n",
        "\n",
        "    is_correct = (pred.lower() == label.lower())\n",
        "\n",
        "    results.append((label, pred, is_correct))\n",
        "    correct += int(is_correct)\n",
        "\n",
        "accuracy = correct / len(test_labels)\n",
        "print(f\"Overall Accuracy: {accuracy:.2f}\\n\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "VE8BH-DCqzUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell `Baseline SVM Model` implements the traditional machine-learning baseline used to compare against the in-context learning approaches. Rather than learning patterns directly from image pixels, this baseline relies on hand-crafted visual descriptors—specifically Histogram of Oriented Gradients (HOG)—which are then classified using a linear Support Vector Machine (SVM). The code loads the training images, converts them to grayscale, resizes each to 256×256, and extracts their HOG feature vectors. The same preprocessing steps are applied to the test set, ensuring a consistent feature representation. Once the features are assembled, a LinearSVC classifier is trained and evaluated on the same set of test images used by the ICL methods. The resulting accuracy provides a structured, feature-engineered benchmark to compare against the LLM’s prompt-based classification performance."
      ],
      "metadata": {
        "id": "OpteMWm3xbAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Baseline SVM Model**\n",
        "from skimage.feature import hog\n",
        "from skimage.io import imread\n",
        "from skimage.color import rgb2gray\n",
        "from skimage.transform import resize\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load train data\n",
        "X_train = []\n",
        "y_train = []\n",
        "\n",
        "for path, label in zip(train_paths, train_labels):\n",
        "    img = imread(path)\n",
        "    img = resize(img, (256, 256), anti_aliasing=True)\n",
        "    img = rgb2gray(img)\n",
        "\n",
        "    feats = hog(img, pixels_per_cell=(16,16), cells_per_block=(2,2))\n",
        "    X_train.append(feats)\n",
        "    y_train.append(label)\n",
        "\n",
        "# Load test data\n",
        "X_test = []\n",
        "y_test = []\n",
        "\n",
        "for i, label in enumerate(test_labels, start=1):\n",
        "    img = imread(f\"intro_to_icl_data/industrial_defects/test{i}.jpg\")\n",
        "    img = resize(img, (256, 256), anti_aliasing=True)\n",
        "    img = rgb2gray(img)\n",
        "\n",
        "    feats = hog(img, pixels_per_cell=(16,16), cells_per_block=(2,2))\n",
        "    X_test.append(feats)\n",
        "    y_test.append(label)\n",
        "\n",
        "# Train classifier\n",
        "clf = LinearSVC()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "preds = clf.predict(X_test)\n",
        "print(\"Baseline HOG+SVM Accuracy:\", accuracy_score(y_test, preds))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "heJjTaJ1RpPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **ICL and Baseline SVM Comparison**\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# Extract predictions from your existing ICL results\n",
        "# results = [(label, pred, is_correct), ...]\n",
        "icl_true = [r[0] for r in results]\n",
        "icl_pred = [r[1] for r in results]\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# Extract baseline predictions (already in 'preds')\n",
        "baseline_pred = list(preds)\n",
        "baseline_true = list(test_labels)   # Same ordering as test inputs\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# Compute accuracies\n",
        "icl_accuracy = sum(np.array(icl_true) == np.array(icl_pred)) / len(icl_true)\n",
        "baseline_accuracy = sum(np.array(baseline_true) == np.array(baseline_pred)) / len(baseline_true)\n",
        "\n",
        "print(\"===========================================\")\n",
        "print(\"         Model Comparison Summary\")\n",
        "print(\"===========================================\\n\")\n",
        "\n",
        "print(f\"ICL Accuracy:       {icl_accuracy:.3f}\")\n",
        "print(f\"Baseline Accuracy:  {baseline_accuracy:.3f}\\n\")\n",
        "\n",
        "print(\"ICL Classification Report:\")\n",
        "print(classification_report(icl_true, icl_pred, zero_division=0))\n",
        "\n",
        "print(\"Baseline Classification Report:\")\n",
        "print(classification_report(baseline_true, baseline_pred, zero_division=0))\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# Confusion Matrices (ICL and Baseline)\n",
        "labels_sorted = sorted(list(set(test_labels)))\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# ICL Confusion Matrix\n",
        "cm_icl = confusion_matrix(icl_true, icl_pred, labels=labels_sorted)\n",
        "disp_icl = ConfusionMatrixDisplay(confusion_matrix=cm_icl, display_labels=labels_sorted)\n",
        "disp_icl.plot(ax=axes[0], xticks_rotation=45, cmap=\"Blues\", colorbar=False)\n",
        "axes[0].set_title(\"ICL Confusion Matrix\")\n",
        "\n",
        "# Baseline Confusion Matrix\n",
        "cm_baseline = confusion_matrix(baseline_true, baseline_pred, labels=labels_sorted)\n",
        "disp_base = ConfusionMatrixDisplay(confusion_matrix=cm_baseline, display_labels=labels_sorted)\n",
        "disp_base.plot(ax=axes[1], xticks_rotation=45, cmap=\"Greens\", colorbar=False)\n",
        "axes[1].set_title(\"Baseline Confusion Matrix\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "W3eSReNEq4AZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Summary**\n",
        "\n",
        "This demo illustrates how LLMs can perform in-context learning (ICL) for multi-class image classification, using real weld defect imagery as the target domain. By supplying the model with a small set of annotated image–label pairs—covering six defect categories such as weld cracks, burn through, slag inclusion, weld splatter, lack of fusion, and surface porosity—we show that the model can infer subtle structural cues that distinguish one defect type from another. These cues include local texture disruptions, cavity patterns, shape irregularities, and characteristic weld-surface anomalies. Crucially, the model learns entirely from the examples embedded in the prompt: no fine-tuning, no gradient updates, and no specialized vision training occurs.\n",
        "\n",
        "To contextualize performance, the ICL approach is compared against a traditional machine-learning baseline that must learn directly from pixel-level information. While the baseline relies on supervised training and engineered features, the LLM derives its classification behavior purely from pattern recognition using images and the prompt. When evaluated on unseen weld defect images, ICL consistently performs better than the baseline—demonstrating stronger generalization from just a few examples and outperforming the benchmark in overall accuracy. These results highlight the efficiency and adaptability of ICL for inspection-style tasks, especially when training data is scarce or rapid deployment is required.\n",
        "\n",
        "## **Conclusion**\n",
        "\n",
        "This demonstration shows that LLMs can successfully classify complex weld defects using only in-context visual examples, effectively acting as prompt-driven inspectors capable of recognizing defect signatures from minimal supervision. The ICL paradigm proves especially powerful in this setting: with only nine example images, the model generalizes to new, unseen weld defects more reliably than the supervised baseline, reflecting the model’s flexibility and its ability to internalize visual patterns without any training pipeline.\n",
        "\n",
        "Compared to traditional approaches—which typically require substantial datasets, model tuning, and iterative optimization—the ICL method provides a fast, low-overhead alternative that can be adapted to new defect categories simply by revising the prompt. Together, the comparison between ICL and the baseline model demonstrates why LLMs are well-suited for rapid, lightweight visual classification tasks such as weld inspection, quality assurance, and defect triage, offering accurate and consistent performance with minimal setup and significantly reduced computational cost."
      ],
      "metadata": {
        "id": "fGbNdcqOx4u_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DD3usKIxx7db"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}