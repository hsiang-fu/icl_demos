{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fhNlpM88MMV"
      },
      "source": [
        "# **Introduction to In-Context Learning Demo: Linear Regression**\n",
        "This notebook provides a step-by-step walkthrough of an interactive demonstration showing how **Large Language Models (LLMs) can act as optimizers for optimal parameter estimation**. Inspired by recent work positioning LLMs as flexible meta-learners, this experiment uses a simple linear regression setting to investigate whether an LLM can iteratively propose improved parameter values (w, b) by observing model performance, previous attempts, and its own update history. Rather than computing gradients analytically, we allow the LLM—operating purely in non-reasoning mode—to navigate the optimization landscape using only in-context cues.\n",
        "\n",
        "The goal is to illustrate how an **LLM can behave like a general optimization engine: adjusting parameters, reducing loss,** and **converging towards a solution** after multiple rounds of proposal and feedback.\n",
        "\n",
        "## **Overview**\n",
        "This demonstration guides you through:\n",
        "1. Generating synthetic training data from a ground-truth linear function\n",
        "2. Running an iterative loop where the LLM proposes updated parameters\n",
        "3. Visualizing the optimization path across iterations\n",
        "4. Rendering an animation showing the model’s gradual convergence\n",
        "\n",
        "The result is a clear visualization of how an LLM can perform optimization through pattern-based updates—even in a numerical, regression-style task.\n",
        "\n",
        "## **Background**\n",
        "\n",
        "This demo centers on the classic linear regression form:\n",
        "`y = w⋅x + b`\n",
        "\n",
        "We generate a small set of 3 points from this ground-truth linear regression line and then give the LLM only the examples of points and a record of previous attempts. The model is not given gradients, derivative formulas, or closed-form solutions. Instead, it receives:\n",
        "* The training pairs\n",
        "* Its previous guesses\n",
        "* The resulting loss values\n",
        "\n",
        "From this, the model must infer a direction of improvement and produce new (w, b) proposals.\n",
        "\n",
        "What makes this setup interesting:\n",
        "* The LLM acts like an optimizer rather than a solver\n",
        "* The updates depend solely on in-context patterns\n",
        "* The model relies on recognizing relationships between parameters and loss\n",
        "\n",
        "This makes it an intuitive testbed for evaluating whether LLMs can learn to optimize through context alone.\n",
        "\n",
        "## **Let’s Take a Look at an Example**\n",
        "\n",
        "The visualization below shows:\n",
        "* The training points used for estimation\n",
        "* The sequences of candidate regression lines proposed across iterations\n",
        "* A colorbar indicating iteration step\n",
        "\n",
        "Each line other than the initial line represents the LLM’s current guess for the underlying linear function.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/intro-to-icl/intro-to-icl.github.io/refs/heads/master/static/images/linear_regression.gif\" height=\"500\">\n",
        "\n",
        "As the iterations progress, the proposed regression line typically shifts toward the true relationship, demonstrating how the model improves purely based on the examples and the loss given.\n",
        "\n",
        "## **LLM as the Optimizer**\n",
        "In this demo, we use Gemini 2.0/2.5 Flash in non-reasoning mode. This encourages the model to act not through symbolic derivation, but through pattern recognition and iterative refinement.\n",
        "\n",
        "The workflow is simple:\n",
        "- Provide the three (x,y) points\n",
        "- Supply the model’s current (w, b) guess and its loss\n",
        "- Ask the model to propose new values that reduce the loss\n",
        "- Repeat for 30–100 iterations\n",
        "\n",
        "The LLM becomes an optimization engine—adjusting parameters using only contextual signals. This further highlights the emerging ability of LLMs to perform optimization without explicit algorithmic implementations.\n",
        "\n",
        "## **Code Overview**\n",
        "The implementation is organized into a modular structure, with each component responsible for a different stage of the linear regression optimization pipeline. This design separates data loading, visualization, prompt construction, and LLM inference, making the system easy to understand, modify, and extend.\n",
        "\n",
        "## **Before Running the Demo, Follow the Instructions Below**\n",
        "To run the full experiment:\n",
        "1. Ensure all dependencies are imported and installed.\n",
        "2.\tVisit Google AI Studio (https://aistudio.google.com/) to obtain your Gemini API key.\n",
        "3.\tOnce API key is generated, copy and paste it into the demo when prompted.\n",
        "\n",
        "#### **If having trouble creating an API Key, follow the link below for instructions:**\n",
        "* #### **[Instructions on How to Obtain a Gemini API Key](https://docs.google.com/document/d/17pgikIpvZCBgcRh4NNrcee-zp3Id0pU0vq2SG52KIGE/edit?usp=sharing)**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhzfLjFw50v5"
      },
      "outputs": [],
      "source": [
        "#@title **Import Necessary Libraries**\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "import re\n",
        "import itertools\n",
        "import math\n",
        "import ast\n",
        "import getpass\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import ipywidgets as widgets\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import Image\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "d2ROw87H5j0O"
      },
      "outputs": [],
      "source": [
        "#@title **Setting Up Gemini Client**\n",
        "apikey = getpass.getpass(\"Enter your Gemini API Key: \")\n",
        "client = genai.Client(api_key=apikey)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Choose a Model**\n",
        "model_dropdown = widgets.Dropdown(\n",
        "    options=[\n",
        "        (\"Gemini 2.5 Flash\", \"gemini-2.5-flash\"),\n",
        "        (\"Gemini 2.0 Flash\", \"gemini-2.0-flash\")\n",
        "    ],\n",
        "    description=\"Model:\",\n",
        "    value=\"gemini-2.5-flash\",\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "confirm_button = widgets.Button(\n",
        "    description=\"Confirm Selection\"\n",
        ")\n",
        "\n",
        "output = widgets.Output()\n",
        "\n",
        "model_name = None\n",
        "\n",
        "def on_confirm_click(b):\n",
        "    global model_name, batch_size\n",
        "\n",
        "    model_name = model_dropdown.value\n",
        "\n",
        "    with output:\n",
        "        output.clear_output()\n",
        "        print(f\"\\nSelected model: {model_name}\")\n",
        "\n",
        "confirm_button.on_click(on_confirm_click)\n",
        "\n",
        "display(model_dropdown, confirm_button, output)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ZsHMlYEy7kJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkEeqSfR5j0P"
      },
      "source": [
        "## **Utlity Functions**\n",
        "\n",
        "Below are the cells that act as the backbone to the entire LLM optimization pipeline.\n",
        "\n",
        "The cell `Linear Task Randomized Generation` is in charge of generating the linear task the LLM is supposed to predict. It uses the line generated by random values of the slope and intercept to obtain 35 random noisy data points using the line and returns the true slope and intercept, along with the 35 (x, y) pairs\n",
        "\n",
        "The next cell `Prompt Generation` builds the text prompt given to the LLM during each optimization step. It formats the training point pairs and the history of previous (w, b) guesses with their losses, then inserts them into a structured instruction block. The function returns a prompt that tells the model to propose a better (w, b) and output it strictly as a JSON object.\n",
        "\n",
        "The following cell `Model Inference` contains a function that sends the generated prompt to the LLM and retrieves its proposed (w, b) values. It calls the model, retries after a delay if a quota error occurs, and then cleans and parses the model’s JSON-only response. Finally, it extracts the \"w\" and \"b\" fields from the parsed output and returns them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9i30_Zzj1IOs"
      },
      "outputs": [],
      "source": [
        "#@title **Linear Task Randomized Generation**\n",
        "def generate_linear_task(noise_ratio=0.2):\n",
        "    w = np.random.uniform(-5, 5)   # slope\n",
        "    b = np.random.uniform(-5, 5)   # intercept\n",
        "    n_train = 35                   # 50 points\n",
        "\n",
        "    x_train = np.random.uniform(-5, 5, n_train)\n",
        "\n",
        "    # Clean line\n",
        "    y_clean = w * x_train + b\n",
        "\n",
        "    # Scale noise to the signal range (10% of typical variation)\n",
        "    y_range = np.max(y_clean) - np.min(y_clean)\n",
        "    noise_std = noise_ratio * y_range\n",
        "\n",
        "    # Add Gaussian noise\n",
        "    noise = np.random.normal(0, noise_std, n_train)\n",
        "    y_train = y_clean + noise\n",
        "\n",
        "    # Round for readability\n",
        "    train_pairs = [(round(x, 2), round(y, 2)) for x, y in zip(x_train, y_train)]\n",
        "    w = round(w, 2)\n",
        "    b = round(b, 2)\n",
        "    return train_pairs, w, b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bh5rOyr1_BjH"
      },
      "outputs": [],
      "source": [
        "#@title **Prompt Generation**\n",
        "def generate_optimizer_prompt(train_pairs, history, it):\n",
        "\n",
        "    history_str = \"\\n\".join([f\"(w={w}, b={b}): loss={round(loss,2)}\" for w, b, loss in history])\n",
        "\n",
        "    examples_str = \"\\n\".join([f\"({x},{y})\" for x, y in train_pairs])\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are helping to discover the underlying linear relationship y=w*x+b.\n",
        "\n",
        "Here are the (x, y) numerical examples:\n",
        "{examples_str}\n",
        "\n",
        "Here are the previously tried (w, b) values and their losses:\n",
        "{history_str}\n",
        "\n",
        "You are currently at iteration {it} of 50.\n",
        "\n",
        "Your task:\n",
        "- Infer a better (w, b) that reduces the loss.\n",
        "- Propose exactly one (w, b) pair.\n",
        "- Return ONLY a JSON object of the form:\n",
        "  {{\"w\": number, \"b\": number}}\n",
        "\n",
        "Do not include any text, explanation, or code.\n",
        "    \"\"\"\n",
        "    return prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7BTEkiE_EVA"
      },
      "outputs": [],
      "source": [
        "#@title **Model Inference**\n",
        "def propose_wb(train_pairs, history,it):\n",
        "    prompt = generate_optimizer_prompt(train_pairs, history,it)\n",
        "    try:\n",
        "      response = client.models.generate_content(\n",
        "          model=model_name,\n",
        "          contents=[prompt],\n",
        "          config=types.GenerateContentConfig(\n",
        "              thinking_config=types.ThinkingConfig(thinking_budget=0)\n",
        "          ),\n",
        "      )\n",
        "    except:\n",
        "      print(\"\\nWaiting 60 seconds for quota limit reset\")\n",
        "      time.sleep(60)\n",
        "      return propose_wb(train_pairs, history,it)\n",
        "\n",
        "    cleaned = response.text.replace(\"```\",\"\").replace(\"json\",\"\").replace(\"python\",\"\").strip()\n",
        "\n",
        "    parsed = ast.literal_eval(cleaned)\n",
        "    w = parsed[\"w\"]\n",
        "    b = parsed[\"b\"]\n",
        "    return w, b\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfSgciiSXPGj"
      },
      "source": [
        "## **Let's look at an example of the prompt**\n",
        "```\n",
        "You are trying to discover the underlying linear line from the numerical examples.\n",
        "\n",
        "Here are the example (x,y) points:\n",
        "{xy_points}\n",
        "\n",
        "Previously tried w, b and the loss:\n",
        "{optimization_history}\n",
        "\n",
        "You are currently at iteration {it} out of 50\n",
        "\n",
        "Your task:\n",
        "- Infer a better (w, b) that reduces the loss.\n",
        "- Propose exactly one (w, b) pair.\n",
        "- Return ONLY a JSON object of the form:\n",
        "  {{\"w\": number, \"b\": number}}\n",
        "\n",
        "Do not include any text, explanation, or code.\n",
        "```\n",
        "\n",
        "This prompt shows the model several numerical point pairs along with a running history of previously attempted linear parameters, each associated with its loss. The examples illustrate the points from the true line the model is trying to find, while the optimization history demonstrates how well past guesses have performed. The model’s objective is to infer the underlying linear relationship and propose a better (w,b) that should reduce the loss in the next step.\n",
        "\n",
        "This is considered ICL because the model is never retrained or fine-tuned; it performs all reasoning directly inside the prompt. The example points and the optimization history act as temporary, in-context supervision that guides the model toward better parameter choices. By interpreting these examples and losses, the model infers the pattern, adjusts its guess, and outputs improved parameters—all within a single forward pass—demonstrating learning from context rather than parameter updates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xETg1ERbZZK5"
      },
      "source": [
        "The cell below `Main Optimization Loop` runs the full optimization loop where the LLM repeatedly proposes improved values of w and b. It begins by generating a synthetic linear task, computing an initial random guess, and recording its loss; then for each iteration, it calls the LLM to propose a new (w,b), evaluates the loss of that proposed values, and appends the result to the optimization history. After completing all iterations, it returns the training data, the full sequence of guesses, and the true underlying parameters, and finally prints the loss of the last LLM-proposed solution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fonu9u_D5j0P"
      },
      "outputs": [],
      "source": [
        "#@title **Main Optimization Loop**\n",
        "from tqdm import tqdm\n",
        "\n",
        "def compute_loss(train_pairs, w, b):\n",
        "    xs = np.array([p[0] for p in train_pairs])\n",
        "    ys = np.array([p[1] for p in train_pairs])\n",
        "    pred = w * xs + b\n",
        "    return float(np.mean((pred - ys) ** 2))\n",
        "\n",
        "train_pairs, w_true, b_true = generate_linear_task()\n",
        "\n",
        "def run_iterations(n_iterations=50):\n",
        "\n",
        "    history = []\n",
        "    w_initial = round(np.random.uniform(-5, 5), 2)\n",
        "    b_initial = round(np.random.uniform(-5, 5), 2)\n",
        "    loss_initial = compute_loss(train_pairs, w_initial, b_initial)\n",
        "    history.append((float(w_initial), float(b_initial), loss_initial))\n",
        "\n",
        "\n",
        "    # Add tqdm progress bar\n",
        "    for it in tqdm(range(n_iterations), desc=\"Optimizing (LLM proposing w,b)\"):\n",
        "        w, b = propose_wb(train_pairs, history, it)\n",
        "        if w is None or b is None:\n",
        "            history.append((None, None, 1e9))\n",
        "            continue\n",
        "\n",
        "        loss = compute_loss(train_pairs, w, b)\n",
        "        history.append((float(w), float(b), loss))\n",
        "\n",
        "    return train_pairs, history, w_true, b_true\n",
        "\n",
        "n_iterations = 50\n",
        "train_pairs, history, w_true, b_true = run_iterations(n_iterations)\n",
        "\n",
        "final_w, final_b, final_loss = history[-1]\n",
        "print(\"\\nFinal loss:\", final_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdNBCw9NZut2"
      },
      "source": [
        "The cell below dispalys a cisualization for how each LLM-proposed (w,b) evolves over the course of the ICL optimization process. It constructs every model-predicted line from the history, assigns each iteration a color on a gradient, and plots the example points provided. Then, using an animation, it draws one fitted line per frame—adding the newest line and shows how the model's guesses gradually move toward the correct linear fit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9yTQGc9DN2N"
      },
      "outputs": [],
      "source": [
        "#@title **Linear Regression Optimization Process**\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML\n",
        "from matplotlib.lines import Line2D\n",
        "import matplotlib.cm as cm\n",
        "from matplotlib.colors import Normalize\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "x = np.linspace(-10, 10, 200)\n",
        "y_true = w_true * x + b_true\n",
        "model_lines = [(w, b, w * x + b) for (w, b, _) in history]\n",
        "num_iters = len(model_lines)\n",
        "\n",
        "cmap = matplotlib.colormaps.get_cmap(\"plasma\")\n",
        "norm = Normalize(vmin=0, vmax=num_iters - 1)\n",
        "colors = [cmap(norm(i)) for i in range(num_iters)]\n",
        "\n",
        "xs = np.array([p[0] for p in train_pairs], dtype=float)\n",
        "ys = np.array([p[1] for p in train_pairs], dtype=float)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# ax.plot(x, y_true, \"k--\", label=\"Ground Truth\", linewidth=2)\n",
        "ax.scatter(xs, ys, facecolor='white', edgecolor='black', s=100, linewidths=2)\n",
        "ax.set_xlim(x.min(), x.max())\n",
        "ax.set_ylim(-25, 25)\n",
        "ax.set_xlabel(\"x\")\n",
        "ax.set_ylabel(\"y\")\n",
        "\n",
        "divider = make_axes_locatable(ax)\n",
        "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.3)\n",
        "\n",
        "cb = plt.colorbar(\n",
        "    cm.ScalarMappable(norm=norm, cmap=cmap),\n",
        "    cax=cax\n",
        ")\n",
        "cb.set_label(\"Step\", rotation=90)\n",
        "\n",
        "plotted_lines = []\n",
        "\n",
        "def update(frame):\n",
        "    w, b, y = model_lines[frame]\n",
        "\n",
        "    # color for this iteration\n",
        "    color = colors[frame]\n",
        "\n",
        "    # newest line (opaque)\n",
        "    new_line, = ax.plot(x, y, color=color, alpha=0.5, linewidth=2)\n",
        "    plotted_lines.append(new_line)\n",
        "\n",
        "    # fade old lines\n",
        "    for i, line in enumerate(plotted_lines):\n",
        "        age = len(plotted_lines) - 1 - i\n",
        "        # line.set_alpha(max(0.1, 1 - age * 0.07))\n",
        "\n",
        "    ax.set_title(\n",
        "\n",
        "        f\"Linear Regression Optimization | Iteration {frame+1}/{num_iters}\"\n",
        "    )\n",
        "\n",
        "    return plotted_lines\n",
        "\n",
        "ani = FuncAnimation(fig, update, frames=num_iters, interval=400, blit=False)\n",
        "ani.save(\"linear_regression_optimization_path.gif\", writer=\"pillow\", fps=5)\n",
        "\n",
        "plt.close()\n",
        "gif_path = \"linear_regression_optimization_path.gif\"\n",
        "# Display interactive animation only\n",
        "display(Image(filename=gif_path))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Comparison between true slope/intercept values & predicted slope/interecept values**\n",
        "print(f\"Ground Truth - Slope: {w_true}, Intercept: {b_true}\")\n",
        "print(f\"Predicted - Slope: {final_w}, Intercept: {final_b}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "zCT9UEMoTREd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVaMXybUZ_lC"
      },
      "source": [
        "The cell below then extracts the loss from each optimization step and plots how it changes over time. It creates a simple line plot where the x-axis represents the iteration number and the y-axis shows the corresponding loss value. The resulting figure visualizes whether the LLM’s proposed (w,b) values are improving by showing the trend of the loss across iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "N3fHxaYa5j0Q"
      },
      "outputs": [],
      "source": [
        "#@title **Visualize Distribution of Loss Over Iterations**\n",
        "\n",
        "loss_values = [entry[2] for entry in history]\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(loss_values)\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Loss over Iterations\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cbur2GHraYQk"
      },
      "source": [
        "## **Summary**\n",
        "\n",
        "This demo illustrates how LLMs can perform ICL by iteratively improving a linear line by using only numerical examples and loss feedback contained within the prompt. By presenting a small set of (x,y) pairs along with the history of previously attempted (w,b) values and their losses, the model is encouraged to infer the underlying linear relationship and propose better parameters without any explicit gradient formulas or training loops. Each iteration provides new context—updated guesses and their associated errors—allowing the LLM to adjust its predictions and gradually approach the true line through pattern recognition rather than parameter updates. Through this setup, we observe how the model implicitly learns to reduce error over time, effectively behaving as an optimizer driven purely by context.\n",
        "\n",
        "Beyond this specific linear regression example, the same methodology can be extended to a broad range of optimization and model-fitting problems. By embedding intermediate states, errors, or constraints into the prompt, LLMs can be guided to improve polynomial fits, tune hyperparameters, refine curve approximations, or even propose solutions to custom objective functions defined by the user. These variations highlight the versatility of the in-context learning paradigm for continuous and numerical reasoning tasks, demonstrating how LLMs can perform iterative improvement when framed through well-structured, example-driven prompts.\n",
        "\n",
        "## **Conclusion**\n",
        "\n",
        "This demonstration shows that LLMs can engage in iterative numerical reasoning using only the information supplied in the prompt—capturing the core idea of in-context learning in the setting of linear regression. Without receiving gradients, explicit optimization rules, or model updates, the LLM learns from the pattern of losses and examples to produce increasingly accurate (w,b) values over time. Although the model is not performing true mathematical optimization, its ability to approximate the underlying line through repeated contextual feedback reveals a promising direction for using LLMs as lightweight, prompt-driven optimizers. Linear regression therefore serves as a clear and intuitive example of how LLMs can adapt, refine, and improve numerical predictions purely from context."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q8pVXS6QVg7F"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}