{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction to In-Context Learning Demo: Using LLMs as Numerical Optimizers**\n",
        "\n",
        "This notebook provides a step-by-step walkthrough to demonstrate the capabilities of LLMs as Numerical Optimizers. In one of the previous works of the lab: *[SAS Prompt](https://arxiv.org/pdf/2504.20459)* (Ben Amor et al., 2025), we show that LLMs have a built-in ability to perform (stochastic) numerical optimization and that this property can be leveraged for explainable robot policy search. In this tutorial we would be optimizing 1D Function and 2D Ackley using Gemini 2.0/2.5 Flash.\n",
        "\n",
        "# **Overview**\n",
        "This demonstration guides you through:\n",
        "- **Part 1**: Optimizing 1D Ackley Functions using Gemini, compare to Adam's optimization process and visualize the difference in the optimization process.\n",
        "- **Part 2**: Optimizing 2D Ackley Functions using Gemini, compare to Adam's optimization process and visualize the difference in the optimization process.\n",
        "- **Part 3**: Comparing Both 1D and 2D Ackley Optimizations using Gemini and other baselines.\n",
        "\n",
        "The result contains clear visualizations of how LLMs can behave as gradient-free numerical optimizers, and how they compare to traditional approaches.\n",
        "\n",
        "# **Background**\n",
        "This demo adapts one key concept from the SAS-Prompt methodology presented in the original work(Ben Amor et al., 2025):\n",
        "1. LLMs as Numerical Optimizers: the SAS-Prompt paper shows that LLMs can perform global optimization on functions such as Ackley and Rastrigin—using no derivative information—by proposing successive candidate values and adjusting based on observed outcomes. This emergent ability enables gradient-free policy search directly inside the LLM.\n",
        "\n",
        "**Why This Is Interesting**\n",
        "\n",
        "Just as the original paper demonstrates numerical optimization without explicit reward functions or gradient updates, this demo shows that:\n",
        "\n",
        "1. The LLM infers parameter–behavior relationships (a textual gradient)\n",
        "\n",
        "2. Optimization happens entirely through language(prompting) + numbers\n",
        "\n",
        "3. The LLM reasons and synthesizes without external training\n",
        "\n",
        "This setup benchmarks the LLM’s ability to conduct explainable, in-context, and iterative optimization based solely on observed results.\n",
        "\n",
        "##  **Let's take a look at two examples (1D & 2D Numerical Optimization)**\n",
        "\n",
        "The following animations visualize the LLM-based optimization process. We notice how the LLM gradually moves the current best estimate closer and closer to the global minimum. Our results indicate that this LLM-based numerical optimization is competitive with standard optimization algorithms (Adam, Nelder-Mead, GD) in low-dimensional settings (up to 8 dimensions studied in the paper). (Ben Amor et al., 2025)\n",
        "\n",
        "\n",
        "### **1D Ackley Numerical Optimization**\n",
        "\n",
        "This first animation displays Part 1 of the demo, using a LLM to optimize a 1D Ackley Function, here we can notice how the LLM gradually moves its best estimate closer and closer to the global minimum.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/intro-to-icl/intro-to-icl.github.io/refs/heads/master/static/images/1d_optimization.gif\" height=\"500\">\n",
        "\n",
        "### **2D Ackley Numerical Optimization**\n",
        "\n",
        "This second animation displays Part 2 of the demo, using a LLM to optimize a 2D Ackley Function, here we can notice how the LLM gradually moves its best estimate closer and closer to the global minimum.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/intro-to-icl/intro-to-icl.github.io/refs/heads/master/static/images/numerical_optimization.gif\" height=\"500\">\n",
        "\n",
        "## **LLM as the Optimizer**\n",
        "\n",
        "As in the original SAS prompt experiments, we use Gemini 2.0/2.5 Flash, ensuring behavior consistent with pattern-based, gradient-free optimization rather than explicit symbolic reasoning.\n",
        "\n",
        "The workflow follows the Numerical Optimization section in the SAS-Prompt paper.\n",
        "\n",
        "## **Code Overview**\n",
        "The implementation is organized into a modular structure, with each component responsible for a different stage of the numerical optimization pipeline. This design separates data generation, visualization, prompt construction, LLM inference, and result interpretation, making the system easy to understand, modify, and extend.\n",
        "\n",
        "## **Before Running the Demo, Follow the Instructions Below**\n",
        "To run the full experiment:\n",
        "1. Ensure all dependencies are imported and installed.\n",
        "2.\tVisit Google AI Studio (https://aistudio.google.com/) to obtain your Gemini API key.\n",
        "3.\tOnce API key is generated, copy and paste it into the demo when prompted.\n",
        "\n",
        "#### **If having trouble creating an API Key, follow the link below for instructions:**\n",
        "* #### **[Instructions on How to Obtain a Gemini API Key](https://docs.google.com/document/d/17pgikIpvZCBgcRh4NNrcee-zp3Id0pU0vq2SG52KIGE/edit?usp=sharing)**\n",
        "\n"
      ],
      "metadata": {
        "id": "JWBl0iMOvbE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Import Necessary Libraries**\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from google import genai\n",
        "import matplotlib.colors as mcolors\n",
        "import matplotlib.cm as cm\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "import re\n",
        "from scipy.optimize import minimize\n",
        "from google.genai import types\n",
        "import getpass\n",
        "import ipywidgets as widgets"
      ],
      "metadata": {
        "id": "qDV03CJIve4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lh6d1fWygOfe"
      },
      "outputs": [],
      "source": [
        "#@title **Setting Up Gemini Client**\n",
        "apikey = getpass.getpass(\"Enter your Gemini API Key: \")\n",
        "client = genai.Client(api_key=apikey)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Choose a Model**\n",
        "model_dropdown = widgets.Dropdown(\n",
        "    options=[\n",
        "        (\"Gemini 2.5 Flash\", \"gemini-2.5-flash\"),\n",
        "        (\"Gemini 2.0 Flash\", \"gemini-2.0-flash\")\n",
        "    ],\n",
        "    description=\"Model:\",\n",
        "    value=\"gemini-2.5-flash\",\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "confirm_button = widgets.Button(\n",
        "    description=\"Confirm Selection\"\n",
        ")\n",
        "\n",
        "output = widgets.Output()\n",
        "\n",
        "model_name = None\n",
        "\n",
        "def on_confirm_click(b):\n",
        "    global model_name, batch_size\n",
        "\n",
        "    model_name = model_dropdown.value\n",
        "\n",
        "    with output:\n",
        "        output.clear_output()\n",
        "        print(f\"\\nSelected model: {model_name}\")\n",
        "\n",
        "confirm_button.on_click(on_confirm_click)\n",
        "\n",
        "display(model_dropdown, confirm_button, output)"
      ],
      "metadata": {
        "id": "s_qmVjat2iqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cell below `Numerical Function Definitions` defines the two benchmark functions used in this numerical optimization demo.\n",
        "- 1D variant of the Ackley function. It produces a non-convex landscape with a mixture of exponential, oscillatory, and quadratic behavior, making it useful for testing how optimization methods handle multiple local minima in one dimension.\n",
        "- A shifted 2D Ackley function. By allowing horizontal and vertical shifts (and an optional vertical offset), it lets you reposition the global minimum and modify the landscape. The 2D Ackley is a classic stress-test function for global optimizers because of its many local minima surrounding a central basin."
      ],
      "metadata": {
        "id": "G_OcFRX3sp01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Numerical Function Definitions**\n",
        "def one_dimensional_function(x):\n",
        "  return x**2 + 5*torch.sin(2*x)\n",
        "\n",
        "def shifted_ackley(x, y, h_x=10, h_y=15, c=0):\n",
        "    \"\"\"\n",
        "    Shifted Ackley function.\n",
        "\n",
        "    This function computes the Shifted Ackley function, a 2D benchmark function\n",
        "    often used in optimization. It applies horizontal and vertical shifts to the\n",
        "    standard Ackley function.\n",
        "\n",
        "    Args:\n",
        "        x (torch.Tensor): Input tensor for the x-dimension.\n",
        "        y (torch.Tensor): Input tensor for the y-dimension.\n",
        "        h_x (float, optional): Horizontal shift value. Defaults to 10.\n",
        "        h_y (float, optional): Vertical shift value. Defaults to 15.\n",
        "        c (float, optional): Vertical shift constant. Defaults to 0.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: The computed Shifted Ackley function value.\n",
        "    \"\"\"\n",
        "    # Apply horizontal shift\n",
        "    x_shifted = x - h_x\n",
        "    y_shifted = y - h_y\n",
        "\n",
        "    # Compute the original Ackley function with shifted inputs\n",
        "    term1 = -20 * torch.exp(-0.2 * torch.sqrt(0.5 * (x_shifted**2 + y_shifted**2)))\n",
        "    term2 = -torch.exp(0.5 * (torch.cos(2 * np.pi * x_shifted) + torch.cos(2 * np.pi * y_shifted)))\n",
        "    return term1 + term2 + 20 + np.e + c  # Apply vertical shift by adding c"
      ],
      "metadata": {
        "id": "xq29ulL2m-iY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `Utility Functions` cell provides a set of utility functions that support the numerical optimization demo by handling visualization, text parsing, and initializing baseline model comparisons. The plotting functions (plot_1d and plot_2d) create visualizations that show how an optimizer or LLM moves across the 1D and 2D objective landscapes, overlaying its trajectory on top of the true function surface. These visual tools make it easy to diagnose behavior such as convergence, oscillation, or getting stuck in local minima.\n",
        "\n",
        "The other help the workflow operate smoothly: small parsing helpers extract numerical values from LLM-generated text, while the included baseline optimizers (Nelder–Mead and gradient descent) provide comparison results to compare against ICL."
      ],
      "metadata": {
        "id": "OFUTLDI5RppT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Utility Functions**\n",
        "def plot_1d(data_trace, x_range, title):\n",
        "\n",
        "  x_values = np.linspace(x_range[0], x_range[1], 400)\n",
        "  t = torch.tensor(x_values, dtype=torch.float32)\n",
        "  y_values = one_dimensional_function(t).detach().numpy()\n",
        "  fig, ax = plt.subplots(figsize=(8, 6), dpi=120)\n",
        "  ax.plot(x_values, y_values, color='black', label='f(x)', zorder=1, lw=2)\n",
        "  colors = plt.cm.viridis(np.linspace(0, 1, len(data_trace)))\n",
        "\n",
        "  for step, pt in enumerate(data_trace):\n",
        "    ax.scatter(pt[0], pt[1], s=130, color=colors[step], marker='o', zorder=2+step)\n",
        "\n",
        "  ax.set_title(f'{title}')\n",
        "  ax.set_xlabel('x')\n",
        "  ax.set_ylabel('f(x)')\n",
        "  ax.legend()\n",
        "  norm = mcolors.Normalize(vmin=0, vmax=len(data_trace))\n",
        "  sm = cm.ScalarMappable(cmap='plasma', norm=norm)\n",
        "  sm.set_array([])\n",
        "  fig.colorbar(sm, ax=ax, label='Step')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_2d(data_trace, title, elev=20, azim=50, dpi=120, width=10, height=8):\n",
        "    figsize = (width, height)\n",
        "    fig = plt.figure(figsize=figsize, dpi=dpi)\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    ax.grid(False)\n",
        "\n",
        "    x = torch.linspace(0, 20, 500)\n",
        "    y = torch.linspace(5, 25, 500)\n",
        "    X, Y = torch.meshgrid(x, y, indexing='ij')\n",
        "    Z = shifted_ackley(X, Y)\n",
        "\n",
        "\n",
        "    surf = ax.plot_surface(X.numpy(), Y.numpy(), Z.numpy(), cmap='Blues_r', edgecolor='none', alpha=0.3, zorder=1)\n",
        "    ax.view_init(elev=elev, azim=azim)\n",
        "\n",
        "    x_points = np.array([pt[0] for pt in data_trace])\n",
        "    y_points = np.array([pt[1] for pt in data_trace])\n",
        "    z_points = np.array([pt[2] for pt in data_trace])\n",
        "    z_points_above = z_points + 2.5\n",
        "\n",
        "\n",
        "    cmap_points = cm.plasma\n",
        "    norm = plt.Normalize(0, len(data_trace))\n",
        "    point_colors = cmap_points(norm(range(len(data_trace))))\n",
        "    point_colors[:, -1] = 1.0\n",
        "\n",
        "\n",
        "    ax.scatter(x_points, y_points, z_points_above, c=point_colors, s=80, depthshade=False, edgecolor='black', linewidth=0.5, zorder=2)\n",
        "    cmap = cm.plasma\n",
        "    norm = plt.Normalize(0, len(data_trace) - 1)\n",
        "    sm = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
        "    # sm.set_array([])\n",
        "    cbar = plt.colorbar(sm, ax=ax, pad=0.1, shrink=0.6, aspect=20)\n",
        "    cbar.set_label('Step')\n",
        "\n",
        "    ax.set_title(f'{title}')\n",
        "    ax.set_xlabel('X-axis')\n",
        "    ax.set_ylabel('Y-axis')\n",
        "    ax.set_zlabel('Z-axis')\n",
        "\n",
        "    ax.set_xlim(x.min().item(), x.max().item())\n",
        "    ax.set_ylim(y.min().item(), y.max().item())\n",
        "    ax.set_zlim(Z.min().item(), Z.max().item() + 2.5)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def extract_llm_x(text):\n",
        "    pattern = r'[-+]?\\d*\\.\\d+|[-+]?\\d+'\n",
        "    match = re.search(pattern, text)\n",
        "    if match:\n",
        "        return float(match.group())\n",
        "    return None\n",
        "\n",
        "def extract_llm_xy(text):\n",
        "    pattern = r'x:\\s*([-+]?\\d*\\.\\d+|[-+]?\\d+).*?y:\\s*([-+]?\\d*\\.\\d+|[-+]?\\d+)'\n",
        "    match = re.search(pattern, text)\n",
        "    if match:\n",
        "        x_value = float(match.group(1))\n",
        "        y_value = float(match.group(2))\n",
        "        return x_value, y_value\n",
        "    return None, None\n",
        "\n",
        "def optimize_with_nelder_mead(func, initial_x, max_iter=100):\n",
        "    # Function to convert tensor calculations for scipy optimization\n",
        "    def func_wrapper(x):\n",
        "        if len(initial_x)==2:\n",
        "          return func(torch.tensor(x[0], dtype=torch.float32), torch.tensor(x[1], dtype=torch.float32)).item()\n",
        "        else:\n",
        "          return func(torch.tensor(x[0], dtype=torch.float32)).item()\n",
        "\n",
        "    result = minimize(func_wrapper, initial_x, method='Nelder-Mead', options={'maxiter': max_iter})\n",
        "    return result\n",
        "\n",
        "def optimize_gd(initial_x, func, MAX_STEPS=100, learning_rate=0.005):\n",
        "    initial_x = torch.tensor(initial_x)\n",
        "    x = initial_x.clone().requires_grad_()\n",
        "    for step in range(MAX_STEPS):\n",
        "        if len(initial_x)==2:\n",
        "          loss = func(x[0], x[1])\n",
        "        else:\n",
        "          loss = func(x)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            x -= learning_rate * x.grad\n",
        "        x.grad.zero_()\n",
        "\n",
        "        current_loss = loss.item()\n",
        "\n",
        "    return round(current_loss,3)"
      ],
      "metadata": {
        "id": "wOrkwDO_iCt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Let's take a first look at the prompt**\n",
        "This is the general structure of the prompt that's being sent to the LLM to be used for numerical optimization.\n",
        "\n",
        "**LLM 1D Optimization Prompt**\n",
        "```\n",
        "You are an expert optimization assistant, helping me find the global minimum of a mathematical function. I will give you the function evaluation f(x) and the current iteration number at each step. Your goal is to propose input values 'x' that efficiently lead us to the global minimum within a limited number of iterations (MAX_STEPS).\n",
        "\n",
        "Here's how we'll interact:\n",
        "\n",
        "1. I will first provide MAX_STEPS along with a few training examples of the form 'x, f(x)'.\n",
        "2. You will provide your response in the following exact format:\n",
        "    * Line 1: a new input 'x: ', aiming to minimize the function's value f(x). Choose x in range [-10,10]\n",
        "    * Line 2: a brief explanation of why you chose that input in one sentence, considering the current iteration.\n",
        "3. I will then provide the function's value f(x) at your suggested point, and the current iteration.\n",
        "4. We will repeat steps 2-3 until we reach the maximum number of iterations.\n",
        "\n",
        "Remember:\n",
        "\n",
        "* **Assume no prior knowledge about the function's specific form.**\n",
        "* **Balance Exploitation and Exploration:**  Early on, explore the input space broadly. As iterations increase, focus more on promising regions based on observed f(x) values.\n",
        "* **Be adaptable:**  Your approach might need to change based on the function's behavior and the remaining iterations. If you suspect a local minimum or slow progress for too long, try more exploratory values and then exploit any promising findings based on your understanding of the function.\n",
        "\"\"\"\n",
        "```\n",
        "\n",
        "**LLM 2D Optimization Prompt**\n",
        "```\n",
        "You are an optimization assistant, helping me find the global minimum of a mathematical function.  I will give you the function evaluation and the current iteration number at each step. Your goal is to propose input values that efficiently lead us to the global minimum within a limited number of iterations (MAX_STEPS).\n",
        "\n",
        "Here's how we'll interact:\n",
        "\n",
        "1. I will first provide MAX_STEPS along with few training examples.\n",
        "2. You will provide your response in the following exact format:\n",
        "    * Line 1: a new input 'x: , y: ', aiming to minimize the function's value f(x,y). Choose x is range [2,12] and y in [8,19]\n",
        "    * Line 2: a brief (one sentence) explanation of why you chose that input, considering the current iteration.\n",
        "3. I will then provide the function's value f(x,y) at that point, and the current iteration.\n",
        "4. We will repeat steps 2-3 until we reach the maximum number of iterations.\n",
        "\n",
        "Remember:\n",
        "\n",
        "* **Assume no prior knowledge about the function's specific form.**\n",
        "* **Balance Exploitation and Exploration:**  Early on, explore broadly. As iterations increase, focus more on promising regions.\n",
        "* **Be adaptable:**  Your approach might need to change based on the function's behavior and the remaining iterations. If you think you are stuck in a local minima or making small increments for too long, try more exploratory values and then eventually exploit new values based on your understanding of the function.\n",
        "\n",
        "\"\"\"\n",
        "```\n",
        "\n",
        "These two prompts each sets up an loop where the LLM acts as an optimization agent trying to find the global minimum of an unknown function using only function evaluations. The model is instructed to propose new inputs step by step, balancing early exploration of the search space with later exploitation of promising regions, while adapting its strategy based on observed values and remaining iterations. By providing example evaluations and enforcing a strict output format, the prompt guides the LLM to behave like a numerical optimization assistant, updating its beliefs about where the minimum may lie, and adjusting its search trajectory across iterations without assuming any explicit form of the underlying function."
      ],
      "metadata": {
        "id": "1t9hVOL44wuS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 1: Optimizing 1D Ackley Fuction**\n",
        "\n",
        "The `Initial Values` cell is in charge of defining the initial start point for the optimization problem and the amount of iterations to perform for optimization."
      ],
      "metadata": {
        "id": "fABV2_ODEUgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Initial Values**\n",
        "Initial_1d_x = -8.7\n",
        "# Initial_1d_x = 6.25\n",
        "\n",
        "x_1d_range = (-10, 10)\n",
        "\n",
        "max_steps = 50\n",
        "num_train_samples = 2\n"
      ],
      "metadata": {
        "id": "h_QXu9OalO_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1D Optimization Using Adam**\n",
        "\n",
        "This series of cells performs 1D optimization using the Adam optimizer by repeatedly updating a single variable, recording each step’s (x, f(x)) values. After completing the optimization loop, it visualizes Adam’s trajectory on the 1D function using plot_1d, allowing you to see how the optimizer navigates the landscape and converges toward a minimum."
      ],
      "metadata": {
        "id": "NkxcjLfcvzzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Optimizing 1D Functions using Adam**\n",
        "x_1d = torch.tensor([Initial_1d_x], requires_grad=True)\n",
        "optimizer_1d = optim.Adam([x_1d], lr=0.5)\n",
        "LLM_1D_Training_Samples = \"\"\n",
        "adam_1d_data_trace = []\n",
        "for step in range(max_steps):\n",
        "    optimizer_1d.zero_grad()\n",
        "    loss = one_dimensional_function(x_1d)\n",
        "\n",
        "    if step < num_train_samples:\n",
        "        LLM_1D_Training_Samples+= f\"x: {round(x_1d.item(),3)}\\nf(x): {round(loss.item(),3)}\\n\\n\"\n",
        "\n",
        "    adam_1d_data_trace.append((x_1d.item(), loss.item()))\n",
        "    loss.backward()\n",
        "    optimizer_1d.step()\n",
        "print(f\"Adam's Results:\\nBest x: {x_1d.item()}, Best f(x): {loss.item()}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "fe0YXZ-ci4re"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Plot Adam Optimization Path**\n",
        "plot_1d(adam_1d_data_trace, x_1d_range, \"Adam's Optimization Path on f(x)\")"
      ],
      "metadata": {
        "id": "gHtLBnHBjBT2",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1D Optimization using an LLM**\n",
        "\n",
        "This series of cells performs 1D numerical optimization using an LLM as the update rule: at each iteration, the model receives the current state, proposes a new x-value, and the true function value f(x) is computed and fed back into the prompt for the next step. The process records each predicted (x, f(x)) pair along with the model’s explanation, building a trajectory of how the LLM explores the search space. This allows you to observe whether the LLM learns from the iterative feedback, improves its proposals over time, and moves toward lower function values in a manner comparable to traditional optimization algorithms. It then visualizes the LLM's trajectory on the 1D function using plot_1d, allowing you to see how the LLM navigates the landscape and converges toward a minimum."
      ],
      "metadata": {
        "id": "cR9XAu-ToCsk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Training Sample for LLM 1D Optimization**\n",
        "print(LLM_1D_Training_Samples)"
      ],
      "metadata": {
        "id": "VkF4TTyojJzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **System Prompt for 1D Optimization**\n",
        "LLM_1D_Training_Samples = \"\"\n",
        "system_instruction = \"\"\"\n",
        "You are an expert optimization assistant, helping me find the global minimum of a mathematical function. I will give you the function evaluation f(x) and the current iteration number at each step. Your goal is to propose input values 'x' that efficiently lead us to the global minimum within a limited number of iterations (MAX_STEPS).\n",
        "\n",
        "Here's how we'll interact:\n",
        "\n",
        "1. I will first provide MAX_STEPS along with a few training examples of the form 'x, f(x)'.\n",
        "2. You will provide your response in the following exact format:\n",
        "    * Line 1: a new input 'x: ', aiming to minimize the function's value f(x). Choose x in range [-10,10]\n",
        "    * Line 2: a brief explanation of why you chose that input in one sentence, considering the current iteration.\n",
        "3. I will then provide the function's value f(x) at your suggested point, and the current iteration.\n",
        "4. We will repeat steps 2-3 until we reach the maximum number of iterations.\n",
        "\n",
        "Remember:\n",
        "\n",
        "* **Assume no prior knowledge about the function's specific form.**\n",
        "* **Balance Exploitation and Exploration:**  Early on, explore the input space broadly. As iterations increase, focus more on promising regions based on observed f(x) values.\n",
        "* **Be adaptable:**  Your approach might need to change based on the function's behavior and the remaining iterations. If you suspect a local minimum or slow progress for too long, try more exploratory values and then exploit any promising findings based on your understanding of the function.\n",
        "\"\"\"\n",
        "\n",
        "context_1d = f\"MAX_STEPS: {max_steps - num_train_samples}\\n\" + LLM_1D_Training_Samples"
      ],
      "metadata": {
        "collapsed": true,
        "id": "LQNXeLbsjP9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **LLM Numerical Optimization**\n",
        "llm_1d_data_trace = []\n",
        "\n",
        "for iter in range(max_steps - num_train_samples):\n",
        "    time.sleep(2)\n",
        "    try:\n",
        "        response = client.models.generate_content(\n",
        "            model=model_name,\n",
        "            contents=[context_1d],\n",
        "            config=types.GenerateContentConfig(\n",
        "                thinking_config=types.ThinkingConfig(thinking_budget=0),\n",
        "                system_instruction=system_instruction),\n",
        "            )\n",
        "\n",
        "        response_text = response.text\n",
        "\n",
        "        llm_x = extract_llm_x(response_text)\n",
        "        llm_explain = response_text.split('\\n')[1]\n",
        "\n",
        "        llm_x = float(llm_x)\n",
        "        f_x = round(one_dimensional_function(torch.tensor([llm_x])).item(), 3)\n",
        "\n",
        "        context_1d = f\"step: {iter}\\nx: {round(llm_x, 3)}\\nf(x): {f_x}\"\n",
        "\n",
        "        llm_1d_data_trace.append((llm_x, f_x))\n",
        "\n",
        "        print(f\"Step: {iter}\")\n",
        "        print(f\"llm_x: {llm_x}\")\n",
        "        print(f\"f(x): {f_x}\")\n",
        "        print(\"LLM's Explanation:\", llm_explain)\n",
        "        print(\" \")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error at step {iter}: {e}\")\n",
        "        print(\"Pausing for 60 seconds before continuing...\")\n",
        "        time.sleep(60)\n",
        "        continue\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "JYTDtp7VjVOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Plot Gemini 1D Optimization Path**\n",
        "plot_1d(llm_1d_data_trace, x_1d_range, \"Gemini's Optimization Path on f(x)\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "eB3hMHFFjzYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 2: Optimizing 2D Ackley Fuction**\n",
        "\n",
        "The `Initial Values` cell is in charge of defining the initial start point (x,y) for the optimization problem and the amount of iterations to perform for optimization."
      ],
      "metadata": {
        "id": "VdIqwwE0ENEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Initial Values**\n",
        "x_2d_range = (2, 12)\n",
        "y_2d_range = (8, 19)\n",
        "\n",
        "#Choose Custom Initial Point\n",
        "initial_2dX = 7.319\n",
        "initial_2dY = 8.299\n",
        "\n",
        "# or Choose Random Initial Point from Range\n",
        "# initial_2dX = round(random.uniform(x_2d_range[0], x_2d_range[1]),3)\n",
        "# initial_2dY = round(random.uniform(y_2d_range[0], y_2d_range[1]),3)\n",
        "\n",
        "max_steps = 100\n",
        "num_train_samples = 2\n",
        "\n",
        "print(f'Initial x: {initial_2dX} Initial y: {initial_2dY}')"
      ],
      "metadata": {
        "id": "DfHhIXpnnfoY",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2D Optimization using Adam**\n",
        "\n",
        "This series of cells performs 2D optimization using the Adam optimizer by repeatedly updating a single variable, recording each step’s (x, y, f(x,y)) values. After completing the optimization loop, it visualizes Adam’s trajectory on the 2D function using plot_2d, allowing you to see how the optimizer navigates the landscape and converges toward a minimum."
      ],
      "metadata": {
        "id": "ReZeeAjFncfI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Optimize Ackley Using Adam**\n",
        "x_2d = torch.tensor([initial_2dX], requires_grad=True)\n",
        "y_2d = torch.tensor([initial_2dY], requires_grad=True)\n",
        "\n",
        "learning_rate = 0.5\n",
        "optimizer_2d = optim.Adam([x_2d,y_2d], lr=learning_rate)\n",
        "adam_2d_data_trace = []\n",
        "LLM_2D_Training_Samples = \"\"\n",
        "for step in range(max_steps):\n",
        "    optimizer_2d.zero_grad()\n",
        "    loss = shifted_ackley(x_2d, y_2d)\n",
        "\n",
        "    if step < num_train_samples:\n",
        "        LLM_2D_Training_Samples+= f\"x: {round(x_2d.item(),3)} y: {round(y_2d.item(), 3)}\\nf(x,y): {round(loss.item(),3)}\\n\\n\"\n",
        "    adam_2d_data_trace.append((x_2d.item(), y_2d.item(), loss.item()))\n",
        "    loss.backward()\n",
        "    optimizer_2d.step()\n",
        "print(f\"Adam's Results:\\nBest x: {x_2d.item()}, Best y: {y_2d.item()}, Best f(x,y): {loss.item()}\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "CE5zcPvGnnjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Plot Adam Optimization Path**\n",
        "plot_2d(adam_2d_data_trace, azim=130, title=\"Adam's Optimization Path on f(x,y)\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "q_DIgFfqntT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Training Sample for 2D Optimization**\n",
        "print(LLM_2D_Training_Samples)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "F0jfuQSln227"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2D Optimization using an LLM**\n",
        "\n",
        "This series of cells performs 2D numerical optimization using an LLM as the update rule: at each iteration, the model receives the current state, proposes a new x-value and y-value, and the true function value f(x,y) is computed and fed back into the prompt for the next step. The process records each predicted (x, y f(x,y)) pair along with the model’s explanation, building a trajectory of how the LLM explores the search space. This allows you to observe whether the LLM learns from the iterative feedback, improves its proposals over time, and moves toward lower function values in a manner comparable to traditional optimization algorithms. It then visualizes the LLM's trajectory on the 1D function using plot_1d, allowing you to see how the LLM navigates the landscape and converges toward a minimum."
      ],
      "metadata": {
        "id": "em4zLQQQpVyf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **System Prompt for 2D Optimization**\n",
        "LLM_2D_Training_Samples = \"\"\n",
        "system_instruction_2 = \"\"\"You are an optimization assistant, helping me find the global minimum of a mathematical function.  I will give you the function evaluation and the current iteration number at each step. Your goal is to propose input values that efficiently lead us to the global minimum within a limited number of iterations (MAX_STEPS).\n",
        "\n",
        "Here's how we'll interact:\n",
        "\n",
        "1. I will first provide MAX_STEPS along with few training examples.\n",
        "2. You will provide your response in the following exact format:\n",
        "    * Line 1: a new input 'x: , y: ', aiming to minimize the function's value f(x,y). Choose x is range [2,12] and y in [8,19]\n",
        "    * Line 2: a brief (one sentence) explanation of why you chose that input, considering the current iteration.\n",
        "3. I will then provide the function's value f(x,y) at that point, and the current iteration.\n",
        "4. We will repeat steps 2-3 until we reach the maximum number of iterations.\n",
        "\n",
        "Remember:\n",
        "\n",
        "* **Assume no prior knowledge about the function's specific form.**\n",
        "* **Balance Exploitation and Exploration:**  Early on, explore broadly. As iterations increase, focus more on promising regions.\n",
        "* **Be adaptable:**  Your approach might need to change based on the function's behavior and the remaining iterations. If you think you are stuck in a local minima or making small increments for too long, try more exploratory values and then eventually exploit new values based on your understanding of the function.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "context_2d = f\"MAX_STEPS: {max_steps - num_train_samples}\\n\" + LLM_2D_Training_Samples # Inital Context for Gemini"
      ],
      "metadata": {
        "cellView": "form",
        "id": "3kEzNSsOoTei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Optimization**\n",
        "llm_2d_data_trace = []\n",
        "\n",
        "for iter in range(max_steps - num_train_samples):\n",
        "    try:\n",
        "        response = client.models.generate_content(\n",
        "            model=model_name,\n",
        "            contents=[context_2d],\n",
        "            config=types.GenerateContentConfig(\n",
        "                thinking_config=types.ThinkingConfig(thinking_budget=0),\n",
        "                system_instruction=system_instruction_2\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        response_text = response.text.strip()\n",
        "\n",
        "        llm_xy = extract_llm_xy(response_text)\n",
        "        llm_explain = response_text.split('\\n')[1] if '\\n' in response_text else \"\"\n",
        "\n",
        "        llm_x = float(llm_xy[0])\n",
        "        llm_y = float(llm_xy[1])\n",
        "\n",
        "        f_xy = shifted_ackley(torch.tensor([llm_x]), torch.tensor([llm_y])).item()\n",
        "\n",
        "        context_2d = (\n",
        "            f\"step: {iter}\\n\"\n",
        "            f\"x: {round(llm_x, 3)} y: {round(llm_y, 3)}\\n\"\n",
        "            f\"f(x,y): {round(f_xy, 3)}\"\n",
        "        )\n",
        "\n",
        "        llm_2d_data_trace.append((llm_x, llm_y, f_xy))\n",
        "\n",
        "        print(f\"Step: {iter}\")\n",
        "        print(f\"llm_x: {llm_x}\")\n",
        "        print(f\"llm_y: {llm_y}\")\n",
        "        print(f\"f(x,y): {f_xy}\")\n",
        "        print(\"LLM's Output:\", response_text)\n",
        "        print(\" \")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error at step {iter}: {e}\")\n",
        "        print(\"Pausing for 60 seconds before continuing...\")\n",
        "        time.sleep(60)\n",
        "        continue\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "p9paN4T1ovem",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Plot Gemini 2D Optimization Path**\n",
        "plot_2d(llm_2d_data_trace, azim=130, title=\"Gemini's Optimization Path on f(x,y)\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "AZ-Ose1kpEae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 3: Comparing Results from Adam, the LLM, and other Baselines**"
      ],
      "metadata": {
        "id": "9VjBxNY_WLjp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1D Baselines & Comparisons**"
      ],
      "metadata": {
        "id": "HKnC90AWpseG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **1D Initial f(x)**\n",
        "init_1d_fx = one_dimensional_function(torch.tensor(Initial_1d_x)).item()\n",
        "print('Inital F(x):', round(init_1d_fx,3))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "k72JW5sSp16b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **1D Nelder Mead**\n",
        "best_nm_1d = optimize_with_nelder_mead(one_dimensional_function, [Initial_1d_x], max_iter=100).fun\n",
        "print(\"Nelder Mead's Best:\", round(best_nm_1d,3))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "bEiBSc1Jp-18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **1D Gradient Descent**\n",
        "best_gd_1d = optimize_gd([Initial_1d_x], one_dimensional_function, MAX_STEPS=max_steps)\n",
        "print(\"Gradient Descent's Best:\", round(best_gd_1d,3))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "SJ1RtHVBqDWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **1D Adam**\n",
        "best_adam_1d = adam_1d_data_trace[-1][1]\n",
        "print(\"Adam's Best:\", round(best_adam_1d ,3))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "20NewLMuqJRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **1D Gemini-2.5-Flash**\n",
        "best_llm_1d = llm_1d_data_trace[-1][1]\n",
        "print(\"Gemini's Best:\", round(best_llm_1d ,3))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "qfSrgNC8qMh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2D Baselines & Comparisons**"
      ],
      "metadata": {
        "id": "cqYs_c2oqVjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **2D Initial f(x,y)**\n",
        "init_2d_fx = shifted_ackley(torch.tensor(initial_2dX), torch.tensor(initial_2dY)).item()\n",
        "print('Inital F(x,y):', round(init_2d_fx,3))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "b4W_EHHpqbQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **2D Nelder Mead**\n",
        "init_2dx = np.array([initial_2dX, initial_2dY])\n",
        "best_nm_2d = optimize_with_nelder_mead(shifted_ackley, init_2dx , max_iter=max_steps).fun\n",
        "print(\"Nelder Mead's Best:\", round(best_nm_2d,3))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "71PiFu_pqgtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **2D Gradient Descent**\n",
        "init_2dx = np.array([initial_2dX, initial_2dY])\n",
        "best_gd_2d = optimize_gd(init_2dx, shifted_ackley, MAX_STEPS=max_steps)\n",
        "print(\"Gradient Descent's Best:\", round(best_gd_2d,3))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "_KaHDszEqlXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **2D Adam**\n",
        "best_adam_2d = adam_2d_data_trace[-1][2]\n",
        "print(\"Adam's Best:\", round(best_adam_2d ,3))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "qnpW2Q_Fqqqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **2D Gemini-2.5-Flash**\n",
        "best_llm_2d = llm_2d_data_trace[-1][2]\n",
        "print(\"Gemini's Best:\", round(best_llm_2d ,3))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "O1x5ZIeLqrSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Summary**\n",
        "This tutorial illustrates how LLMs can perform numerical optimization on both 1D and 2D objective functions by iteratively proposing improved parameter values using only function evaluations embedded within the prompt. By supplying the model with initial samples, the history of previously attempted x or (x, y) values, and their corresponding objective values on functions like the 1D Ackley variant and the shifted 2D Ackley function, the LLM infers patterns in the landscape and adjusts its proposed values over time—without being given gradients or any explicit optimization rules. Each iteration provides fresh contextual feedback derived from real function outputs, allowing the LLM to refine its predictions step-by-step in a manner similar to classical optimizers. Through this setup, we observe the emergence of a “textual gradient,” where the model leverages contextual reasoning to move toward better solutions, providing a gradient-free optimization strategy driven entirely by the prompt.\n",
        "\n",
        "Beyond this specific demonstration, the same approach can extend to a broad range of numerical reasoning and model-based optimization tasks. Embedding execution traces, intermediate values, or feedback signals directly into the prompt enables LLMs to iteratively tune continuous parameters, explore complex landscapes, or adapt strategies across different domains. These variations highlight how prompt-structured feedback loops can unlock optimization capabilities in LLMs, allowing them to operate as general-purpose numerical optimizers even in the absence of analytic derivatives or algorithmic instructions.\n",
        "\n",
        "## **Conclusion**\n",
        "This demonstration shows that LLMs can serve as effective gradient-free optimizers when guided through iterative prompting, capturing the essence of optimization-by-feedback in both 1D and 2D settings. Without access to gradient information, update equations, or traditional optimization heuristics, the LLM improves its proposed x and (x, y) values solely from the pattern of past inputs and objective outputs, progressively moving toward lower-value regions of the Ackley landscapes. Although the model is not computing true gradients internally, its ability to approximate better parameter choices from contextual information supports the core insight: LLMs can perform meaningful numerical optimization when supplied with structured iterative feedback. Our experiments comparing LLM behavior with Adam, Gradient Descent, and Nelder–Mead validate this idea and demonstrate that LLM-driven optimization is both feasible and interpretable, offering a practical way to explore optimization dynamics using language as the feedback medium.\n",
        "\n",
        "<br>\n",
        "\n",
        "For more details, please refer to the [SAS-Prompt Project Page](https://sites.google.com/asu.edu/sas-llm/) and the associated [research paper](https://arxiv.org/pdf/2504.20459)\n",
        "\n",
        "## **References**\n",
        "Ben Amor, H., Graesser, L., Iscen, A., D’Ambrosio, D., Abeyruwan, S., Bewley, A., Zhou, Y., Kalirathinam, K., Mishra, S., & Sanketi, P. (2025, May). SAS-prompt: Large Language Models as Numerical Optimizers for Robot Self-Improvement. In *2025 IEEE International Conference on Robotics and Automation (ICRA)* (pp. 10087-10094). IEEE."
      ],
      "metadata": {
        "id": "wUPBqk0vYzvP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_SfysrspZZG3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "NkxcjLfcvzzw",
        "cR9XAu-ToCsk",
        "ReZeeAjFncfI",
        "em4zLQQQpVyf",
        "HKnC90AWpseG"
      ]
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}