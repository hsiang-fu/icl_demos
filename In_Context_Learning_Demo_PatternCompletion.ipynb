{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction to In-Context Learning Demo: Pattern Completion using the ARC Corpus**\n",
        "\n",
        "In this notebook we present a complete step-by-step walkthrough of how Large Language Models (LLMs) can **learn and apply visual completion and transformation patterns purely from few-shot examples**. Inspired by *[Large Language Models as General Pattern Machines](https://arxiv.org/pdf/2307.04721)* (Mirchandani et al., 2023), this demo uses the Abstract and Reasoning Corpus (ARC) introduced in  *[On the Measure of Intelligence](https://arxiv.org/pdf/1911.01547)* (Chollet, 2019) as the source of structured input–output patterns and evaluates Gemini’s ability to recognize and generalize these patterns through in-context learning (ICL). The goal is to illustrate how an LLM without updates to any weights can observe a handful of small grid transformations and then correctly generate the missing output for a novel test example. The result is a learning paradigm that is purely based on the forward pass of the LLM without requiring any gradient-based updates.\n",
        "\n",
        "## **Overview**\n",
        "\n",
        "This demonstration guides you through:\n",
        "1.   Loading the ARC tasks\n",
        "2.   Constructing an ICL prompt composed of example input–output pairs\n",
        "3.   Prompting Gemini using the constructed prompt for it to infer the underlying transformation\n",
        "4. Extract and render the model’s predicted test output\n",
        "\n",
        "The result is a clear visualization of how LLMs can act as pattern machines, discovering abstract rules from minimal supervision.\n",
        "\n",
        "## **Background**\n",
        "The Abstract and Reasoning Corpus (ARC) is a benchmark introduced by François Chollet to evaluate generalization, abstraction, and concept learning in AI systems. Unlike standard vision datasets, ARC focuses on Few-shot learning and minimal prior knowledge. The underlying hypothesis is that the learning systems would discover abstract rules that allow for human-like pattern induction.\n",
        "\n",
        "## **Let's Take a Look at an Example**\n",
        "\n",
        "The figure below shows an example from the ARC dataset and demonstrates how a model identifies and applies a transformation rule. The training examples provide few-shot demonstrations of the underlying pattern. Each training example has an input pattern and an output pattern. In the example below, the output can be regarded as a de-noised version of the input. The test query is a new input from the corpus that follows the same rule. In the case below, the test query is an input image that includes a number of noisy or corrupted pixels. The task of the LLM is to generate the corrected output image which adjusts for these corrupted pixels. The underlying assumption of ICL is that the large language model can learn what sort of transformation to perform, e.g., de-noising, from the few-shot examples alone.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/intro-to-icl/intro-to-icl.github.io/refs/heads/master/static/images/sequence_transformation_1.png\" height=\"500\">\n",
        "\n",
        "**In this specific example, the learned transformation is a de-noising operation that fills in and removes any corrupted pixels or segments.**\n",
        "\n",
        "## **LLM as the Optimizer**\n",
        "In our demo, we leverage Gemini 2.0/2.5 Flash to perform in-context learning (ICL) on Abstraction and Reasoning Corpus (ARC)–style grid transformation tasks. Unlike traditional supervised learning approaches, the LLM is never trained on the task. Instead, it infers the underlying transformation rule purely from the examples we provide in the prompt.\n",
        "\n",
        "The process can be understood as a sequence of structured steps: preparing the training examples, constructing a carefully engineered prompt, invoking the LLM to infer the rule, and finally converting its output back into a grid. The LLM behaves as a reasoning engine, reading the examples like a story and deducing the pattern that connects them.\n",
        "\n",
        "## **Code Overview**\n",
        "The implementation is organized into a modular structure, with each component responsible for a different stage of the ARC task-solving pipeline. This design separates data loading, visualization, prompt construction, LLM inference, and result interpretation, making the system easy to understand, modify, and extend.\n",
        "\n",
        "## **Before Running the Demo, Follow the Instructions Below**\n",
        "To run the full experiment:\n",
        "1. Ensure all dependencies are imported and installed.\n",
        "2.\tVisit Google AI Studio (https://aistudio.google.com/) to obtain your Gemini API key.\n",
        "3.\tOnce API key is generated, copy and paste it into the demo when prompted.\n",
        "\n",
        "#### **If having trouble creating an API Key, follow the link below for instructions:**\n",
        "* #### **[Instructions on How to Obtain a Gemini API Key](https://docs.google.com/document/d/17pgikIpvZCBgcRh4NNrcee-zp3Id0pU0vq2SG52KIGE/edit?usp=sharing)**\n",
        "\n",
        "\n",
        "### ***Note: Run all the cells below to initialize the environment.**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JWBl0iMOvbE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Import Necessary Libraries**\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "import re\n",
        "import itertools\n",
        "import math\n",
        "import ast\n",
        "import json\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "import os\n",
        "import getpass"
      ],
      "metadata": {
        "id": "qDV03CJIve4R",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lh6d1fWygOfe"
      },
      "outputs": [],
      "source": [
        "#@title **Setting Up Gemini Client**\n",
        "apikey = getpass.getpass(\"Enter your Gemini API Key: \")\n",
        "client = genai.Client(api_key=apikey)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Choose a Model**\n",
        "model_dropdown = widgets.Dropdown(\n",
        "    options=[\n",
        "        (\"Gemini 2.5 Flash\", \"gemini-2.5-flash\"),\n",
        "        (\"Gemini 2.0 Flash\", \"gemini-2.0-flash\")\n",
        "    ],\n",
        "    description=\"Model:\",\n",
        "    value=\"gemini-2.5-flash\",\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "confirm_button = widgets.Button(\n",
        "    description=\"Confirm Selection\"\n",
        ")\n",
        "\n",
        "output = widgets.Output()\n",
        "\n",
        "model_name = None\n",
        "\n",
        "def on_confirm_click(b):\n",
        "    global model_name, batch_size\n",
        "\n",
        "    model_name = model_dropdown.value\n",
        "\n",
        "    with output:\n",
        "        output.clear_output()\n",
        "        print(f\"\\nSelected model: {model_name}\")\n",
        "\n",
        "confirm_button.on_click(on_confirm_click)\n",
        "\n",
        "display(model_dropdown, confirm_button, output)"
      ],
      "metadata": {
        "id": "OdA80Nrvdo2N",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Download Data from GitHub**\n",
        "if not os.path.exists(\"intro_to_icl_data\"):\n",
        "    !git clone https://github.com/hsiang-fu/intro_to_icl_data.git\n",
        "\n"
      ],
      "metadata": {
        "id": "6UMMqb9zX20A",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below cell `Grid Display Function` is responsible for the entire grid display and visual interpretation layer of the ARC demo. Its purpose is to transform the raw numerical grids—integers ranging from 0 to 9—into the colored, human-readable visualizations used throughout the notebook.\n",
        "\n",
        "ARC tasks define grids where each integer corresponds to a color category. The `arc_colors` dictionary encodes this mapping using RGB triplets.\n",
        "For example:\n",
        "- 0 → black,\n",
        "- 1 → blue,\n",
        "- 2 → green,\n",
        "- 3 → red, etc.\n",
        "\n",
        "**These colors reflect the official ARC palette.**"
      ],
      "metadata": {
        "id": "xhByfpVOhj6c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TuKafdRgOfe",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title **Grid Display Function**\n",
        "arc_colors = {\n",
        "    0: (0, 0, 0),\n",
        "    1: (0, 0, 255),\n",
        "    2: (0, 255, 0),\n",
        "    3: (255, 0, 0),\n",
        "    4: (255, 255, 0),\n",
        "    5: (255, 165, 0),\n",
        "    6: (128, 0, 128),\n",
        "    7: (150, 75, 0),\n",
        "    8: (128, 128, 128),\n",
        "    9: (255, 192, 203)\n",
        "}\n",
        "\n",
        "def show_grid(grid, title=\"\"):\n",
        "    arr = np.array(grid)\n",
        "    rgb = np.zeros((arr.shape[0], arr.shape[1], 3), dtype=np.uint8)\n",
        "    for k, v in arc_colors.items():\n",
        "        rgb[arr == k] = v\n",
        "    plt.imshow(rgb)\n",
        "    plt.axis(\"off\")\n",
        "    if title:\n",
        "        plt.title(title)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below cell `ARC ICL Demo` is responsible for running the full interactive ARC demo, including user task selection, prompt construction, LLM inference, visualization of training/test grids, parsing model outputs, and saving the results. It effectively acts as the end-to-end execution and UI layer of the entire notebook.\n",
        "\n",
        "The cell creates a simple user interface, consisting of :\n",
        "- A dropdown menu that lists ARC tasks (Task 1 through Task 10),\n",
        "- A button to confirm the selection.\n",
        "- **This makes the demo interactive, users can dynamically choose which ARC task they want to solve without modifying any code.**\n",
        "\n",
        "The model will then generate and running the visualization cell would show the following:\n",
        "\n",
        "- Training examples\n",
        "- Test example\n",
        "- Expected output\n",
        "- Model-generated output\n",
        "\n",
        "### **Let's Take a Look at an Example of a Prompt:**\n",
        "```\n",
        "Train:\n",
        "Example 1 Input: [[0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0]] -> Example 1 Output: [[0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0]]\n",
        "Example 2 Input: [[0, 3, 0, 0, 0, 3, 3, 0, ..., 0, 0, 0]] -> Example 2 Output: [[0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0]]\n",
        "Example 3 Input: [[0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0]] -> Example 3 Output: [[0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0]]\n",
        "\n",
        "Test Input: [[0, 0, 3, 3, 0, 0, 0, 0, ..., 3, 0, 0, 0, 0]] ->\n",
        "```\n",
        "\n",
        "This prompt shows the model three training examples and one test query from an ARC task (same task as the image above), where each example provides an input grid. The grid consists of rows of colored cells represented by a list of numbers (each number corresponds to a color) and a corresponding output grid that reflects how the grid changes after applying a hidden transformation rule. In ARC, the model’s objective is to infer this underlying pattern solely from the provided examples and then apply it to the test input to generate the appropriate output.\n",
        "\n",
        "This is considered ICL because the model is not being updated or fine-tuned. All “learning” occurs within the prompt itself: the examples become the model’s temporary training data. By observing the input–output pairs embedded in the prompt, the model identifies the pattern, generalizes it, and produces the appropriate output for the test query—all during a single forward pass of the LLM."
      ],
      "metadata": {
        "id": "6qsh3-ijiSq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **ARC ICL Demo — Model Call & Inference**\n",
        "\n",
        "def count_pixel_differences(a, b):\n",
        "    diff = 0\n",
        "    for r1, r2 in zip(a, b):\n",
        "        for p1, p2 in zip(r1, r2):\n",
        "            if p1 != p2:\n",
        "                diff += 1\n",
        "    return diff\n",
        "\n",
        "dropdown = widgets.Dropdown(\n",
        "    options=[f\"Task {i}\" for i in range(1, 11)],\n",
        "    value=\"Task 1\",\n",
        "    description=\"Select Task:\",\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "button = widgets.Button(description=\"Run Inference\")\n",
        "\n",
        "file = None\n",
        "\n",
        "def run_llm_inference(file):\n",
        "    file_path = \"intro_to_icl_data/\" + file + \".json\"\n",
        "\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"File not found: {file_path}\")\n",
        "        return None\n",
        "\n",
        "    with open(file_path, \"r\") as f:\n",
        "        task = json.load(f)\n",
        "\n",
        "    # Build prompting string\n",
        "    prompt = \"Train:\\n\"\n",
        "    for i, example in enumerate(task[\"train\"]):\n",
        "        prompt += (\n",
        "            f\"Example {i+1} Input: {example['input']} -> \"\n",
        "            f\"Example {i+1} Output: {example['output']}\\n\"\n",
        "        )\n",
        "    prompt += f\"\\nTest Input: {task['test'][0]['input']} -> \"\n",
        "\n",
        "    # Run LLM\n",
        "    response = client.models.generate_content(\n",
        "        model=model_name,\n",
        "        contents=[prompt],\n",
        "        config=types.GenerateContentConfig(\n",
        "            thinking_config=types.ThinkingConfig(thinking_budget=0)\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # Extract output tokens\n",
        "    match = re.search(r\"\\[.*\\]\", response.text, re.DOTALL)\n",
        "    if not match:\n",
        "        print(\"No valid output found in model response.\")\n",
        "        return None\n",
        "\n",
        "    # Save original ground truth\n",
        "    task[\"test\"][0][\"ground_truth\"] = task[\"test\"][0][\"output\"]\n",
        "    try:\n",
        "        task[\"test\"][1][\"ground_truth\"] = task[\"test\"][1][\"output\"]\n",
        "    except:\n",
        "        pass\n",
        "    # Save model output\n",
        "    task[\"test\"][0][\"output\"] = ast.literal_eval(match.group(0))\n",
        "\n",
        "    # Write output JSON\n",
        "    directory, filename = os.path.split(file_path)\n",
        "    output_filename = \"output_\" + filename\n",
        "    output_file_path = os.path.join(directory, output_filename)\n",
        "\n",
        "    with open(output_file_path, \"w\") as f:\n",
        "        json.dump(task, f, indent=2)\n",
        "\n",
        "    print(f\"\\nLLM output saved to: {output_file_path}\")\n",
        "    return output_file_path\n",
        "\n",
        "def on_button_click(b):\n",
        "    global file\n",
        "    file = dropdown.value\n",
        "    print(f\"\\nRunning LLM inference for: {file}…\")\n",
        "    run_llm_inference(file)\n",
        "\n",
        "button.on_click(on_button_click)\n",
        "\n",
        "display(widgets.VBox([dropdown, button]))"
      ],
      "metadata": {
        "id": "MZ7Yx5sKw1sg",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5_8IkaFgOff",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title **ARC ICL Demo — Visualization**\n",
        "\n",
        "def visualize_results(file):\n",
        "    file_path = \"intro_to_icl_data/output_\" + file + \".json\"\n",
        "\n",
        "    if not os.path.exists(file_path):\n",
        "        print(\"Run inference first — output file not found.\")\n",
        "        return\n",
        "\n",
        "    with open(file_path, \"r\") as f:\n",
        "        output = json.load(f)\n",
        "\n",
        "    # Visualize train examples\n",
        "    for i, pair in enumerate(output[\"train\"]):\n",
        "        plt.figure(figsize=(4, 2))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        show_grid(pair[\"input\"], \"\")\n",
        "        plt.subplot(1, 2, 2)\n",
        "        show_grid(pair[\"output\"], \"\")\n",
        "        plt.suptitle(f\"Train Example {i+1}\")\n",
        "        plt.show()\n",
        "\n",
        "    for i, pair in enumerate(output[\"test\"]):\n",
        "        plt.figure(figsize=(4, 2))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        show_grid(pair[\"input\"], f\"Test Example {i+1}\")\n",
        "        plt.show()\n",
        "\n",
        "    # Visualize test results\n",
        "    for i, pair in enumerate(output[\"test\"]):\n",
        "        generated = pair[\"output\"]\n",
        "        ground_truth = pair.get(\"ground_truth\", None)\n",
        "\n",
        "        plt.figure(figsize=(4, 2))\n",
        "\n",
        "        if ground_truth:\n",
        "            plt.subplot(1, 2, 1)\n",
        "            show_grid(ground_truth, \"Ground Truth\")\n",
        "\n",
        "            plt.subplot(1, 2, 2)\n",
        "            show_grid(generated, \"Generated Output\")\n",
        "        else:\n",
        "            show_grid(generated, \"Generated Output\")\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "        if ground_truth:\n",
        "            try:\n",
        "                diff = count_pixel_differences(ground_truth, generated)\n",
        "                print(f\"\\nThe Number of Pixel Differences Between the Predicted Output and Ground Truth is: {diff}\\n\")\n",
        "            except:\n",
        "                print(\"\\nError comparing outputs.\\n\")\n",
        "\n",
        "# Run it manually after inference:\n",
        "visualize_results(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Summary**\n",
        "This demo showcases how Large Language Models can perform in-context learning by recognizing visual transformation patterns from a small number of examples. Using tasks from the Abstract and Reasoning Corpus (ARC), we construct prompts that present input–output grid pairs and then ask the model to generate the missing transformation for a new test input. Through this setup, we observe how the model identifies the hidden rule by examining the structure of the grids rather than relying on built-in symbolic reasoning.\n",
        "\n",
        "Beyond the specific task illustrated here, the same methodology can be applied to a wide spectrum of ARC-style transformations. By crafting carefully chosen few-shot examples, we can guide the model toward discovering diverse rules such as color mappings, object counting, symmetry detection, region extraction, movement operations, and shape reconstruction. These extensions demonstrate how flexible the in-context learning framework is when applied to structured, symbolic data.\n",
        "\n",
        "## **Conclusion**\n",
        "This demonstration highlights that LLMs can generalize abstract rules similar to human inductive reasoning, even when reasoning over symbolic grid data. By providing only a few examples, the model can infer the underlying transformation and apply it to new cases—capturing the essence of in-context learning. Although the model does not perform explicit algorithmic reasoning, its ability to internalize and reproduce patterns suggests strong potential for tasks requiring rapid generalization from limited data. ARC serves as a valuable testing ground for understanding these capabilities, revealing both the promise and the limitations of treating LLMs as general pattern machines.\n",
        "\n",
        "## **References**\n",
        "Chollet, F. (2019). On the Measure of Intelligence. *arXiv preprint arXiv:1911.01547.*\n",
        "\n",
        "Mirchandani, S., Xia, F., Florence, P., Ichter, B., Driess, D., Arenas, M.G., Rao, K., Sadigh, D. and Zeng, A. (2023, December). Large Language Models as General Pattern Machines. *In Conference on Robot Learning* (pp. 2498-2518). PMLR."
      ],
      "metadata": {
        "id": "5tkOJMOJX7ix"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wSMbLRACZy3L"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}